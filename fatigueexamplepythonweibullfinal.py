# -*- coding: utf-8 -*-
"""FatigueExamplePythonWeibullFINAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19SkVwgmv7o5LDu8BCdrT4WaRQBcJ7dD_
"""

# ============================================================================
#   FATIGUE ANALYSIS - WEIBULL MODEL (MLE + BAYESIAN)
#   Castillo-Canteli Dimensionless Formulation
#   WITH GOOGLE COLAB FILE UPLOAD SUPPORT
# ============================================================================
# Physical justification:
# - Lower bounded problem (N > 0) ‚Üí Weibull distribution for minima
# - Gumbel is limiting case (Œ≤ ‚Üí ‚àû) naturally captured by Weibull
# ============================================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pymc as pm
import arviz as az
import pytensor.tensor as pt
from scipy import stats
from scipy.optimize import minimize, differential_evolution
import warnings
import sys
import os
import glob

warnings.filterwarnings('ignore')

plt.rcParams['figure.figsize'] = [14, 8]

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

print("="*70)
print("FATIGUE ANALYSIS - WEIBULL MODEL")
print("Maximum Likelihood Estimation + Bayesian Inference")
print("="*70)
print(f"PyMC version: {pm.__version__}")
print(f"ArviZ version: {az.__version__}")

# ============================================================================
# DETECT GOOGLE COLAB ENVIRONMENT
# ============================================================================

try:
    from google.colab import files
    IN_COLAB = True
    print("\n‚úì Google Colab detected")
except:
    IN_COLAB = False
    print("\n‚úì Running in local environment")

# ============================================================================
# CLEAN PREVIOUS OUTPUT FILES
# ============================================================================

print("\n" + "="*70)
print("CLEANING PREVIOUS OUTPUT FILES")
print("="*70)

output_patterns = [
    'data_exploration*.png',
    'mle_results*.png',
    'prior_distributions*.png',
    'trace_plots*.png',
    'posterior_distributions*.png',
    'percentiles_of_percentiles*.png',
    'fatigue_posterior*.nc',
    'fatigue_summary*.csv',
    'percentiles*.npz',
    'mle_estimates.txt'
]

# Check for existing files
existing_files = []
for pattern in output_patterns:
    existing_files.extend(glob.glob(pattern))

if len(existing_files) > 0:
    print(f"\nFound {len(existing_files)} existing output files:")
    for f in existing_files[:10]:  # Show first 10
        print(f"  - {f}")
    if len(existing_files) > 10:
        print(f"  ... and {len(existing_files) - 10} more")

    # Ask for confirmation
    confirm = input("\nDelete all previous output files? (y/n): ").strip().lower()

    if confirm == 'y':
        removed_count = 0
        for filepath in existing_files:
            try:
                os.remove(filepath)
                removed_count += 1
            except Exception as e:
                print(f"  Could not remove {filepath}: {e}")
        print(f"\n‚úì Removed {removed_count} files")
    else:
        print("\n‚úì Keeping existing files (will be overwritten)")
else:
    print("  No previous output files found")

# ============================================================================
# DATA SOURCE SELECTION
# ============================================================================

print("\n" + "="*70)
print("DATA SOURCE SELECTION")
print("="*70)

def get_user_choice():
    """Get user choice for data source."""
    print("\nChoose data source:")
    print("  1 - Use Holmen example data")
    print("  2 - Upload your own data file")

    while True:
        choice = input("\nEnter choice (1 or 2): ").strip()
        if choice in ['1', '2']:
            return choice
        print("Invalid choice. Please enter 1 or 2.")

# Determine data source
if IN_COLAB:
    try:
        data_choice = get_user_choice()
    except (EOFError, KeyboardInterrupt):
        print("\nInput not available, defaulting to file upload mode")
        data_choice = '2'
else:
    print("\nNot in Colab - defaulting to Holmen example")
    print("To use your own data, set data_choice = '2' before this section")
    data_choice = '1'

use_example = (data_choice == '1')

print(f"\nSelected mode: {'Example (Holmen)' if use_example else 'Upload file'}")

# ============================================================================
# DATA LOADING
# ============================================================================

print("\n" + "="*70)
print("DATA LOADING")
print("="*70)

if use_example:
    print("\n‚úì Using Holmen example data...")

    # Holmen example data
    stress_data = np.array([
        0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95,
        0.95, 0.95, 0.95, 0.95, 0.95,
        0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90,
        0.90, 0.90, 0.90, 0.90, 0.90,
        0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825,
        0.825, 0.825, 0.825, 0.825, 0.825,
        0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75,
        0.75, 0.75, 0.75, 0.75, 0.75,
        0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675,
        0.675, 0.675, 0.675, 0.675, 0.675,
    ])

    cycles_data = np.array([
        0.037, 0.072, 0.074, 0.076, 0.083, 0.085, 0.105, 0.109, 0.120, 0.123, 0.143,
        0.203, 0.206, 0.217, 0.257, 0.201, 0.216, 0.226, 0.252, 0.257, 0.295, 0.311,
        0.342, 0.356, 0.451, 0.457, 0.509, 0.540, 0.680, 1.129, 1.246, 1.258, 1.460,
        1.492, 2.400, 2.410, 2.590, 2.903, 3.330, 3.590, 3.847, 4.110, 4.820, 5.560,
        5.598, 6.710, 9.930, 12.600, 15.580, 16.190, 17.280, 18.620, 20.300, 24.900,
        26.260, 27.940, 36.350, 48.420, 50.090, 67.340, 102.950, 280.320, 339.830,
        366.900, 485.620, 658.960, 896.330, 1241.760, 1250.200, 1329.780, 1399.830,
        1459.140, 3294.820, 12709, 14373
    ])

    # Default configuration for Holmen
    config = {
        'n_tune': 1000,
        'n_draws': 1000,
        'n_chains': 1,
        'n_stress_points': 50,
        'n_param_samples': 1000,
        'percentiles_base': [0.01, 0.15, 0.50, 0.85, 0.99],
        'percentiles_sub': [0.01, 0.15, 0.50, 0.85, 0.99],
        'cv_N0': 0.25,
        'cv_Delta0': 0.15,
        'cv_beta': 0.30,
        'cv_lambda': 0.25,
        'cv_delta': 0.30,
        'target_accept': 0.95
    }

    dataset_name = "Holmen"

else:
    print("\nüìÅ Please upload your data file...")
    print("\nFile format requirements:")
    print("  Option 1: CSV or Excel file (.csv, .xlsx, .xls)")
    print("    - Required columns: 'N' (cycles), 'Deltasigma' (stress)")
    print("    - Optional: configuration parameters")
    print("\n  Option 2: R/OpenBUGS format (.txt, .dat)")
    print("    - list(M=..., N=c(...), Deltasigma=c(...), ...)")

    if IN_COLAB:
        print("\nUploading file...")
        uploaded = files.upload()

        if len(uploaded) == 0:
            print("No file uploaded. Using Holmen example instead.")
            use_example = True
        else:
            filename = list(uploaded.keys())[0]
            print(f"\n‚úì File uploaded: {filename}")
            print(f"  File size: {len(uploaded[filename])} bytes")

            # In Colab, the file is already in the current directory
            # Verify it exists
            import os
            if not os.path.exists(filename):
                print(f"  ‚úó Warning: File not found at {filename}")
                print(f"  Current directory: {os.getcwd()}")
                print(f"  Files in directory: {os.listdir('.')[:10]}")
            else:
                print(f"  ‚úì File verified at: {os.path.abspath(filename)}")
    else:
        filename = input("\nEnter data file path: ").strip()
        if not os.path.exists(filename):
            print(f"File not found: {filename}")
            print("Using Holmen example instead.")
            use_example = True

    if not use_example:
        # Read data file
        try:
            # Check file extension
            if filename.endswith(('.txt', '.dat')):
                # R/OpenBUGS format parser
                print("\n  Detected R/OpenBUGS format...")
                print(f"  Reading file: {filename}")

                with open(filename, 'r') as f:
                    content = f.read()

                print(f"  File size: {len(content)} characters")

                # Parse R list format
                import re

                # Extract N values
                n_match = re.search(r'N\s*=\s*c\(([^)]+)\)', content)
                if not n_match:
                    raise ValueError("Could not find 'N=c(...)' in file")
                n_values_str = n_match.group(1)
                # Remove all whitespace and split by comma, handle decimals
                n_values = [float(x.strip()) for x in n_values_str.split(',') if x.strip()]

                # Extract Deltasigma values
                delta_match = re.search(r'Deltasigma\s*=\s*c\(([^)]+)\)', content)
                if not delta_match:
                    raise ValueError("Could not find 'Deltasigma=c(...)' in file")
                delta_values_str = delta_match.group(1)
                # Remove all whitespace and split by comma, handle decimals
                delta_values = [float(x.strip()) for x in delta_values_str.split(',') if x.strip()]

                if len(n_values) != len(delta_values):
                    raise ValueError(f"Length mismatch: N has {len(n_values)} values, Deltasigma has {len(delta_values)}")

                cycles_data = np.array(n_values)
                stress_data = np.array(delta_values)

                print(f"  ‚úì Parsed N: {len(cycles_data)} values")
                print(f"  ‚úì Parsed Deltasigma: {len(stress_data)} values")
                print(f"  ‚úì Created numpy arrays successfully")
                print(f"    cycles_data shape: {cycles_data.shape}")
                print(f"    stress_data shape: {stress_data.shape}")

                # Default configuration
                config = {
                    'n_tune': 1000,
                    'n_draws': 2000,
                    'n_chains': 2,
                    'n_stress_points': 50,
                    'n_param_samples': 1000,
                    'percentiles_base': [0.01, 0.15, 0.50, 0.85, 0.99],
                    'percentiles_sub': [0.01, 0.15, 0.50, 0.85, 0.99],
                    'cv_N0': 0.25,
                    'cv_Delta0': 0.15,
                    'cv_beta': 0.30,
                    'cv_lambda': 0.25,
                    'cv_delta': 0.30,
                    'target_accept': 0.95
                }

                # Try to extract optional parameters from R list
                # M (number of observations)
                m_match = re.search(r'M\s*=\s*(\d+)', content)
                if m_match:
                    M_val = int(m_match.group(1))
                    print(f"  ‚úì Found M={M_val} (verification: {len(cycles_data)} observations)")

                # ns (number of stress points)
                ns_match = re.search(r'ns\s*=\s*(\d+)', content)
                if ns_match:
                    config['n_stress_points'] = int(ns_match.group(1))
                    print(f"  ‚úì Found ns={config['n_stress_points']}")

                # np (number of percentiles)
                np_match = re.search(r'np\s*=\s*(\d+)', content)
                if np_match:
                    np_val = int(np_match.group(1))
                    print(f"  ‚úì Found np={np_val}")

                # percentiles
                perc_match = re.search(r'percentiles\s*=\s*c\(([\d.,\s]+)\)', content)
                if perc_match:
                    perc_values = [float(x.strip()) for x in perc_match.group(1).split(',')]
                    config['percentiles_base'] = perc_values
                    config['percentiles_sub'] = perc_values
                    print(f"  ‚úì Found percentiles: {perc_values}")

                # n_tune (warmup)
                tune_match = re.search(r'n_tune\s*=\s*(\d+)', content)
                if tune_match:
                    config['n_tune'] = int(tune_match.group(1))
                    print(f"  ‚úì Found n_tune={config['n_tune']}")

                # n_draws (samples)
                draws_match = re.search(r'n_draws\s*=\s*(\d+)', content)
                if draws_match:
                    config['n_draws'] = int(draws_match.group(1))
                    print(f"  ‚úì Found n_draws={config['n_draws']}")

                # n_chains
                chains_match = re.search(r'n_chains\s*=\s*(\d+)', content)
                if chains_match:
                    config['n_chains'] = int(chains_match.group(1))
                    print(f"  ‚úì Found n_chains={config['n_chains']}")

                # Coefficient of variation parameters
                cv_params = ['cv_N0', 'cv_Delta0', 'cv_beta', 'cv_lambda', 'cv_delta']
                for cv_param in cv_params:
                    cv_match = re.search(rf'{cv_param}\s*=\s*([\d.]+)', content)
                    if cv_match:
                        config[cv_param] = float(cv_match.group(1))
                        print(f"  ‚úì Found {cv_param}={config[cv_param]}")

                # target_accept
                accept_match = re.search(r'target_accept\s*=\s*([\d.]+)', content)
                if accept_match:
                    config['target_accept'] = float(accept_match.group(1))
                    print(f"  ‚úì Found target_accept={config['target_accept']}")

                dataset_name = filename.split('.')[0]

                print(f"\n  ‚úì‚úì‚úì R FORMAT FILE LOADED SUCCESSFULLY")
                print(f"      Dataset: {dataset_name}")
                print(f"      use_example = {use_example}")
                print(f"      Variables 'cycles_data' and 'stress_data' created")

            elif filename.endswith('.csv'):
                df = pd.read_csv(filename)
            elif filename.endswith(('.xlsx', '.xls')):
                df = pd.read_excel(filename)
            else:
                raise ValueError("Unsupported file format. Use .csv, .xlsx, .xls, .txt, or .dat")

            # CSV/Excel format (if not R format)
            if filename.endswith(('.csv', '.xlsx', '.xls')):
                # Extract required columns
                if 'N' not in df.columns or 'Deltasigma' not in df.columns:
                    raise ValueError("File must contain 'N' and 'Deltasigma' columns")

                cycles_data = df['N'].values
                stress_data = df['Deltasigma'].values

                # Remove NaN values
                valid_mask = ~(np.isnan(cycles_data) | np.isnan(stress_data))
                cycles_data = cycles_data[valid_mask]
                stress_data = stress_data[valid_mask]

                print(f"\n‚úì Data loaded: {len(cycles_data)} observations")

                # Try to read configuration parameters
                config = {
                    'n_tune': 1000,
                    'n_draws': 2000,
                    'n_chains': 2,
                    'n_stress_points': 50,
                    'n_param_samples': 1000,
                    'percentiles_base': [0.01, 0.15, 0.50, 0.85, 0.99],
                    'percentiles_sub': [0.01, 0.15, 0.50, 0.85, 0.99],
                    'cv_N0': 0.25,
                    'cv_Delta0': 0.15,
                    'cv_beta': 0.30,
                    'cv_lambda': 0.25,
                    'cv_delta': 0.30,
                    'target_accept': 0.95
                }

                # Check for configuration in the file
                config_params = {
                    'n_tune': ['n_tune', 'tune', 'warmup'],
                    'n_draws': ['n_draws', 'draws', 'samples'],
                    'n_chains': ['n_chains', 'chains'],
                    'n_stress_points': ['n_stress_points', 'stress_points'],
                    'n_param_samples': ['n_param_samples', 'param_samples'],
                    'cv_N0': ['cv_N0', 'cv_n0'],
                    'cv_Delta0': ['cv_Delta0', 'cv_delta0'],
                    'cv_beta': ['cv_beta'],
                    'cv_lambda': ['cv_lambda'],
                    'cv_delta': ['cv_delta'],
                    'target_accept': ['target_accept', 'accept_rate']
                }

                for param, possible_names in config_params.items():
                    for name in possible_names:
                        if name in df.columns:
                            value = df[name].dropna().iloc[0]
                            config[param] = float(value) if param.startswith('cv_') or param == 'target_accept' else int(value)
                            print(f"  Found config: {param} = {config[param]}")
                            break

                # Try to read percentiles
                if 'percentiles_base' in df.columns:
                    perc_base = df['percentiles_base'].dropna().values
                    config['percentiles_base'] = perc_base.tolist()
                    print(f"  Found percentiles_base: {config['percentiles_base']}")

                if 'percentiles_sub' in df.columns:
                    perc_sub = df['percentiles_sub'].dropna().values
                    config['percentiles_sub'] = perc_sub.tolist()
                    print(f"  Found percentiles_sub: {config['percentiles_sub']}")

                dataset_name = filename.split('.')[0]

        except Exception as e:
            print(f"\n‚úó Error reading file: {e}")
            import traceback
            print("\nFull error traceback:")
            traceback.print_exc()
            print("\nUsing Holmen example instead.")
            use_example = True
            # Make sure to unset any partial data
            if 'cycles_data' in locals():
                del cycles_data
            if 'stress_data' in locals():
                del stress_data

# Final check - if reverted to example OR if data wasn't loaded, use Holmen
if use_example or 'stress_data' not in locals() or 'cycles_data' not in locals():
    if not use_example:
        print("\n‚ö† Data not loaded properly, using Holmen example instead.")
    # Load Holmen data (same as above)
    stress_data = np.array([
        0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95,
        0.95, 0.95, 0.95, 0.95, 0.95,
        0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90,
        0.90, 0.90, 0.90, 0.90, 0.90,
        0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825,
        0.825, 0.825, 0.825, 0.825, 0.825,
        0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75,
        0.75, 0.75, 0.75, 0.75, 0.75,
        0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675,
        0.675, 0.675, 0.675, 0.675, 0.675,
    ])
    cycles_data = np.array([
        0.037, 0.072, 0.074, 0.076, 0.083, 0.085, 0.105, 0.109, 0.120, 0.123, 0.143,
        0.203, 0.206, 0.217, 0.257, 0.201, 0.216, 0.226, 0.252, 0.257, 0.295, 0.311,
        0.342, 0.356, 0.451, 0.457, 0.509, 0.540, 0.680, 1.129, 1.246, 1.258, 1.460,
        1.492, 2.400, 2.410, 2.590, 2.903, 3.330, 3.590, 3.847, 4.110, 4.820, 5.560,
        5.598, 6.710, 9.930, 12.600, 15.580, 16.190, 17.280, 18.620, 20.300, 24.900,
        26.260, 27.940, 36.350, 48.420, 50.090, 67.340, 102.950, 280.320, 339.830,
        366.900, 485.620, 658.960, 896.330, 1241.760, 1250.200, 1329.780, 1399.830,
        1459.140, 3294.820, 12709, 14373
    ])
    config = {
        'n_tune': 1000,
        'n_draws': 2000,
        'n_chains': 2,
        'n_stress_points': 50,
        'n_param_samples': 1000,
        'percentiles_base': [0.01, 0.15, 0.50, 0.85, 0.99],
        'percentiles_sub': [0.01, 0.15, 0.50, 0.85, 0.99],
        'cv_N0': 0.25,
        'cv_Delta0': 0.15,
        'cv_beta': 0.30,
        'cv_lambda': 0.25,
        'cv_delta': 0.30,
        'target_accept': 0.95
    }
    dataset_name = "Holmen"

N_min = cycles_data.min()
N_max = cycles_data.max()

print(f"\n{'='*70}")
print(f"DATASET: {dataset_name}")
print(f"{'='*70}")
print(f"Data source: {'Holmen example (built-in)' if use_example else 'User-uploaded file'}")
print(f"Observations: {len(cycles_data)}")
print(f"Stress range: [{stress_data.min():.3f}, {stress_data.max():.3f}] (dimensionless)")
print(f"Cycles range: [{N_min:.4f}, {N_max:.2f}]")
print(f"\nFirst 5 N values: {cycles_data[:5]}")
print(f"First 5 Deltasigma values: {stress_data[:5]}")

print(f"\n{'='*70}")
print("ANALYSIS CONFIGURATION")
print(f"{'='*70}")
print(f"MCMC Settings:")
print(f"  Tune (warmup): {config['n_tune']}")
print(f"  Draws (samples): {config['n_draws']}")
print(f"  Chains: {config['n_chains']}")
print(f"  Target acceptance: {config['target_accept']}")
print(f"\nPercentiles:")
print(f"  Stress points: {config['n_stress_points']}")
print(f"  Parameter samples: {config['n_param_samples']}")
print(f"  Base percentiles: {[int(p*100) for p in config['percentiles_base']]}%")
print(f"  Sub-percentiles: {[int(p*100) for p in config['percentiles_sub']]}%")
print(f"\nPrior Coefficients of Variation (CV):")
print(f"  CV(N‚ÇÄ) = {config['cv_N0']:.2f}")
print(f"  CV(ŒîœÉ‚ÇÄ) = {config['cv_Delta0']:.2f}")
print(f"  CV(Œ≤) = {config['cv_beta']:.2f}")
print(f"  CV(Œª) = {config['cv_lambda']:.2f}")
print(f"  CV(Œ¥) = {config['cv_delta']:.2f}")

# ============================================================================
# DATA VISUALIZATION
# ============================================================================

print("\n" + "="*70)
print("DATA VISUALIZATION")
print("="*70)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

ax = axes[0]
# Use different colors for each unique stress level
unique_stresses = np.unique(stress_data)
colors_plot = plt.cm.rainbow(np.linspace(0, 1, len(unique_stresses)))

for i, stress_level in enumerate(unique_stresses):
    mask = stress_data == stress_level
    ax.scatter(cycles_data[mask], [stress_level]*np.sum(mask),
              alpha=0.7, s=50, label=f'{stress_level:.3f}',
              color=colors_plot[i], edgecolors='black', linewidths=0.5)

ax.set_xlabel('Cycles to Failure (N)', fontsize=12, fontweight='bold')
ax.set_ylabel('Stress (Deltasigma, dimensionless)', fontsize=12, fontweight='bold')
ax.set_xscale('log')
ax.set_title(f'S-N Data ({dataset_name}) - Weibull Model', fontsize=13, fontweight='bold')
if len(unique_stresses) <= 10:
    ax.legend(fontsize=9, loc='upper right')
ax.grid(True, alpha=0.3, which='both')

ax = axes[1]
ax.scatter(np.log(cycles_data), np.log(stress_data), alpha=0.6, s=40, color='darkblue')
ax.set_xlabel('ln(Cycles)', fontsize=12, fontweight='bold')
ax.set_ylabel('ln(Stress)', fontsize=12, fontweight='bold')
ax.set_title('Log-Log Space', fontsize=13, fontweight='bold')
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('data_exploration_weibull.png', dpi=150, bbox_inches='tight')
plt.show()

print("‚úì Data visualization completed!")

# ============================================================================
# PHASE 1: MAXIMUM LIKELIHOOD ESTIMATION (WEIBULL)
# ============================================================================

print("\n" + "="*70)
print("PHASE 1: MAXIMUM LIKELIHOOD ESTIMATION")
print("="*70)

def weibull_log_likelihood(params, stress, cycles):
    """
    Weibull log-likelihood for fatigue data.
    Castillo-Canteli dimensionless formulation.

    Model for minima (lower bounded data):
    log(N) ~ Weibull with location-scale depending on stress

    Parameters:
    -----------
    params : array [N0, Delta0, beta, lambda_param, delta]
        N0: reference number of cycles
        Delta0: reference stress (endurance limit)
        beta: shape parameter (Weibull)
        lambda_param: location parameter
        delta: scale parameter
    """
    N0, Delta0, beta, lambda_param, delta = params

    # Validations
    if N0 <= 0 or Delta0 <= 0 or beta <= 0 or delta <= 0:
        return -np.inf

    # Dimensionless transformation
    log_N_dimensionless = np.log(cycles) - np.log(N0)
    r = np.log(stress) - np.log(Delta0)

    # Check for valid r values
    if np.any(np.abs(r) < 1e-10):
        return -np.inf

    # Weibull parameters for minima
    # Location parameter
    mu_Y = (-lambda_param - delta) / r
    # Scale parameter
    sigma_Y = delta / (beta * np.abs(r))

    # Check valid sigma
    if np.any(sigma_Y <= 0):
        return -np.inf

    # Standardized variable for Weibull (Gumbel for minima parametrization)
    z = (log_N_dimensionless - mu_Y) / sigma_Y

    # Weibull log-likelihood for minima
    # CDF: F(y) = 1 - exp(-exp(z)) where z = (y - mu)/sigma
    # PDF: f(y) = (1/sigma) * exp(z - exp(z))

    log_lik = -np.log(sigma_Y) + z - np.exp(z)

    # Check for invalid values
    if not np.all(np.isfinite(log_lik)):
        return -np.inf

    return np.sum(log_lik)

def negative_log_likelihood(params, stress, cycles):
    """Negative log-likelihood for minimization."""
    return -weibull_log_likelihood(params, stress, cycles)

# Initial guess based on physical reasoning
N0_init = N_min * 0.5
Delta0_init = stress_data.min() * 0.7
beta_init = 3.0
lambda_init = -8.0
delta_init = 2.0

initial_params = np.array([N0_init, Delta0_init, beta_init, lambda_init, delta_init])

print("\nInitial guess:")
print(f"  N‚ÇÄ = {N0_init:.4f}")
print(f"  ŒîœÉ‚ÇÄ = {Delta0_init:.4f}")
print(f"  Œ≤ = {beta_init:.4f}")
print(f"  Œª = {lambda_init:.4f}")
print(f"  Œ¥ = {delta_init:.4f}")

# Parameter bounds for optimization
bounds = [
    (0.001, N_min * 0.9),           # N0
    (stress_data.min() * 0.4, stress_data.min() * 0.99),  # Delta0
    (0.5, 15.0),                    # beta
    (-12.0, -4.0),                  # lambda
    (0.5, 5.0)                      # delta
]

print("\nRunning global optimization (differential evolution)...")
print("This may take a few minutes...")

result_global = differential_evolution(
    negative_log_likelihood,
    bounds=bounds,
    args=(stress_data, cycles_data),
    seed=RANDOM_SEED,
    maxiter=1000,
    popsize=30,
    tol=1e-7,
    atol=1e-7,
    workers=1,
    updating='deferred',
    polish=True
)

print("\n‚úì Global optimization completed!")
print(f"  Success: {result_global.success}")
print(f"  Log-likelihood: {-result_global.fun:.2f}")
print(f"  Iterations: {result_global.nit}")

mle_params = result_global.x

print("\n" + "="*70)
print("MLE ESTIMATES (WEIBULL MODEL)")
print("="*70)
print(f"  N‚ÇÄ (reference cycles) = {mle_params[0]:.6f}")
print(f"  ŒîœÉ‚ÇÄ (reference stress) = {mle_params[1]:.6f}")
print(f"  Œ≤ (shape parameter)    = {mle_params[2]:.6f}")
print(f"  Œª (location param)     = {mle_params[3]:.6f}")
print(f"  Œ¥ (scale param)        = {mle_params[4]:.6f}")
print(f"\n  Log-likelihood = {-result_global.fun:.2f}")

# Save MLE results
with open('mle_estimates.txt', 'w') as f:
    f.write(f"MLE ESTIMATES - WEIBULL MODEL - {dataset_name}\n")
    f.write("="*50 + "\n")
    f.write(f"N0 = {mle_params[0]:.8f}\n")
    f.write(f"Delta0 = {mle_params[1]:.8f}\n")
    f.write(f"beta = {mle_params[2]:.8f}\n")
    f.write(f"lambda = {mle_params[3]:.8f}\n")
    f.write(f"delta = {mle_params[4]:.8f}\n")
    f.write(f"Log-likelihood = {-result_global.fun:.8f}\n")

print("\n‚úì MLE estimates saved to 'mle_estimates.txt'")

# Visualize MLE fit
print("\nVisualizing MLE fit...")

fig, ax = plt.subplots(figsize=(14, 8))

# Plot observed data
ax.scatter(cycles_data, stress_data, c='black', s=80,
          alpha=0.9, label='Observed data', zorder=100, marker='o',
          edgecolors='white', linewidths=1.5)

# Plot MLE curves for different percentiles
stress_range_plot = np.linspace(stress_data.min() * 0.97, stress_data.max() * 1.03, 100)
percentiles_mle = [0.01, 0.10, 0.50, 0.90, 0.99]
colors_mle = ['#8B0000', '#FF8C00', '#228B22', '#4169E1', '#8B008B']

N0_mle, Delta0_mle, beta_mle, lambda_mle, delta_mle = mle_params

for perc, color in zip(percentiles_mle, colors_mle):
    N_perc = []
    for stress in stress_range_plot:
        if stress > 0 and Delta0_mle > 0:
            r = np.log(stress / Delta0_mle)
            if abs(r) > 1e-10:
                mu_Y = (-lambda_mle - delta_mle) / r
                sigma_Y = delta_mle / (beta_mle * abs(r))

                # Weibull quantile for minima (Gumbel parametrization)
                z_p = np.log(-np.log(1 - perc))
                Y_p = mu_Y + sigma_Y * z_p
                N_p = N0_mle * np.exp(Y_p)

                if N_p > 0 and np.isfinite(N_p):
                    N_perc.append(N_p)
                else:
                    N_perc.append(np.nan)
            else:
                N_perc.append(np.nan)
        else:
            N_perc.append(np.nan)

    valid_mask = ~np.isnan(N_perc)
    if np.sum(valid_mask) > 0:
        ax.plot(np.array(N_perc)[valid_mask], stress_range_plot[valid_mask],
               color=color, linewidth=2.5, label=f'P{int(perc*100)} MLE',
               alpha=0.8)

ax.set_xlabel('Cycles to Failure (N)', fontsize=13, fontweight='bold')
ax.set_ylabel('Stress (Deltasigma, dimensionless)', fontsize=13, fontweight='bold')
ax.set_xscale('log')
ax.set_title(f'MLE Fit - Weibull Model ({dataset_name})',
            fontsize=14, fontweight='bold')
ax.legend(fontsize=10, loc='upper right')
ax.grid(True, alpha=0.3, which='both')
ax.set_xlim([N_min * 0.1, N_max * 10])

plt.tight_layout()
plt.savefig('mle_results.png', dpi=150, bbox_inches='tight')
plt.show()

print("‚úì MLE visualization completed!")

# ============================================================================
# PHASE 2: DEFINE INFORMATIVE PRIORS FROM MLE
# ============================================================================

print("\n" + "="*70)
print("PHASE 2: DEFINING BAYESIAN PRIORS FROM MLE")
print("="*70)

# Use MLE estimates to define informative Normal priors
# Standard deviations from coefficients of variation

N0_mle_val = mle_params[0]
Delta0_mle_val = mle_params[1]
beta_mle_val = mle_params[2]
lambda_mle_val = mle_params[3]
delta_mle_val = mle_params[4]

# Prior standard deviations using CV from config
N0_std = N0_mle_val * config['cv_N0']
Delta0_std = Delta0_mle_val * config['cv_Delta0']
beta_std = beta_mle_val * config['cv_beta']
lambda_std = abs(lambda_mle_val) * config['cv_lambda']
delta_std = delta_mle_val * config['cv_delta']

print("\nInformative Normal Priors (centered on MLE):")
print(f"  N‚ÇÄ     ~ Normal({N0_mle_val:.4f}, {N0_std:.4f})  [CV={config['cv_N0']:.2f}]")
print(f"  ŒîœÉ‚ÇÄ    ~ Normal({Delta0_mle_val:.6f}, {Delta0_std:.6f})  [CV={config['cv_Delta0']:.2f}]")
print(f"  Œ≤      ~ Normal({beta_mle_val:.4f}, {beta_std:.4f})  [CV={config['cv_beta']:.2f}]")
print(f"  Œª      ~ Normal({lambda_mle_val:.4f}, {lambda_std:.4f})  [CV={config['cv_lambda']:.2f}]")
print(f"  Œ¥      ~ Normal({delta_mle_val:.4f}, {delta_std:.4f})  [CV={config['cv_delta']:.2f}]")

print("\nThese priors will help achieve faster convergence in Bayesian inference.")

# ============================================================================
# PHASE 3: BAYESIAN INFERENCE WITH WEIBULL MODEL
# ============================================================================

print("\n" + "="*70)
print("PHASE 3: BAYESIAN INFERENCE")
print("="*70)

initial_values = {
    'N0': N0_mle_val,
    'Delta0': Delta0_mle_val,
    'beta': beta_mle_val,
    'lambda_param': lambda_mle_val,
    'delta': delta_mle_val
}

with pm.Model() as fatigue_model:
    # PRIORS - Informative Normal distributions based on MLE
    N0 = pm.TruncatedNormal('N0', mu=N0_mle_val, sigma=N0_std,
                            lower=0.001, upper=N_min)
    Delta0 = pm.TruncatedNormal('Delta0', mu=Delta0_mle_val, sigma=Delta0_std,
                                lower=stress_data.min() * 0.3, upper=stress_data.min())
    beta = pm.TruncatedNormal('beta', mu=beta_mle_val, sigma=beta_std,
                             lower=0.5, upper=20.0)
    lambda_param = pm.Normal('lambda_param', mu=lambda_mle_val, sigma=lambda_std)
    delta = pm.TruncatedNormal('delta', mu=delta_mle_val, sigma=delta_std,
                              lower=0.1, upper=10.0)

    # Transform to dimensionless log-space
    log_N_dimensionless = pt.log(cycles_data) - pt.log(N0)
    r = pt.log(stress_data) - pt.log(Delta0)

    # Weibull parameters for minima
    mu_Y = (-lambda_param - delta) / r
    sigma_Y = delta / (beta * pt.abs(r) + 1e-8)

    # Standardized variable
    z = (log_N_dimensionless - mu_Y) / (sigma_Y + 1e-8)

    # Log-likelihood (Weibull for minima, Gumbel parametrization)
    log_lik = -pt.log(sigma_Y + 1e-8) + z - pt.exp(z)

    # Total likelihood
    likelihood = pm.Potential('likelihood', pt.sum(log_lik))

print("‚úì Bayesian model defined with informative priors")

# ============================================================================
# PHASE 4: PRIOR PREDICTIVE CHECK
# ============================================================================

print("\n" + "="*70)
print("PRIOR PREDICTIVE CHECK")
print("="*70)

print("\nSampling from prior predictive distribution...")

with fatigue_model:
    prior_predictive = pm.sample_prior_predictive(
        samples=500,
        random_seed=RANDOM_SEED
    )

print("‚úì Prior predictive samples generated")

# Visualize prior distributions
print("\nVisualizing prior distributions...")

fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

var_names = ['N0', 'Delta0', 'beta', 'lambda_param', 'delta']
var_labels = ['N‚ÇÄ (reference cycles)', 'ŒîœÉ‚ÇÄ (reference stress)', 'Œ≤', 'Œª', 'Œ¥']

for ax, var, label in zip(axes[:5], var_names, var_labels):
    samples = prior_predictive.prior[var].values.flatten()

    ax.hist(samples, bins=50, alpha=0.7, color='steelblue', edgecolor='black')
    ax.set_xlabel(label, fontsize=11, fontweight='bold')
    ax.set_ylabel('Frequency', fontsize=11)
    ax.set_title(f'Prior: {label}', fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3)

    ax.axvline(np.median(samples), color='red', linestyle='--', linewidth=2, label='Median')

    # Add MLE value line
    if var == 'N0':
        ax.axvline(N0_mle_val, color='green', linestyle=':', linewidth=2, label='MLE')
    elif var == 'Delta0':
        ax.axvline(Delta0_mle_val, color='green', linestyle=':', linewidth=2, label='MLE')
    elif var == 'beta':
        ax.axvline(beta_mle_val, color='green', linestyle=':', linewidth=2, label='MLE')
    elif var == 'lambda_param':
        ax.axvline(lambda_mle_val, color='green', linestyle=':', linewidth=2, label='MLE')
    elif var == 'delta':
        ax.axvline(delta_mle_val, color='green', linestyle=':', linewidth=2, label='MLE')

    ax.legend()

axes[5].axis('off')

plt.suptitle(f'Prior Distributions (Centered on MLE) - {dataset_name}', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig('prior_distributions.png', dpi=150, bbox_inches='tight')
plt.show()

print("‚úì Prior distributions visualized")

# ============================================================================
# PHASE 5: SAMPLE FROM POSTERIOR
# ============================================================================

print("\n" + "="*70)
print("SAMPLING FROM POSTERIOR DISTRIBUTION")
print("="*70)

print("\nSampling strategy:")
print(f"  ‚Ä¢ Using informative priors from MLE")
print(f"  ‚Ä¢ Tune: {config['n_tune']}, Draws: {config['n_draws']}, Chains: {config['n_chains']}")
print(f"  ‚Ä¢ Target acceptance = {config['target_accept']}")
print("\nThis may take 5-15 minutes depending on configuration...\n")

with fatigue_model:
    trace = pm.sample(
        draws=config['n_draws'],
        tune=config['n_tune'],
        chains=config['n_chains'],
        cores=1,
        random_seed=RANDOM_SEED,
        return_inferencedata=True,
        target_accept=config['target_accept'],
        init='adapt_diag',
        initvals=initial_values
    )

print("\n‚úì Sampling completed!")

# ============================================================================
# PHASE 6: CONVERGENCE DIAGNOSTICS
# ============================================================================

print("\n" + "="*70)
print("CONVERGENCE DIAGNOSTICS")
print("="*70)

summary_table = az.summary(
    trace,
    var_names=['N0', 'Delta0', 'beta', 'lambda_param', 'delta'],
    hdi_prob=0.95
)
print("\n--- Posterior Summary ---")
print(summary_table)

rhat_values = az.rhat(trace)
print(f"\n--- R-hat Diagnostic (should be < 1.01) ---")
all_rhat_good = True
for var in ['N0', 'Delta0', 'beta', 'lambda_param', 'delta']:
    rhat_val = rhat_values[var].values
    status = "‚úì" if rhat_val < 1.01 else "‚úó"
    if rhat_val >= 1.01:
        all_rhat_good = False
    print(f"{var:15s}: {rhat_val:.4f} {status}")

ess_values = az.ess(trace)
print(f"\n--- Effective Sample Size (should be > 1000) ---")
all_ess_good = True
for var in ['N0', 'Delta0', 'beta', 'lambda_param', 'delta']:
    ess_val = ess_values[var].values
    status = "‚úì" if ess_val > 1000 else "‚úó"
    if ess_val <= 1000:
        all_ess_good = False
    print(f"{var:15s}: {ess_val:.0f} {status}")

n_divergences = trace.sample_stats.diverging.sum().values
total_samples = config['n_draws'] * config['n_chains']
print(f"\n--- Divergence Check ---")
print(f"Number of divergent transitions: {n_divergences}")
if n_divergences > 0:
    print(f"‚ö† Warning: {n_divergences} divergences detected")
    print(f"  Divergence rate: {n_divergences / total_samples:.2%}")
else:
    print("‚úì No divergences detected!")

print(f"\n--- Overall Convergence Assessment ---")
if all_rhat_good and all_ess_good and n_divergences == 0:
    print("‚úì‚úì‚úì EXCELLENT: Model has converged successfully!")
elif all_rhat_good and n_divergences < 50:
    print("‚úì‚úì GOOD: Model convergence is acceptable")
else:
    print("‚úó WARNING: Model may not have converged properly")

# Trace plots
print("\nGenerating trace plots...")
fig, axes = plt.subplots(5, 2, figsize=(14, 16))
az.plot_trace(
    trace,
    var_names=['N0', 'Delta0', 'beta', 'lambda_param', 'delta'],
    compact=False,
    axes=axes
)
plt.suptitle(f'Trace Plots and Posterior Distributions - {dataset_name}',
            fontsize=14, fontweight='bold', y=1.001)
plt.tight_layout()
plt.savefig('trace_plots.png', dpi=150, bbox_inches='tight')
plt.show()

# Posterior distributions
fig = plt.figure(figsize=(15, 10))
az.plot_posterior(
    trace,
    var_names=['N0', 'Delta0', 'beta', 'lambda_param', 'delta'],
    hdi_prob=0.95,
    figsize=(15, 10)
)
plt.suptitle(f'Posterior Distributions with 95% HDI - {dataset_name}',
            fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig('posterior_distributions.png', dpi=150, bbox_inches='tight')
plt.show()

print("\n‚úì Diagnostics completed!")

# ============================================================================
# PHASE 7: PERCENTILES OF PERCENTILES
# ============================================================================

print("\n" + "="*70)
print("COMPUTING PERCENTILES OF PERCENTILES")
print("="*70)

posterior = trace.posterior

# Define stress range for plotting
stress_margin = 0.03
stress_min_plot = stress_data.min() - stress_margin
stress_max_plot = stress_data.max() + stress_margin
stress_min_plot = max(stress_min_plot, 0.01)

stress_range = np.linspace(stress_min_plot, stress_max_plot, config['n_stress_points'])

percentiles_base = config['percentiles_base']
percentiles_sub = config['percentiles_sub']

print(f"\nConfiguration:")
print(f"  Stress points: {config['n_stress_points']}")
print(f"  Parameter samples: {config['n_param_samples']}")
print(f"  Base percentiles: {[int(p*100) for p in percentiles_base]}%")
print(f"  Sub-percentiles: {[int(p*100) for p in percentiles_sub]}%")

def compute_percentile(stress, N0, Delta0, beta, lambda_p, delta, prob):
    """Compute N_p for given stress and failure probability (Weibull)."""
    if stress <= 0 or N0 <= 0 or Delta0 <= 0 or beta <= 0 or delta <= 0:
        return np.nan
    if prob <= 0 or prob >= 1:
        return np.nan

    try:
        r = np.log(stress / Delta0)
        if abs(r) < 1e-10:
            return np.nan

        mu_Y = (-lambda_p - delta) / r
        sigma_Y = delta / (beta * abs(r))

        # Weibull quantile for minima (Gumbel parametrization)
        z_p = np.log(-np.log(1 - prob))
        Y_p = mu_Y + sigma_Y * z_p
        N_p = N0 * np.exp(Y_p)

        if not np.isfinite(N_p) or N_p <= 0:
            return np.nan

        if N_p < 1e-6 or N_p > 1e10:
            return np.nan

        return N_p
    except:
        return np.nan

# Extract posterior samples
N0_samples = posterior['N0'].values.flatten()
Delta0_samples = posterior['Delta0'].values.flatten()
beta_samples = posterior['beta'].values.flatten()
lambda_samples = posterior['lambda_param'].values.flatten()
delta_samples = posterior['delta'].values.flatten()

total_samples = len(N0_samples)
sample_indices = np.random.choice(total_samples, size=min(config['n_param_samples'], total_samples), replace=False)

# Storage for percentiles of percentiles
percentiles_of_percentiles = {}

print("\nComputing percentiles of percentiles...")

for perc_base_idx, perc_base in enumerate(percentiles_base):
    print(f"\n  Processing base percentile P{int(perc_base*100)}...")

    percentile_matrix = np.zeros((config['n_stress_points'], len(sample_indices)))

    for i, stress in enumerate(stress_range):
        if i % max(1, config['n_stress_points']//5) == 0:
            print(f"    Stress point {i+1}/{config['n_stress_points']}...")

        for j, idx in enumerate(sample_indices):
            N0_s = N0_samples[idx]
            Delta0_s = Delta0_samples[idx]
            beta_s = beta_samples[idx]
            lambda_s = lambda_samples[idx]
            delta_s = delta_samples[idx]

            N_p = compute_percentile(stress, N0_s, Delta0_s, beta_s, lambda_s, delta_s, perc_base)

            if not np.isnan(N_p):
                percentile_matrix[i, j] = N_p
            else:
                percentile_matrix[i, j] = np.nan

    n_valid = np.sum(~np.isnan(percentile_matrix), axis=1)
    print(f"    Valid values per stress: min={n_valid.min()}, max={n_valid.max()}, mean={n_valid.mean():.1f}")

    # Sort each row
    percentile_matrix_sorted = np.sort(percentile_matrix, axis=1)

    # Extract sub-percentiles
    percentile_indices = [int(p * (len(sample_indices) - 1)) for p in percentiles_sub]
    perc_of_perc_curves = percentile_matrix_sorted[:, percentile_indices]

    percentiles_of_percentiles[perc_base] = perc_of_perc_curves

print("\n‚úì Percentiles of percentiles computed!")

# ============================================================================
# PHASE 8: PLOT WITH SHADED REGIONS
# ============================================================================

print("\nPlotting percentiles of percentiles with shaded regions...")

fig, ax = plt.subplots(figsize=(18, 11), facecolor='white')
ax.set_facecolor('white')

# Colors for shading and lines
colors_base = ['#8B0000', '#FF8C00', '#228B22', '#4169E1', '#8B008B']
colors_shaded = ['#FFB6B9', '#FFCC80', '#A5D6A7', '#90CAF9', '#CE93D8']

# Generate labels for percentiles
perc_names = [f'P{int(p*100)}' for p in percentiles_base]

# Ensure we have enough colors
if len(percentiles_base) > len(colors_base):
    colors_base = plt.cm.rainbow(np.linspace(0, 1, len(percentiles_base)))
    colors_shaded = plt.cm.rainbow(np.linspace(0, 1, len(percentiles_base)))

# FIRST: Plot shaded regions
print("  Plotting shaded uncertainty bands...")
for base_idx, (perc_base, perc_name) in enumerate(zip(percentiles_base, perc_names)):
    curves = percentiles_of_percentiles[perc_base]

    curve_p_min = curves[:, 0]  # First sub-percentile
    curve_p_max = curves[:, -1]  # Last sub-percentile

    valid_mask = (~np.isnan(curve_p_min)) & (~np.isnan(curve_p_max))

    if np.sum(valid_mask) > 3:
        color_shaded = colors_shaded[base_idx % len(colors_shaded)]
        ax.fill_betweenx(stress_range[valid_mask],
                curve_p_min[valid_mask],
                curve_p_max[valid_mask],
                color=color_shaded,
                alpha=0.85,
                label=f'{perc_name} uncertainty band',
                zorder=base_idx + 1)

# SECOND: Plot intermediate sub-percentile curves
print("  Plotting intermediate sub-percentile curves...")
if len(percentiles_sub) > 2:
    for base_idx, (perc_base, perc_name) in enumerate(zip(percentiles_base, perc_names)):
        curves = percentiles_of_percentiles[perc_base]
        color_base = colors_base[base_idx % len(colors_base)]

        for sub_idx in range(1, len(percentiles_sub)-1):
            curve = curves[:, sub_idx]
            valid_mask = ~np.isnan(curve)
            if np.sum(valid_mask) > 3:
                linestyle = '--' if sub_idx == 1 else ':'
                ax.plot(curve[valid_mask], stress_range[valid_mask],
                       color=color_base, linestyle=linestyle, linewidth=1.2,
                       alpha=0.5, zorder=10 + base_idx)

# THIRD: Plot extreme sub-percentiles
print("  Plotting extreme sub-percentile curves...")
for base_idx, (perc_base, perc_name) in enumerate(zip(percentiles_base, perc_names)):
    curves = percentiles_of_percentiles[perc_base]
    color_base = colors_base[base_idx % len(colors_base)]

    curve_p_min = curves[:, 0]
    valid_mask = ~np.isnan(curve_p_min)
    if np.sum(valid_mask) > 3:
        ax.plot(curve_p_min[valid_mask], stress_range[valid_mask],
               color=color_base, linestyle=(0, (5, 2)), linewidth=1.5,
               alpha=0.7, zorder=15 + base_idx)

    curve_p_max = curves[:, -1]
    valid_mask = ~np.isnan(curve_p_max)
    if np.sum(valid_mask) > 3:
        ax.plot(curve_p_max[valid_mask], stress_range[valid_mask],
               color=color_base, linestyle=(0, (1, 1)), linewidth=1.5,
               alpha=0.7, zorder=15 + base_idx)

# FOURTH: Plot median curves - THICK
print("  Plotting median curves...")
median_idx = len(percentiles_sub) // 2
for base_idx, (perc_base, perc_name) in enumerate(zip(percentiles_base, perc_names)):
    curves = percentiles_of_percentiles[perc_base]
    color_base = colors_base[base_idx % len(colors_base)]

    curve_median = curves[:, median_idx]

    valid_mask = ~np.isnan(curve_median)

    if np.sum(valid_mask) > 3:
        ax.plot(curve_median[valid_mask], stress_range[valid_mask],
               color=color_base, linewidth=3.5,
               label=f'{perc_name} (median curve)',
               zorder=50 + base_idx)

# FIFTH: Plot observed data on TOP
print("  Plotting observed data...")
ax.scatter(cycles_data, stress_data, c='black', s=80,
          alpha=0.9, label='Observed data', zorder=100, marker='o',
          edgecolors='white', linewidths=1.5)

cycles_min_plot = N_min * 0.1
cycles_max_plot = N_max * 10.0

ax.set_xlim([cycles_min_plot, cycles_max_plot])

ax.set_xlabel('Cycles to Failure (N)', fontsize=15, fontweight='bold')
ax.set_ylabel('Stress (Deltasigma, dimensionless)', fontsize=15, fontweight='bold')
ax.set_xscale('log')
ax.set_title(f'Percentiles of Percentiles with Uncertainty Bands ({dataset_name})\n' +
            'Bayesian Weibull Model - Castillo-Canteli Formulation',
            fontsize=15, fontweight='bold', pad=20)

# Legend
ax.legend(loc='upper right', fontsize=10, framealpha=0.98, ncol=2,
         columnspacing=1.0, handlelength=2.5,
         title='Base Percentiles & Uncertainty Bands',
         title_fontsize=11)

ax.grid(True, alpha=0.3, which='both', linestyle='-', linewidth=0.5)
ax.set_xlim([cycles_min_plot, cycles_max_plot])
ax.set_ylim([stress_min_plot - 0.01, stress_max_plot + 0.01])

plt.tight_layout()
plt.savefig('percentiles_of_percentiles_shaded.png', dpi=250, bbox_inches='tight')
plt.show()

print("‚úì Shaded plot completed!")

# ============================================================================
# PHASE 9: ALTERNATIVE PLOT - ALL CURVES
# ============================================================================

print("\nCreating alternative plot with all curves...")

fig, ax = plt.subplots(figsize=(18, 11), facecolor='white')
ax.set_facecolor('white')

# Plot shaded regions first
for base_idx, perc_base in enumerate(percentiles_base):
    curves = percentiles_of_percentiles[perc_base]
    curve_p_min = curves[:, 0]
    curve_p_max = curves[:, -1]
    valid_mask = (~np.isnan(curve_p_min)) & (~np.isnan(curve_p_max))

    if np.sum(valid_mask) > 3:
        color_shaded = colors_shaded[base_idx % len(colors_shaded)]
        ax.fill_betweenx(stress_range[valid_mask],
                        curve_p_min[valid_mask],
                        curve_p_max[valid_mask],
                        color=color_shaded,
                        alpha=0.5,
                        zorder=base_idx + 1)

# Plot ALL curves with labels
linestyles_sub = ['-', '--', '-', ':', (0, (1, 1))]
if len(percentiles_sub) > len(linestyles_sub):
    linestyles_sub = ['-'] * len(percentiles_sub)

linewidths_sub = [1.5] * len(percentiles_sub)
linewidths_sub[median_idx] = 3.5  # Median thicker

alpha_sub = [0.6] * len(percentiles_sub)
alpha_sub[median_idx] = 1.0  # Median fully opaque

curve_count = 0
for base_idx, (perc_base, perc_name_base) in enumerate(zip(percentiles_base, perc_names)):
    curves = percentiles_of_percentiles[perc_base]
    color_base = colors_base[base_idx % len(colors_base)]

    perc_names_sub = [f'P{int(p*100)}' for p in percentiles_sub]

    for sub_idx in range(len(percentiles_sub)):
        curve = curves[:, sub_idx]
        valid_mask = ~np.isnan(curve)

        if np.sum(valid_mask) > 3:
            if sub_idx == median_idx:
                label = f'{perc_name_base}-{perc_names_sub[sub_idx]} (median)'
                zorder_val = 50 + base_idx
            elif sub_idx in [0, len(percentiles_sub)-1]:
                label = f'{perc_name_base}-{perc_names_sub[sub_idx]} (bound)'
                zorder_val = 30 + base_idx
            else:
                label = f'{perc_name_base}-{perc_names_sub[sub_idx]}'
                zorder_val = 20 + base_idx

            ax.plot(curve[valid_mask], stress_range[valid_mask],
                   color=color_base, linestyle=linestyles_sub[sub_idx],
                   linewidth=linewidths_sub[sub_idx],
                   label=label, alpha=alpha_sub[sub_idx], zorder=zorder_val)
            curve_count += 1

# Plot observed data
ax.scatter(cycles_data, stress_data, c='black', s=80,
          alpha=0.9, label='Observed data', zorder=100, marker='o',
          edgecolors='white', linewidths=1.5)

total_curves = len(percentiles_base) * len(percentiles_sub)
print(f"  Total curves plotted: {curve_count}/{total_curves}")

ax.set_xlabel('Cycles to Failure (N)', fontsize=15, fontweight='bold')
ax.set_ylabel('Stress (Deltasigma, dimensionless)', fontsize=15, fontweight='bold')
ax.set_xscale('log')
ax.set_title(f'All {total_curves} Percentile Curves ({len(percentiles_base)} Base √ó {len(percentiles_sub)} Sub) - {dataset_name}\n' +
            'Complete Uncertainty Quantification',
            fontsize=15, fontweight='bold', pad=20)

ax.legend(loc='upper right', fontsize=8, framealpha=0.95, ncol=3,
         columnspacing=0.6, handlelength=2.0, handletextpad=0.5,
         title='Base-Sub Percentile Curves', title_fontsize=9,
         borderpad=0.5, labelspacing=0.3)

ax.grid(True, alpha=0.3, which='both', linestyle='-', linewidth=0.5)
ax.set_xlim([cycles_min_plot, cycles_max_plot])
ax.set_ylim([stress_min_plot - 0.01, stress_max_plot + 0.01])

plt.tight_layout()
plt.savefig('percentiles_of_percentiles_all_curves.png', dpi=250, bbox_inches='tight')
plt.show()

print("‚úì Alternative plot completed!")

# ============================================================================
# PHASE 10: SUMMARY
# ============================================================================

print("\n" + "="*70)
print("SUMMARY")
print("="*70)

print(f"\nDataset: {dataset_name}")
print(f"Observations: {len(cycles_data)}")

print("\n1. MLE ESTIMATES:")
print(f"   N‚ÇÄ = {mle_params[0]:.6f}")
print(f"   ŒîœÉ‚ÇÄ = {mle_params[1]:.6f}")
print(f"   Œ≤ = {mle_params[2]:.3f}")
print(f"   Œª = {mle_params[3]:.3f}")
print(f"   Œ¥ = {mle_params[4]:.3f}")

print("\n2. POSTERIOR MEDIANS:")
print(f"   N‚ÇÄ = {posterior['N0'].median().values:.6f}")
print(f"   ŒîœÉ‚ÇÄ = {posterior['Delta0'].median().values:.6f}")
print(f"   Œ≤ = {posterior['beta'].median().values:.3f}")
print(f"   Œª = {posterior['lambda_param'].median().values:.3f}")
print(f"   Œ¥ = {posterior['delta'].median().values:.3f}")

print("\n3. CONVERGENCE:")
print(f"   R-hat all < 1.01: {'‚úì' if all_rhat_good else '‚úó'}")
print(f"   ESS all > 1000: {'‚úì' if all_ess_good else '‚úó'}")
print(f"   Divergences: {n_divergences}")
if n_divergences > 0:
    print(f"   Divergence rate: {n_divergences/total_samples*100:.2f}%")

print("\n4. PERCENTILES OF PERCENTILES:")
print(f"   Total curves generated: {len(percentiles_base) * len(percentiles_sub)} ({len(percentiles_base)} base √ó {len(percentiles_sub)} sub)")
print(f"   Two visualization approaches:")
print(f"     ‚Ä¢ Main plot: Shaded bands with key curves")
print(f"     ‚Ä¢ Alternative: All curves individually labeled")

print("\n5. FILES CREATED:")
print("   ‚Ä¢ data_exploration_weibull.png")
print("   ‚Ä¢ mle_results.png")
print("   ‚Ä¢ mle_estimates.txt")
print("   ‚Ä¢ prior_distributions.png")
print("   ‚Ä¢ trace_plots.png")
print("   ‚Ä¢ posterior_distributions.png")
print("   ‚Ä¢ percentiles_of_percentiles_shaded.png")
print("   ‚Ä¢ percentiles_of_percentiles_all_curves.png")

print("\n" + "="*70)
print("ANALYSIS COMPLETE!")
print("="*70)

# Save results
trace.to_netcdf('fatigue_posterior_weibull.nc')
summary_table.to_csv('fatigue_summary_weibull.csv')

perc_of_perc_data = {}
for perc_base in percentiles_base:
    perc_of_perc_data[f'P{int(perc_base*100)}'] = percentiles_of_percentiles[perc_base]

np.savez('percentiles_of_percentiles.npz',
         stress_range=stress_range,
         percentiles_base=percentiles_base,
         percentiles_sub=percentiles_sub,
         **perc_of_perc_data)

print("\n‚úì Results saved:")
print("  ‚Ä¢ fatigue_posterior_weibull.nc")
print("  ‚Ä¢ fatigue_summary_weibull.csv")
print("  ‚Ä¢ percentiles_of_percentiles.npz")

# ============================================================================
# PHASE 11: SYNTHETIC DATA GENERATION FROM POSTERIOR SAMPLES
# ============================================================================

print("\n" + "="*70)
print("SYNTHETIC DATA GENERATION FROM POSTERIOR SAMPLES")
print("="*70)

print("\nWould you like to generate synthetic datasets using posterior samples?")
print("(This uses the actual MCMC samples, excluding warmup)")
generate_synthetic = input("\nGenerate synthetic data? (y/n): ").strip().lower()

if generate_synthetic == 'y':

    # Ask for number of observations per dataset
    while True:
        try:
            n_obs_per_dataset = int(input("\nHow many observations per synthetic dataset? (e.g., 75, 100, 360): ").strip())
            if n_obs_per_dataset > 0 and n_obs_per_dataset <= 20000:
                break
            else:
                print("Please enter a number between 1 and 20000")
        except ValueError:
            print("Invalid input. Please enter a number.")

    # Ask for number of datasets to generate
    while True:
        try:
            n_datasets = int(input("\nHow many synthetic datasets to generate? (e.g., 1, 10, 100): ").strip())
            if n_datasets > 0 and n_datasets <= 20000:
                break
            else:
                print("Please enter a number between 1 and 20000")
        except ValueError:
            print("Invalid input. Please enter a number.")

    print(f"\n{'='*70}")
    print(f"CONFIGURATION:")
    print(f"  Observations per dataset: {n_obs_per_dataset}")
    print(f"  Number of datasets: {n_datasets}")
    print(f"  Total posterior samples available: {len(N0_samples)}")
    print(f"{'='*70}")

    # Check if we have enough samples
    if n_datasets > len(N0_samples):
        print(f"\n‚ö† Warning: Requested {n_datasets} datasets but only {len(N0_samples)} posterior samples available.")
        print(f"  Will generate {min(n_datasets, len(N0_samples))} datasets (one per unique sample).")
        n_datasets = min(n_datasets, len(N0_samples))

    # Select random posterior samples (one per dataset)
    selected_sample_indices = np.random.choice(len(N0_samples), size=n_datasets, replace=False)

    print(f"\n‚úì Selected {n_datasets} random posterior samples")

    # Create directory for synthetic data
    import os
    synthetic_dir = f'synthetic_data_{dataset_name}'
    if not os.path.exists(synthetic_dir):
        os.makedirs(synthetic_dir)
    print(f"‚úì Created directory: {synthetic_dir}/")

    # Generate synthetic datasets
    print(f"\nGenerating {n_datasets} synthetic datasets...")

    all_synthetic_data = []

    for dataset_idx, sample_idx in enumerate(selected_sample_indices):
        if (dataset_idx + 1) % max(1, n_datasets // 10) == 0 or dataset_idx == 0:
            print(f"  Generating dataset {dataset_idx + 1}/{n_datasets}...")

        # Get parameter values from this posterior sample
        N0_synth = N0_samples[sample_idx]
        Delta0_synth = Delta0_samples[sample_idx]
        beta_synth = beta_samples[sample_idx]
        lambda_synth = lambda_samples[sample_idx]
        delta_synth = delta_samples[sample_idx]

        # Generate stress levels (uniform distribution within observed range)
        stress_synthetic = np.random.uniform(
            low=stress_data.min(),
            high=stress_data.max(),
            size=n_obs_per_dataset
        )

        # Generate cycles for each stress level using the Weibull model
        cycles_synthetic = np.zeros(n_obs_per_dataset)

        for i in range(n_obs_per_dataset):
            stress = stress_synthetic[i]

            # Weibull parameters for this stress level
            r = np.log(stress / Delta0_synth)
            mu_Y = (-lambda_synth - delta_synth) / r
            sigma_Y = delta_synth / (beta_synth * abs(r))

            # Generate random Gumbel (for minima) variable
            u = np.random.uniform(0, 1)
            z = np.log(-np.log(1 - u))  # Inverse CDF

            Y = mu_Y + sigma_Y * z
            N = N0_synth * np.exp(Y)

            cycles_synthetic[i] = N

        # Create DataFrame for this dataset
        synthetic_df = pd.DataFrame({
            'N': cycles_synthetic,
            'Deltasigma': stress_synthetic
        })

        # Sort by stress level
        synthetic_df = synthetic_df.sort_values('Deltasigma').reset_index(drop=True)

        # Store parameters used
        synthetic_df.attrs['N0'] = N0_synth
        synthetic_df.attrs['Delta0'] = Delta0_synth
        synthetic_df.attrs['beta'] = beta_synth
        synthetic_df.attrs['lambda'] = lambda_synth
        synthetic_df.attrs['delta'] = delta_synth
        synthetic_df.attrs['sample_idx'] = sample_idx

        all_synthetic_data.append(synthetic_df)

        # Save individual CSV file
        csv_filename = f'{synthetic_dir}/synthetic_{dataset_idx+1:04d}.csv'
        synthetic_df.to_csv(csv_filename, index=False)

        # Save individual R/OpenBUGS format file
        r_filename = f'{synthetic_dir}/synthetic_{dataset_idx+1:04d}.txt'
        with open(r_filename, 'w') as f:
            f.write(f"# Synthetic dataset {dataset_idx+1}/{n_datasets}\n")
            f.write(f"# Generated from posterior sample {sample_idx}\n")
            f.write(f"# Parameters: N0={N0_synth:.6f}, Delta0={Delta0_synth:.6f}, beta={beta_synth:.3f}, lambda={lambda_synth:.3f}, delta={delta_synth:.3f}\n")
            f.write(f"list(M={n_obs_per_dataset},\n")
            f.write(f"ns=50,\n")
            f.write(f"np=5,\n")
            f.write(f"percentiles=c({','.join([str(p) for p in config['percentiles_base']])}),\n")

            # Write Deltasigma
            f.write("Deltasigma=c(")
            deltasigma_str = ','.join([f"{s:.3f}" for s in synthetic_df['Deltasigma'].values])
            f.write(deltasigma_str)
            f.write("),\n")

            # Write N
            f.write("N=c(")
            n_str = ','.join([f"{n:.1f}" for n in synthetic_df['N'].values])
            f.write(n_str)
            f.write(")\n")
            f.write(")\n")

    print(f"\n‚úì Generated {n_datasets} synthetic datasets!")
    print(f"  Files saved in: {synthetic_dir}/")

    # Create summary file with parameters used
    params_summary = pd.DataFrame({
        'dataset': [f'synthetic_{i+1:04d}' for i in range(n_datasets)],
        'sample_idx': [df.attrs['sample_idx'] for df in all_synthetic_data],
        'N0': [df.attrs['N0'] for df in all_synthetic_data],
        'Delta0': [df.attrs['Delta0'] for df in all_synthetic_data],
        'beta': [df.attrs['beta'] for df in all_synthetic_data],
        'lambda': [df.attrs['lambda'] for df in all_synthetic_data],
        'delta': [df.attrs['delta'] for df in all_synthetic_data]
    })

    params_summary_file = f'{synthetic_dir}/parameters_summary.csv'
    params_summary.to_csv(params_summary_file, index=False)
    print(f"‚úì Parameters summary saved to: {params_summary_file}")

    # Create consolidated CSV with all datasets
    consolidated_data = []
    for idx, df in enumerate(all_synthetic_data):
        df_copy = df.copy()
        df_copy['dataset'] = idx + 1
        consolidated_data.append(df_copy)

    consolidated_df = pd.concat(consolidated_data, ignore_index=True)
    consolidated_file = f'{synthetic_dir}/all_synthetic_data.csv'
    consolidated_df.to_csv(consolidated_file, index=False)
    print(f"‚úì Consolidated data saved to: {consolidated_file}")

    # Summary statistics
    print("\n" + "="*70)
    print("SYNTHETIC DATA SUMMARY")
    print("="*70)

    print(f"\nGenerated datasets: {n_datasets}")
    print(f"Observations per dataset: {n_obs_per_dataset}")
    print(f"Total synthetic observations: {n_datasets * n_obs_per_dataset}")

    print(f"\nParameter ranges across datasets:")
    print(f"  N‚ÇÄ:     [{params_summary['N0'].min():.6f}, {params_summary['N0'].max():.6f}]")
    print(f"  ŒîœÉ‚ÇÄ:    [{params_summary['Delta0'].min():.6f}, {params_summary['Delta0'].max():.6f}]")
    print(f"  Œ≤:      [{params_summary['beta'].min():.3f}, {params_summary['beta'].max():.3f}]")
    print(f"  Œª:      [{params_summary['lambda'].min():.3f}, {params_summary['lambda'].max():.3f}]")
    print(f"  Œ¥:      [{params_summary['delta'].min():.3f}, {params_summary['delta'].max():.3f}]")

    # Visualize first few datasets
    print("\nVisualizing first 3 synthetic datasets vs observed data...")

    n_plots = min(3, n_datasets)
    fig, axes = plt.subplots(1, n_plots + 1, figsize=(5 * (n_plots + 1), 5))

    if n_plots == 1:
        axes = [axes]

    # Plot observed data
    ax = axes[0]
    ax.scatter(cycles_data, stress_data, c='blue', s=50, alpha=0.7,
              label='Observed', edgecolors='black', linewidths=0.5)
    ax.set_xlabel('Cycles (N)', fontsize=10, fontweight='bold')
    ax.set_ylabel('Stress (ŒîœÉ)', fontsize=10, fontweight='bold')
    ax.set_xscale('log')
    ax.set_title(f'Observed Data\n({len(cycles_data)} obs)', fontsize=11, fontweight='bold')
    ax.grid(True, alpha=0.3, which='both')
    ax.legend(fontsize=9)

    # Plot first few synthetic datasets
    colors = ['red', 'green', 'orange']
    for i in range(n_plots):
        ax = axes[i + 1]
        df = all_synthetic_data[i]
        ax.scatter(df['N'].values, df['Deltasigma'].values, c=colors[i], s=30, alpha=0.6,
                  label=f'Synthetic {i+1}', marker='^', edgecolors='black', linewidths=0.5)
        ax.set_xlabel('Cycles (N)', fontsize=10, fontweight='bold')
        ax.set_ylabel('Stress (ŒîœÉ)', fontsize=10, fontweight='bold')
        ax.set_xscale('log')
        ax.set_title(f'Synthetic Dataset {i+1}\n({len(df)} obs)', fontsize=11, fontweight='bold')
        ax.grid(True, alpha=0.3, which='both')
        ax.legend(fontsize=9)

    plt.tight_layout()
    comparison_plot = f'{synthetic_dir}/synthetic_vs_observed_comparison.png'
    plt.savefig(comparison_plot, dpi=150, bbox_inches='tight')
    plt.show()
    print(f"‚úì Comparison plot saved to: {comparison_plot}")

    # Create ZIP file with all synthetic data
    print("\nCreating ZIP archive with all synthetic data...")
    import zipfile

    zip_filename = f'{synthetic_dir}.zip'
    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
        # Add all files in the synthetic directory
        for root, dirs, files in os.walk(synthetic_dir):
            for file in files:
                file_path = os.path.join(root, file)
                arcname = os.path.relpath(file_path, os.path.dirname(synthetic_dir))
                zipf.write(file_path, arcname)

    print(f"‚úì ZIP archive created: {zip_filename}")
    print(f"  Size: {os.path.getsize(zip_filename) / (1024*1024):.2f} MB")

    # Download in Colab
    if IN_COLAB:
        print("\nDownloading ZIP file...")
        try:
            files.download(zip_filename)
            print(f"  ‚úì Downloaded: {zip_filename}")
        except Exception as e:
            print(f"  ‚úó Could not download: {e}")
            print(f"  Files are available in: {synthetic_dir}/")
    else:
        print(f"\n‚úì Files ready in: {synthetic_dir}/")
        print(f"‚úì ZIP archive: {zip_filename}")

    print("\n" + "="*70)
    print("FILES IN ZIP ARCHIVE:")
    print("="*70)
    print(f"  ‚Ä¢ synthetic_XXXX.csv ({n_datasets} files) - Individual datasets in CSV format")
    print(f"  ‚Ä¢ synthetic_XXXX.txt ({n_datasets} files) - Individual datasets in R/OpenBUGS format")
    print(f"  ‚Ä¢ all_synthetic_data.csv - All datasets combined")
    print(f"  ‚Ä¢ parameters_summary.csv - Parameters used for each dataset")
    print(f"  ‚Ä¢ synthetic_vs_observed_comparison.png - Visual comparison")

    print("\n‚úì Synthetic data generation complete!")

else:
    print("\n‚úì Skipping synthetic data generation")

# Download files in Colab
if IN_COLAB:
    print("\n" + "="*70)
    print("DOWNLOAD FILES (Google Colab)")
    print("="*70)
    print("\nDownloading all result files...")

    files_to_download = [
        'data_exploration_weibull.png',
        'mle_results.png',
        'mle_estimates.txt',
        'prior_distributions.png',
        'trace_plots.png',
        'posterior_distributions.png',
        'percentiles_of_percentiles_shaded.png',
        'percentiles_of_percentiles_all_curves.png',
        'fatigue_posterior_weibull.nc',
        'fatigue_summary_weibull.csv',
        'percentiles_of_percentiles.npz'
    ]

    for fname in files_to_download:
        if os.path.exists(fname):
            try:
                files.download(fname)
                print(f"  ‚úì Downloaded: {fname}")
            except:
                print(f"  ‚úó Could not download: {fname}")

    print("\n‚úì Download complete!")

# ============================================================================
#   FATIGUE ANALYSIS - WEIBULL MODEL (MLE + BAYESIAN)
#   Castillo-Canteli Dimensionless Formulation
#   WITH GOOGLE COLAB FILE UPLOAD SUPPORT
# ============================================================================
# Physical justification:
# - Lower bounded problem (N > 0) ‚Üí Weibull distribution for minima
# - Gumbel is limiting case (Œ≤ ‚Üí ‚àû) naturally captured by Weibull
# ============================================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pymc as pm
import arviz as az
import pytensor.tensor as pt
from scipy import stats
from scipy.optimize import minimize, differential_evolution
import warnings
import sys
import os
import glob

warnings.filterwarnings('ignore')

plt.rcParams['figure.figsize'] = [14, 8]

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

print("="*70)
print("FATIGUE ANALYSIS - WEIBULL MODEL")
print("Maximum Likelihood Estimation + Bayesian Inference")
print("="*70)
print(f"PyMC version: {pm.__version__}")
print(f"ArviZ version: {az.__version__}")

# ============================================================================
# DETECT GOOGLE COLAB ENVIRONMENT
# ============================================================================

try:
    from google.colab import files
    IN_COLAB = True
    print("\n‚úì Google Colab detected")
except:
    IN_COLAB = False
    print("\n‚úì Running in local environment")

# ============================================================================
# CLEAN PREVIOUS OUTPUT FILES
# ============================================================================

print("\n" + "="*70)
print("CLEANING PREVIOUS OUTPUT FILES")
print("="*70)

output_patterns = [
    'data_exploration*.png',
    'mle_results*.png',
    'prior_distributions*.png',
    'trace_plots*.png',
    'posterior_distributions*.png',
    'percentiles_of_percentiles*.png',
    'fatigue_posterior*.nc',
    'fatigue_summary*.csv',
    'percentiles*.npz',
    'mle_estimates.txt'
]

# Check for existing files
existing_files = []
for pattern in output_patterns:
    existing_files.extend(glob.glob(pattern))

if len(existing_files) > 0:
    print(f"\nFound {len(existing_files)} existing output files:")
    for f in existing_files[:10]:  # Show first 10
        print(f"  - {f}")
    if len(existing_files) > 10:
        print(f"  ... and {len(existing_files) - 10} more")

    # Ask for confirmation
    confirm = input("\nDelete all previous output files? (y/n): ").strip().lower()

    if confirm == 'y':
        removed_count = 0
        for filepath in existing_files:
            try:
                os.remove(filepath)
                removed_count += 1
            except Exception as e:
                print(f"  Could not remove {filepath}: {e}")
        print(f"\n‚úì Removed {removed_count} files")
    else:
        print("\n‚úì Keeping existing files (will be overwritten)")
else:
    print("  No previous output files found")

# ============================================================================
# DATA SOURCE SELECTION
# ============================================================================

print("\n" + "="*70)
print("DATA SOURCE SELECTION")
print("="*70)

def get_user_choice():
    """Get user choice for data source."""
    print("\nChoose data source:")
    print("  1 - Use Holmen example data")
    print("  2 - Upload your own data file")

    while True:
        choice = input("\nEnter choice (1 or 2): ").strip()
        if choice in ['1', '2']:
            return choice
        print("Invalid choice. Please enter 1 or 2.")

# Determine data source
if IN_COLAB:
    try:
        data_choice = get_user_choice()
    except (EOFError, KeyboardInterrupt):
        print("\nInput not available, defaulting to file upload mode")
        data_choice = '2'
else:
    print("\nNot in Colab - defaulting to Holmen example")
    print("To use your own data, set data_choice = '2' before this section")
    data_choice = '1'

use_example = (data_choice == '1')

print(f"\nSelected mode: {'Example (Holmen)' if use_example else 'Upload file'}")

# ============================================================================
# DATA LOADING
# ============================================================================

print("\n" + "="*70)
print("DATA LOADING")
print("="*70)

if use_example:
    print("\n‚úì Using Holmen example data...")

    # Holmen example data
    stress_data = np.array([
        0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95,
        0.95, 0.95, 0.95, 0.95, 0.95,
        0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90,
        0.90, 0.90, 0.90, 0.90, 0.90,
        0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825,
        0.825, 0.825, 0.825, 0.825, 0.825,
        0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75,
        0.75, 0.75, 0.75, 0.75, 0.75,
        0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675,
        0.675, 0.675, 0.675, 0.675, 0.675,
    ])

    cycles_data = np.array([
        0.037, 0.072, 0.074, 0.076, 0.083, 0.085, 0.105, 0.109, 0.120, 0.123, 0.143,
        0.203, 0.206, 0.217, 0.257, 0.201, 0.216, 0.226, 0.252, 0.257, 0.295, 0.311,
        0.342, 0.356, 0.451, 0.457, 0.509, 0.540, 0.680, 1.129, 1.246, 1.258, 1.460,
        1.492, 2.400, 2.410, 2.590, 2.903, 3.330, 3.590, 3.847, 4.110, 4.820, 5.560,
        5.598, 6.710, 9.930, 12.600, 15.580, 16.190, 17.280, 18.620, 20.300, 24.900,
        26.260, 27.940, 36.350, 48.420, 50.090, 67.340, 102.950, 280.320, 339.830,
        366.900, 485.620, 658.960, 896.330, 1241.760, 1250.200, 1329.780, 1399.830,
        1459.140, 3294.820, 12709, 14373
    ])

    # Default configuration for Holmen
    config = {
        'n_tune': 1000,
        'n_draws': 2000,
        'n_chains': 2,
        'n_stress_points': 50,
        'n_param_samples': 1000,
        'percentiles_base': [0.01, 0.15, 0.50, 0.85, 0.99],
        'percentiles_sub': [0.01, 0.15, 0.50, 0.85, 0.99],
        'cv_N0': 0.25,
        'cv_Delta0': 0.15,
        'cv_beta': 0.30,
        'cv_lambda': 0.25,
        'cv_delta': 0.30,
        'target_accept': 0.95
    }

    dataset_name = "Holmen"

else:
    print("\nüìÅ Please upload your data file...")
    print("\nFile format requirements:")
    print("  Option 1: CSV or Excel file (.csv, .xlsx, .xls)")
    print("    - Required columns: 'N' (cycles), 'Deltasigma' (stress)")
    print("    - Optional: configuration parameters")
    print("\n  Option 2: R/OpenBUGS format (.txt, .dat)")
    print("    - list(M=..., N=c(...), Deltasigma=c(...), ...)")

    if IN_COLAB:
        print("\nUploading file...")
        uploaded = files.upload()

        if len(uploaded) == 0:
            print("No file uploaded. Using Holmen example instead.")
            use_example = True
        else:
            filename = list(uploaded.keys())[0]
            print(f"\n‚úì File uploaded: {filename}")
            print(f"  File size: {len(uploaded[filename])} bytes")

            # In Colab, the file is already in the current directory
            # Verify it exists
            import os
            if not os.path.exists(filename):
                print(f"  ‚úó Warning: File not found at {filename}")
                print(f"  Current directory: {os.getcwd()}")
                print(f"  Files in directory: {os.listdir('.')[:10]}")
            else:
                print(f"  ‚úì File verified at: {os.path.abspath(filename)}")
    else:
        filename = input("\nEnter data file path: ").strip()
        if not os.path.exists(filename):
            print(f"File not found: {filename}")
            print("Using Holmen example instead.")
            use_example = True

    if not use_example:
        # Read data file
        try:
            # Check file extension
            if filename.endswith(('.txt', '.dat')):
                # R/OpenBUGS format parser
                print("\n  Detected R/OpenBUGS format...")
                print(f"  Reading file: {filename}")

                with open(filename, 'r') as f:
                    content = f.read()

                print(f"  File size: {len(content)} characters")

                # Parse R list format
                import re

                # Extract N values
                n_match = re.search(r'N\s*=\s*c\(([^)]+)\)', content)
                if not n_match:
                    raise ValueError("Could not find 'N=c(...)' in file")
                n_values_str = n_match.group(1)
                # Remove all whitespace and split by comma, handle decimals
                n_values = [float(x.strip()) for x in n_values_str.split(',') if x.strip()]

                # Extract Deltasigma values
                delta_match = re.search(r'Deltasigma\s*=\s*c\(([^)]+)\)', content)
                if not delta_match:
                    raise ValueError("Could not find 'Deltasigma=c(...)' in file")
                delta_values_str = delta_match.group(1)
                # Remove all whitespace and split by comma, handle decimals
                delta_values = [float(x.strip()) for x in delta_values_str.split(',') if x.strip()]

                if len(n_values) != len(delta_values):
                    raise ValueError(f"Length mismatch: N has {len(n_values)} values, Deltasigma has {len(delta_values)}")

                cycles_data = np.array(n_values)
                stress_data = np.array(delta_values)

                print(f"  ‚úì Parsed N: {len(cycles_data)} values")
                print(f"  ‚úì Parsed Deltasigma: {len(stress_data)} values")
                print(f"  ‚úì Created numpy arrays successfully")
                print(f"    cycles_data shape: {cycles_data.shape}")
                print(f"    stress_data shape: {stress_data.shape}")

                # Default configuration
                config = {
                    'n_tune': 1000,
                    'n_draws': 1000,
                    'n_chains': 1,
                    'n_stress_points': 50,
                    'n_param_samples': 1000,
                    'percentiles_base': [0.01, 0.15, 0.50, 0.85, 0.99],
                    'percentiles_sub': [0.01, 0.15, 0.50, 0.85, 0.99],
                    'cv_N0': 0.25,
                    'cv_Delta0': 0.15,
                    'cv_beta': 0.30,
                    'cv_lambda': 0.25,
                    'cv_delta': 0.30,
                    'target_accept': 0.95
                }

                # Try to extract optional parameters from R list
                # M (number of observations)
                m_match = re.search(r'M\s*=\s*(\d+)', content)
                if m_match:
                    M_val = int(m_match.group(1))
                    print(f"  ‚úì Found M={M_val} (verification: {len(cycles_data)} observations)")

                # ns (number of stress points)
                ns_match = re.search(r'ns\s*=\s*(\d+)', content)
                if ns_match:
                    config['n_stress_points'] = int(ns_match.group(1))
                    print(f"  ‚úì Found ns={config['n_stress_points']}")

                # np (number of percentiles)
                np_match = re.search(r'np\s*=\s*(\d+)', content)
                if np_match:
                    np_val = int(np_match.group(1))
                    print(f"  ‚úì Found np={np_val}")

                # percentiles
                perc_match = re.search(r'percentiles\s*=\s*c\(([\d.,\s]+)\)', content)
                if perc_match:
                    perc_values = [float(x.strip()) for x in perc_match.group(1).split(',')]
                    config['percentiles_base'] = perc_values
                    config['percentiles_sub'] = perc_values
                    print(f"  ‚úì Found percentiles: {perc_values}")

                # n_tune (warmup)
                tune_match = re.search(r'n_tune\s*=\s*(\d+)', content)
                if tune_match:
                    config['n_tune'] = int(tune_match.group(1))
                    print(f"  ‚úì Found n_tune={config['n_tune']}")

                # n_draws (samples)
                draws_match = re.search(r'n_draws\s*=\s*(\d+)', content)
                if draws_match:
                    config['n_draws'] = int(draws_match.group(1))
                    print(f"  ‚úì Found n_draws={config['n_draws']}")

                # n_chains
                chains_match = re.search(r'n_chains\s*=\s*(\d+)', content)
                if chains_match:
                    config['n_chains'] = int(chains_match.group(1))
                    print(f"  ‚úì Found n_chains={config['n_chains']}")

                # Coefficient of variation parameters
                cv_params = ['cv_N0', 'cv_Delta0', 'cv_beta', 'cv_lambda', 'cv_delta']
                for cv_param in cv_params:
                    cv_match = re.search(rf'{cv_param}\s*=\s*([\d.]+)', content)
                    if cv_match:
                        config[cv_param] = float(cv_match.group(1))
                        print(f"  ‚úì Found {cv_param}={config[cv_param]}")

                # target_accept
                accept_match = re.search(r'target_accept\s*=\s*([\d.]+)', content)
                if accept_match:
                    config['target_accept'] = float(accept_match.group(1))
                    print(f"  ‚úì Found target_accept={config['target_accept']}")

                dataset_name = filename.split('.')[0]

                print(f"\n  ‚úì‚úì‚úì R FORMAT FILE LOADED SUCCESSFULLY")
                print(f"      Dataset: {dataset_name}")
                print(f"      use_example = {use_example}")
                print(f"      Variables 'cycles_data' and 'stress_data' created")

            elif filename.endswith('.csv'):
                df = pd.read_csv(filename)
            elif filename.endswith(('.xlsx', '.xls')):
                df = pd.read_excel(filename)
            else:
                raise ValueError("Unsupported file format. Use .csv, .xlsx, .xls, .txt, or .dat")

            # CSV/Excel format (if not R format)
            if filename.endswith(('.csv', '.xlsx', '.xls')):
                # Extract required columns
                if 'N' not in df.columns or 'Deltasigma' not in df.columns:
                    raise ValueError("File must contain 'N' and 'Deltasigma' columns")

                cycles_data = df['N'].values
                stress_data = df['Deltasigma'].values

                # Remove NaN values
                valid_mask = ~(np.isnan(cycles_data) | np.isnan(stress_data))
                cycles_data = cycles_data[valid_mask]
                stress_data = stress_data[valid_mask]

                print(f"\n‚úì Data loaded: {len(cycles_data)} observations")

                # Try to read configuration parameters
                config = {
                    'n_tune': 2000,
                    'n_draws': 5000,
                    'n_chains': 1,
                    'n_stress_points': 50,
                    'n_param_samples': 1000,
                    'percentiles_base': [0.01, 0.15, 0.50, 0.85, 0.99],
                    'percentiles_sub': [0.01, 0.15, 0.50, 0.85, 0.99],
                    'cv_N0': 0.25,
                    'cv_Delta0': 0.15,
                    'cv_beta': 0.30,
                    'cv_lambda': 0.25,
                    'cv_delta': 0.30,
                    'target_accept': 0.95
                }

                # Check for configuration in the file
                config_params = {
                    'n_tune': ['n_tune', 'tune', 'warmup'],
                    'n_draws': ['n_draws', 'draws', 'samples'],
                    'n_chains': ['n_chains', 'chains'],
                    'n_stress_points': ['n_stress_points', 'stress_points'],
                    'n_param_samples': ['n_param_samples', 'param_samples'],
                    'cv_N0': ['cv_N0', 'cv_n0'],
                    'cv_Delta0': ['cv_Delta0', 'cv_delta0'],
                    'cv_beta': ['cv_beta'],
                    'cv_lambda': ['cv_lambda'],
                    'cv_delta': ['cv_delta'],
                    'target_accept': ['target_accept', 'accept_rate']
                }

                for param, possible_names in config_params.items():
                    for name in possible_names:
                        if name in df.columns:
                            value = df[name].dropna().iloc[0]
                            config[param] = float(value) if param.startswith('cv_') or param == 'target_accept' else int(value)
                            print(f"  Found config: {param} = {config[param]}")
                            break

                # Try to read percentiles
                if 'percentiles_base' in df.columns:
                    perc_base = df['percentiles_base'].dropna().values
                    config['percentiles_base'] = perc_base.tolist()
                    print(f"  Found percentiles_base: {config['percentiles_base']}")

                if 'percentiles_sub' in df.columns:
                    perc_sub = df['percentiles_sub'].dropna().values
                    config['percentiles_sub'] = perc_sub.tolist()
                    print(f"  Found percentiles_sub: {config['percentiles_sub']}")

                dataset_name = filename.split('.')[0]

        except Exception as e:
            print(f"\n‚úó Error reading file: {e}")
            import traceback
            print("\nFull error traceback:")
            traceback.print_exc()
            print("\nUsing Holmen example instead.")
            use_example = True
            # Make sure to unset any partial data
            if 'cycles_data' in locals():
                del cycles_data
            if 'stress_data' in locals():
                del stress_data

# Final check - if reverted to example OR if data wasn't loaded, use Holmen
if use_example or 'stress_data' not in locals() or 'cycles_data' not in locals():
    if not use_example:
        print("\n‚ö† Data not loaded properly, using Holmen example instead.")
    # Load Holmen data (same as above)
    stress_data = np.array([
        0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95,
        0.95, 0.95, 0.95, 0.95, 0.95,
        0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90,
        0.90, 0.90, 0.90, 0.90, 0.90,
        0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825,
        0.825, 0.825, 0.825, 0.825, 0.825,
        0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75,
        0.75, 0.75, 0.75, 0.75, 0.75,
        0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675,
        0.675, 0.675, 0.675, 0.675, 0.675,
    ])
    cycles_data = np.array([
        0.037, 0.072, 0.074, 0.076, 0.083, 0.085, 0.105, 0.109, 0.120, 0.123, 0.143,
        0.203, 0.206, 0.217, 0.257, 0.201, 0.216, 0.226, 0.252, 0.257, 0.295, 0.311,
        0.342, 0.356, 0.451, 0.457, 0.509, 0.540, 0.680, 1.129, 1.246, 1.258, 1.460,
        1.492, 2.400, 2.410, 2.590, 2.903, 3.330, 3.590, 3.847, 4.110, 4.820, 5.560,
        5.598, 6.710, 9.930, 12.600, 15.580, 16.190, 17.280, 18.620, 20.300, 24.900,
        26.260, 27.940, 36.350, 48.420, 50.090, 67.340, 102.950, 280.320, 339.830,
        366.900, 485.620, 658.960, 896.330, 1241.760, 1250.200, 1329.780, 1399.830,
        1459.140, 3294.820, 12709, 14373
    ])
    config = {
        'n_tune': 1000,
        'n_draws': 2000,
        'n_chains': 2,
        'n_stress_points': 50,
        'n_param_samples': 1000,
        'percentiles_base': [0.01, 0.15, 0.50, 0.85, 0.99],
        'percentiles_sub': [0.01, 0.15, 0.50, 0.85, 0.99],
        'cv_N0': 0.25,
        'cv_Delta0': 0.15,
        'cv_beta': 0.30,
        'cv_lambda': 0.25,
        'cv_delta': 0.30,
        'target_accept': 0.95
    }
    dataset_name = "Holmen"

N_min = cycles_data.min()
N_max = cycles_data.max()

print(f"\n{'='*70}")
print(f"DATASET: {dataset_name}")
print(f"{'='*70}")
print(f"Data source: {'Holmen example (built-in)' if use_example else 'User-uploaded file'}")
print(f"Observations: {len(cycles_data)}")
print(f"Stress range: [{stress_data.min():.3f}, {stress_data.max():.3f}] (dimensionless)")
print(f"Cycles range: [{N_min:.4f}, {N_max:.2f}]")
print(f"\nFirst 5 N values: {cycles_data[:5]}")
print(f"First 5 Deltasigma values: {stress_data[:5]}")

print(f"\n{'='*70}")
print("ANALYSIS CONFIGURATION")
print(f"{'='*70}")
print(f"MCMC Settings:")
print(f"  Tune (warmup): {config['n_tune']}")
print(f"  Draws (samples): {config['n_draws']}")
print(f"  Chains: {config['n_chains']}")
print(f"  Target acceptance: {config['target_accept']}")
print(f"\nPercentiles:")
print(f"  Stress points: {config['n_stress_points']}")
print(f"  Parameter samples: {config['n_param_samples']}")
print(f"  Base percentiles: {[int(p*100) for p in config['percentiles_base']]}%")
print(f"  Sub-percentiles: {[int(p*100) for p in config['percentiles_sub']]}%")
print(f"\nPrior Coefficients of Variation (CV):")
print(f"  CV(N‚ÇÄ) = {config['cv_N0']:.2f}")
print(f"  CV(ŒîœÉ‚ÇÄ) = {config['cv_Delta0']:.2f}")
print(f"  CV(Œ≤) = {config['cv_beta']:.2f}")
print(f"  CV(Œª) = {config['cv_lambda']:.2f}")
print(f"  CV(Œ¥) = {config['cv_delta']:.2f}")

# ============================================================================
# DATA VISUALIZATION
# ============================================================================

print("\n" + "="*70)
print("DATA VISUALIZATION")
print("="*70)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

ax = axes[0]
# Use different colors for each unique stress level
unique_stresses = np.unique(stress_data)
colors_plot = plt.cm.rainbow(np.linspace(0, 1, len(unique_stresses)))

for i, stress_level in enumerate(unique_stresses):
    mask = stress_data == stress_level
    ax.scatter(cycles_data[mask], [stress_level]*np.sum(mask),
              alpha=0.7, s=50, label=f'{stress_level:.3f}',
              color=colors_plot[i], edgecolors='black', linewidths=0.5)

ax.set_xlabel('Cycles to Failure (N)', fontsize=12, fontweight='bold')
ax.set_ylabel('Stress (Deltasigma, dimensionless)', fontsize=12, fontweight='bold')
ax.set_xscale('log')
ax.set_title(f'S-N Data ({dataset_name}) - Weibull Model', fontsize=13, fontweight='bold')
if len(unique_stresses) <= 10:
    ax.legend(fontsize=9, loc='upper right')
ax.grid(True, alpha=0.3, which='both')

ax = axes[1]
ax.scatter(np.log(cycles_data), np.log(stress_data), alpha=0.6, s=40, color='darkblue')
ax.set_xlabel('ln(Cycles)', fontsize=12, fontweight='bold')
ax.set_ylabel('ln(Stress)', fontsize=12, fontweight='bold')
ax.set_title('Log-Log Space', fontsize=13, fontweight='bold')
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('data_exploration_weibull.png', dpi=150, bbox_inches='tight')
plt.show()

print("‚úì Data visualization completed!")

# ============================================================================
# PHASE 1: MAXIMUM LIKELIHOOD ESTIMATION (WEIBULL)
# ============================================================================

print("\n" + "="*70)
print("PHASE 1: MAXIMUM LIKELIHOOD ESTIMATION")
print("="*70)

def weibull_log_likelihood(params, stress, cycles):
    """
    Weibull log-likelihood for fatigue data.
    Castillo-Canteli dimensionless formulation.

    Model for minima (lower bounded data):
    log(N) ~ Weibull with location-scale depending on stress

    Parameters:
    -----------
    params : array [N0, Delta0, beta, lambda_param, delta]
        N0: reference number of cycles
        Delta0: reference stress (endurance limit)
        beta: shape parameter (Weibull)
        lambda_param: location parameter
        delta: scale parameter
    """
    N0, Delta0, beta, lambda_param, delta = params

    # Validations
    if N0 <= 0 or Delta0 <= 0 or beta <= 0 or delta <= 0:
        return -np.inf

    # Dimensionless transformation
    log_N_dimensionless = np.log(cycles) - np.log(N0)
    r = np.log(stress) - np.log(Delta0)

    # Check for valid r values
    if np.any(np.abs(r) < 1e-10):
        return -np.inf

    # Weibull parameters for minima
    # Location parameter
    mu_Y = (-lambda_param - delta) / r
    # Scale parameter
    sigma_Y = delta / (beta * np.abs(r))

    # Check valid sigma
    if np.any(sigma_Y <= 0):
        return -np.inf

    # Standardized variable for Weibull (Gumbel for minima parametrization)
    z = (log_N_dimensionless - mu_Y) / sigma_Y

    # Weibull log-likelihood for minima
    # CDF: F(y) = 1 - exp(-exp(z)) where z = (y - mu)/sigma
    # PDF: f(y) = (1/sigma) * exp(z - exp(z))

    log_lik = -np.log(sigma_Y) + z - np.exp(z)

    # Check for invalid values
    if not np.all(np.isfinite(log_lik)):
        return -np.inf

    return np.sum(log_lik)

def negative_log_likelihood(params, stress, cycles):
    """Negative log-likelihood for minimization."""
    return -weibull_log_likelihood(params, stress, cycles)

# Initial guess based on physical reasoning
N0_init = N_min * 0.5
Delta0_init = stress_data.min() * 0.7
beta_init = 3.0
lambda_init = -8.0
delta_init = 2.0

initial_params = np.array([N0_init, Delta0_init, beta_init, lambda_init, delta_init])

print("\nInitial guess:")
print(f"  N‚ÇÄ = {N0_init:.4f}")
print(f"  ŒîœÉ‚ÇÄ = {Delta0_init:.4f}")
print(f"  Œ≤ = {beta_init:.4f}")
print(f"  Œª = {lambda_init:.4f}")
print(f"  Œ¥ = {delta_init:.4f}")

# Parameter bounds for optimization
bounds = [
    (0.001, N_min * 0.9),           # N0
    (stress_data.min() * 0.4, stress_data.min() * 0.99),  # Delta0
    (0.5, 15.0),                    # beta
    (-12.0, -4.0),                  # lambda
    (0.5, 5.0)                      # delta
]

print("\nRunning global optimization (differential evolution)...")
print("This may take a few minutes...")

result_global = differential_evolution(
    negative_log_likelihood,
    bounds=bounds,
    args=(stress_data, cycles_data),
    seed=RANDOM_SEED,
    maxiter=1000,
    popsize=30,
    tol=1e-7,
    atol=1e-7,
    workers=1,
    updating='deferred',
    polish=True
)

print("\n‚úì Global optimization completed!")
print(f"  Success: {result_global.success}")
print(f"  Log-likelihood: {-result_global.fun:.2f}")
print(f"  Iterations: {result_global.nit}")

mle_params = result_global.x

print("\n" + "="*70)
print("MLE ESTIMATES (WEIBULL MODEL)")
print("="*70)
print(f"  N‚ÇÄ (reference cycles) = {mle_params[0]:.6f}")
print(f"  ŒîœÉ‚ÇÄ (reference stress) = {mle_params[1]:.6f}")
print(f"  Œ≤ (shape parameter)    = {mle_params[2]:.6f}")
print(f"  Œª (location param)     = {mle_params[3]:.6f}")
print(f"  Œ¥ (scale param)        = {mle_params[4]:.6f}")
print(f"\n  Log-likelihood = {-result_global.fun:.2f}")

# Save MLE results
with open('mle_estimates.txt', 'w') as f:
    f.write(f"MLE ESTIMATES - WEIBULL MODEL - {dataset_name}\n")
    f.write("="*50 + "\n")
    f.write(f"N0 = {mle_params[0]:.8f}\n")
    f.write(f"Delta0 = {mle_params[1]:.8f}\n")
    f.write(f"beta = {mle_params[2]:.8f}\n")
    f.write(f"lambda = {mle_params[3]:.8f}\n")
    f.write(f"delta = {mle_params[4]:.8f}\n")
    f.write(f"Log-likelihood = {-result_global.fun:.8f}\n")

print("\n‚úì MLE estimates saved to 'mle_estimates.txt'")

# Visualize MLE fit
print("\nVisualizing MLE fit...")

fig, ax = plt.subplots(figsize=(14, 8))

# Plot observed data
ax.scatter(cycles_data, stress_data, c='black', s=80,
          alpha=0.9, label='Observed data', zorder=100, marker='o',
          edgecolors='white', linewidths=1.5)

# Plot MLE curves for different percentiles
stress_range_plot = np.linspace(stress_data.min() * 0.97, stress_data.max() * 1.03, 100)
percentiles_mle = [0.01, 0.10, 0.50, 0.90, 0.99]
colors_mle = ['#8B0000', '#FF8C00', '#228B22', '#4169E1', '#8B008B']

N0_mle, Delta0_mle, beta_mle, lambda_mle, delta_mle = mle_params

for perc, color in zip(percentiles_mle, colors_mle):
    N_perc = []
    for stress in stress_range_plot:
        if stress > 0 and Delta0_mle > 0:
            r = np.log(stress / Delta0_mle)
            if abs(r) > 1e-10:
                mu_Y = (-lambda_mle - delta_mle) / r
                sigma_Y = delta_mle / (beta_mle * abs(r))

                # Weibull quantile for minima (Gumbel parametrization)
                z_p = np.log(-np.log(1 - perc))
                Y_p = mu_Y + sigma_Y * z_p
                N_p = N0_mle * np.exp(Y_p)

                if N_p > 0 and np.isfinite(N_p):
                    N_perc.append(N_p)
                else:
                    N_perc.append(np.nan)
            else:
                N_perc.append(np.nan)
        else:
            N_perc.append(np.nan)

    valid_mask = ~np.isnan(N_perc)
    if np.sum(valid_mask) > 0:
        ax.plot(np.array(N_perc)[valid_mask], stress_range_plot[valid_mask],
               color=color, linewidth=2.5, label=f'P{int(perc*100)} MLE',
               alpha=0.8)

ax.set_xlabel('Cycles to Failure (N)', fontsize=13, fontweight='bold')
ax.set_ylabel('Stress (Deltasigma, dimensionless)', fontsize=13, fontweight='bold')
ax.set_xscale('log')
ax.set_title(f'MLE Fit - Weibull Model ({dataset_name})',
            fontsize=14, fontweight='bold')
ax.legend(fontsize=10, loc='upper right')
ax.grid(True, alpha=0.3, which='both')
ax.set_xlim([N_min * 0.1, N_max * 10])

plt.tight_layout()
plt.savefig('mle_results.png', dpi=150, bbox_inches='tight')
plt.show()

print("‚úì MLE visualization completed!")

# ============================================================================
# PHASE 2: DEFINE INFORMATIVE PRIORS FROM MLE
# ============================================================================

print("\n" + "="*70)
print("PHASE 2: DEFINING BAYESIAN PRIORS FROM MLE")
print("="*70)

# Use MLE estimates to define informative Normal priors
# Standard deviations from coefficients of variation

N0_mle_val = mle_params[0]
Delta0_mle_val = mle_params[1]
beta_mle_val = mle_params[2]
lambda_mle_val = mle_params[3]
delta_mle_val = mle_params[4]

# Prior standard deviations using CV from config
N0_std = N0_mle_val * config['cv_N0']
Delta0_std = Delta0_mle_val * config['cv_Delta0']
beta_std = beta_mle_val * config['cv_beta']
lambda_std = abs(lambda_mle_val) * config['cv_lambda']
delta_std = delta_mle_val * config['cv_delta']

print("\nInformative Normal Priors (centered on MLE):")
print(f"  N‚ÇÄ     ~ Normal({N0_mle_val:.4f}, {N0_std:.4f})  [CV={config['cv_N0']:.2f}]")
print(f"  ŒîœÉ‚ÇÄ    ~ Normal({Delta0_mle_val:.6f}, {Delta0_std:.6f})  [CV={config['cv_Delta0']:.2f}]")
print(f"  Œ≤      ~ Normal({beta_mle_val:.4f}, {beta_std:.4f})  [CV={config['cv_beta']:.2f}]")
print(f"  Œª      ~ Normal({lambda_mle_val:.4f}, {lambda_std:.4f})  [CV={config['cv_lambda']:.2f}]")
print(f"  Œ¥      ~ Normal({delta_mle_val:.4f}, {delta_std:.4f})  [CV={config['cv_delta']:.2f}]")

print("\nThese priors will help achieve faster convergence in Bayesian inference.")

# ============================================================================
# PHASE 3: BAYESIAN INFERENCE WITH WEIBULL MODEL
# ============================================================================

print("\n" + "="*70)
print("PHASE 3: BAYESIAN INFERENCE")
print("="*70)

initial_values = {
    'N0': N0_mle_val,
    'Delta0': Delta0_mle_val,
    'beta': beta_mle_val,
    'lambda_param': lambda_mle_val,
    'delta': delta_mle_val
}

with pm.Model() as fatigue_model:
    # PRIORS - Informative Normal distributions based on MLE
    N0 = pm.TruncatedNormal('N0', mu=N0_mle_val, sigma=N0_std,
                            lower=0.001, upper=N_min)
    Delta0 = pm.TruncatedNormal('Delta0', mu=Delta0_mle_val, sigma=Delta0_std,
                                lower=stress_data.min() * 0.3, upper=stress_data.min())
    beta = pm.TruncatedNormal('beta', mu=beta_mle_val, sigma=beta_std,
                             lower=0.5, upper=20.0)
    lambda_param = pm.Normal('lambda_param', mu=lambda_mle_val, sigma=lambda_std)
    delta = pm.TruncatedNormal('delta', mu=delta_mle_val, sigma=delta_std,
                              lower=0.1, upper=10.0)

    # Transform to dimensionless log-space
    log_N_dimensionless = pt.log(cycles_data) - pt.log(N0)
    r = pt.log(stress_data) - pt.log(Delta0)

    # Weibull parameters for minima
    mu_Y = (-lambda_param - delta) / r
    sigma_Y = delta / (beta * pt.abs(r) + 1e-8)

    # Standardized variable
    z = (log_N_dimensionless - mu_Y) / (sigma_Y + 1e-8)

    # Log-likelihood (Weibull for minima, Gumbel parametrization)
    log_lik = -pt.log(sigma_Y + 1e-8) + z - pt.exp(z)

    # Total likelihood
    likelihood = pm.Potential('likelihood', pt.sum(log_lik))

print("‚úì Bayesian model defined with informative priors")

# ============================================================================
# PHASE 4: PRIOR PREDICTIVE CHECK
# ============================================================================

print("\n" + "="*70)
print("PRIOR PREDICTIVE CHECK")
print("="*70)

print("\nSampling from prior predictive distribution...")

with fatigue_model:
    prior_predictive = pm.sample_prior_predictive(
        samples=500,
        random_seed=RANDOM_SEED
    )

print("‚úì Prior predictive samples generated")

# Visualize prior distributions
print("\nVisualizing prior distributions...")

fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

var_names = ['N0', 'Delta0', 'beta', 'lambda_param', 'delta']
var_labels = ['N‚ÇÄ (reference cycles)', 'ŒîœÉ‚ÇÄ (reference stress)', 'Œ≤', 'Œª', 'Œ¥']

for ax, var, label in zip(axes[:5], var_names, var_labels):
    samples = prior_predictive.prior[var].values.flatten()

    ax.hist(samples, bins=50, alpha=0.7, color='steelblue', edgecolor='black')
    ax.set_xlabel(label, fontsize=11, fontweight='bold')
    ax.set_ylabel('Frequency', fontsize=11)
    ax.set_title(f'Prior: {label}', fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3)

    ax.axvline(np.median(samples), color='red', linestyle='--', linewidth=2, label='Median')

    # Add MLE value line
    if var == 'N0':
        ax.axvline(N0_mle_val, color='green', linestyle=':', linewidth=2, label='MLE')
    elif var == 'Delta0':
        ax.axvline(Delta0_mle_val, color='green', linestyle=':', linewidth=2, label='MLE')
    elif var == 'beta':
        ax.axvline(beta_mle_val, color='green', linestyle=':', linewidth=2, label='MLE')
    elif var == 'lambda_param':
        ax.axvline(lambda_mle_val, color='green', linestyle=':', linewidth=2, label='MLE')
    elif var == 'delta':
        ax.axvline(delta_mle_val, color='green', linestyle=':', linewidth=2, label='MLE')

    ax.legend()

axes[5].axis('off')

plt.suptitle(f'Prior Distributions (Centered on MLE) - {dataset_name}', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig('prior_distributions.png', dpi=150, bbox_inches='tight')
plt.show()

print("‚úì Prior distributions visualized")

# ============================================================================
# PHASE 5: SAMPLE FROM POSTERIOR
# ============================================================================

print("\n" + "="*70)
print("SAMPLING FROM POSTERIOR DISTRIBUTION")
print("="*70)

print("\nSampling strategy:")
print(f"  ‚Ä¢ Using informative priors from MLE")
print(f"  ‚Ä¢ Tune: {config['n_tune']}, Draws: {config['n_draws']}, Chains: {config['n_chains']}")
print(f"  ‚Ä¢ Target acceptance = {config['target_accept']}")
print("\nThis may take 5-15 minutes depending on configuration...\n")

with fatigue_model:
    trace = pm.sample(
        draws=config['n_draws'],
        tune=config['n_tune'],
        chains=config['n_chains'],
        cores=1,
        random_seed=RANDOM_SEED,
        return_inferencedata=True,
        target_accept=config['target_accept'],
        init='adapt_diag',
        initvals=initial_values
    )

print("\n‚úì Sampling completed!")

# ============================================================================
# PHASE 6: CONVERGENCE DIAGNOSTICS
# ============================================================================

print("\n" + "="*70)
print("CONVERGENCE DIAGNOSTICS")
print("="*70)

summary_table = az.summary(
    trace,
    var_names=['N0', 'Delta0', 'beta', 'lambda_param', 'delta'],
    hdi_prob=0.95
)
print("\n--- Posterior Summary ---")
print(summary_table)

rhat_values = az.rhat(trace)
print(f"\n--- R-hat Diagnostic (should be < 1.01) ---")
all_rhat_good = True
for var in ['N0', 'Delta0', 'beta', 'lambda_param', 'delta']:
    rhat_val = rhat_values[var].values
    status = "‚úì" if rhat_val < 1.01 else "‚úó"
    if rhat_val >= 1.01:
        all_rhat_good = False
    print(f"{var:15s}: {rhat_val:.4f} {status}")

ess_values = az.ess(trace)
print(f"\n--- Effective Sample Size (should be > 1000) ---")
all_ess_good = True
for var in ['N0', 'Delta0', 'beta', 'lambda_param', 'delta']:
    ess_val = ess_values[var].values
    status = "‚úì" if ess_val > 1000 else "‚úó"
    if ess_val <= 1000:
        all_ess_good = False
    print(f"{var:15s}: {ess_val:.0f} {status}")

n_divergences = trace.sample_stats.diverging.sum().values
total_samples = config['n_draws'] * config['n_chains']
print(f"\n--- Divergence Check ---")
print(f"Number of divergent transitions: {n_divergences}")
if n_divergences > 0:
    print(f"‚ö† Warning: {n_divergences} divergences detected")
    print(f"  Divergence rate: {n_divergences / total_samples:.2%}")
else:
    print("‚úì No divergences detected!")

print(f"\n--- Overall Convergence Assessment ---")
if all_rhat_good and all_ess_good and n_divergences == 0:
    print("‚úì‚úì‚úì EXCELLENT: Model has converged successfully!")
elif all_rhat_good and n_divergences < 50:
    print("‚úì‚úì GOOD: Model convergence is acceptable")
else:
    print("‚úó WARNING: Model may not have converged properly")

# Trace plots
print("\nGenerating trace plots...")
fig, axes = plt.subplots(5, 2, figsize=(14, 16))
az.plot_trace(
    trace,
    var_names=['N0', 'Delta0', 'beta', 'lambda_param', 'delta'],
    compact=False,
    axes=axes
)
plt.suptitle(f'Trace Plots and Posterior Distributions - {dataset_name}',
            fontsize=14, fontweight='bold', y=1.001)
plt.tight_layout()
plt.savefig('trace_plots.png', dpi=150, bbox_inches='tight')
plt.show()

# Posterior distributions
fig = plt.figure(figsize=(15, 10))
az.plot_posterior(
    trace,
    var_names=['N0', 'Delta0', 'beta', 'lambda_param', 'delta'],
    hdi_prob=0.95,
    figsize=(15, 10)
)
plt.suptitle(f'Posterior Distributions with 95% HDI - {dataset_name}',
            fontsize=14, fontweight='bold')
plt.tight_layout()
plt.savefig('posterior_distributions.png', dpi=150, bbox_inches='tight')
plt.show()

print("\n‚úì Diagnostics completed!")

# ============================================================================
# PHASE 7: PERCENTILES OF PERCENTILES
# ============================================================================

print("\n" + "="*70)
print("COMPUTING PERCENTILES OF PERCENTILES")
print("="*70)

posterior = trace.posterior

# Define stress range for plotting
stress_margin = 0.03
stress_min_plot = stress_data.min() - stress_margin
stress_max_plot = stress_data.max() + stress_margin
stress_min_plot = max(stress_min_plot, 0.01)

stress_range = np.linspace(stress_min_plot, stress_max_plot, config['n_stress_points'])

percentiles_base = config['percentiles_base']
percentiles_sub = config['percentiles_sub']

print(f"\nConfiguration:")
print(f"  Stress points: {config['n_stress_points']}")
print(f"  Parameter samples: {config['n_param_samples']}")
print(f"  Base percentiles: {[int(p*100) for p in percentiles_base]}%")
print(f"  Sub-percentiles: {[int(p*100) for p in percentiles_sub]}%")

def compute_percentile(stress, N0, Delta0, beta, lambda_p, delta, prob):
    """Compute N_p for given stress and failure probability (Weibull)."""
    if stress <= 0 or N0 <= 0 or Delta0 <= 0 or beta <= 0 or delta <= 0:
        return np.nan
    if prob <= 0 or prob >= 1:
        return np.nan

    try:
        r = np.log(stress / Delta0)
        if abs(r) < 1e-10:
            return np.nan

        mu_Y = (-lambda_p - delta) / r
        sigma_Y = delta / (beta * abs(r))

        # Weibull quantile for minima (Gumbel parametrization)
        z_p = np.log(-np.log(1 - prob))
        Y_p = mu_Y + sigma_Y * z_p
        N_p = N0 * np.exp(Y_p)

        if not np.isfinite(N_p) or N_p <= 0:
            return np.nan

        if N_p < 1e-6 or N_p > 1e10:
            return np.nan

        return N_p
    except:
        return np.nan

# Extract posterior samples
N0_samples = posterior['N0'].values.flatten()
Delta0_samples = posterior['Delta0'].values.flatten()
beta_samples = posterior['beta'].values.flatten()
lambda_samples = posterior['lambda_param'].values.flatten()
delta_samples = posterior['delta'].values.flatten()

total_samples = len(N0_samples)
sample_indices = np.random.choice(total_samples, size=min(config['n_param_samples'], total_samples), replace=False)

# Storage for percentiles of percentiles
percentiles_of_percentiles = {}

print("\nComputing percentiles of percentiles...")

for perc_base_idx, perc_base in enumerate(percentiles_base):
    print(f"\n  Processing base percentile P{int(perc_base*100)}...")

    percentile_matrix = np.zeros((config['n_stress_points'], len(sample_indices)))

    for i, stress in enumerate(stress_range):
        if i % max(1, config['n_stress_points']//5) == 0:
            print(f"    Stress point {i+1}/{config['n_stress_points']}...")

        for j, idx in enumerate(sample_indices):
            N0_s = N0_samples[idx]
            Delta0_s = Delta0_samples[idx]
            beta_s = beta_samples[idx]
            lambda_s = lambda_samples[idx]
            delta_s = delta_samples[idx]

            N_p = compute_percentile(stress, N0_s, Delta0_s, beta_s, lambda_s, delta_s, perc_base)

            if not np.isnan(N_p):
                percentile_matrix[i, j] = N_p
            else:
                percentile_matrix[i, j] = np.nan

    n_valid = np.sum(~np.isnan(percentile_matrix), axis=1)
    print(f"    Valid values per stress: min={n_valid.min()}, max={n_valid.max()}, mean={n_valid.mean():.1f}")

    # Sort each row
    percentile_matrix_sorted = np.sort(percentile_matrix, axis=1)

    # Extract sub-percentiles
    percentile_indices = [int(p * (len(sample_indices) - 1)) for p in percentiles_sub]
    perc_of_perc_curves = percentile_matrix_sorted[:, percentile_indices]

    percentiles_of_percentiles[perc_base] = perc_of_perc_curves

print("\n‚úì Percentiles of percentiles computed!")

# ============================================================================
# PHASE 8: PLOT WITH SHADED REGIONS
# ============================================================================

print("\nPlotting percentiles of percentiles with shaded regions...")

fig, ax = plt.subplots(figsize=(18, 11), facecolor='white')
ax.set_facecolor('white')

# Colors for shading and lines
colors_base = ['#8B0000', '#FF8C00', '#228B22', '#4169E1', '#8B008B']
colors_shaded = ['#FFB6B9', '#FFCC80', '#A5D6A7', '#90CAF9', '#CE93D8']

# Generate labels for percentiles
perc_names = [f'P{int(p*100)}' for p in percentiles_base]

# Ensure we have enough colors
if len(percentiles_base) > len(colors_base):
    colors_base = plt.cm.rainbow(np.linspace(0, 1, len(percentiles_base)))
    colors_shaded = plt.cm.rainbow(np.linspace(0, 1, len(percentiles_base)))

# FIRST: Plot shaded regions
print("  Plotting shaded uncertainty bands...")
for base_idx, (perc_base, perc_name) in enumerate(zip(percentiles_base, perc_names)):
    curves = percentiles_of_percentiles[perc_base]

    curve_p_min = curves[:, 0]  # First sub-percentile
    curve_p_max = curves[:, -1]  # Last sub-percentile

    valid_mask = (~np.isnan(curve_p_min)) & (~np.isnan(curve_p_max))

    if np.sum(valid_mask) > 3:
        color_shaded = colors_shaded[base_idx % len(colors_shaded)]
        ax.fill_betweenx(stress_range[valid_mask],
                curve_p_min[valid_mask],
                curve_p_max[valid_mask],
                color=color_shaded,
                alpha=0.85,
                label=f'{perc_name} uncertainty band',
                zorder=base_idx + 1)

# SECOND: Plot intermediate sub-percentile curves
print("  Plotting intermediate sub-percentile curves...")
if len(percentiles_sub) > 2:
    for base_idx, (perc_base, perc_name) in enumerate(zip(percentiles_base, perc_names)):
        curves = percentiles_of_percentiles[perc_base]
        color_base = colors_base[base_idx % len(colors_base)]

        for sub_idx in range(1, len(percentiles_sub)-1):
            curve = curves[:, sub_idx]
            valid_mask = ~np.isnan(curve)
            if np.sum(valid_mask) > 3:
                linestyle = '--' if sub_idx == 1 else ':'
                ax.plot(curve[valid_mask], stress_range[valid_mask],
                       color=color_base, linestyle=linestyle, linewidth=1.2,
                       alpha=0.5, zorder=10 + base_idx)

# THIRD: Plot extreme sub-percentiles
print("  Plotting extreme sub-percentile curves...")
for base_idx, (perc_base, perc_name) in enumerate(zip(percentiles_base, perc_names)):
    curves = percentiles_of_percentiles[perc_base]
    color_base = colors_base[base_idx % len(colors_base)]

    curve_p_min = curves[:, 0]
    valid_mask = ~np.isnan(curve_p_min)
    if np.sum(valid_mask) > 3:
        ax.plot(curve_p_min[valid_mask], stress_range[valid_mask],
               color=color_base, linestyle=(0, (5, 2)), linewidth=1.5,
               alpha=0.7, zorder=15 + base_idx)

    curve_p_max = curves[:, -1]
    valid_mask = ~np.isnan(curve_p_max)
    if np.sum(valid_mask) > 3:
        ax.plot(curve_p_max[valid_mask], stress_range[valid_mask],
               color=color_base, linestyle=(0, (1, 1)), linewidth=1.5,
               alpha=0.7, zorder=15 + base_idx)

# FOURTH: Plot median curves - THICK
print("  Plotting median curves...")
median_idx = len(percentiles_sub) // 2
for base_idx, (perc_base, perc_name) in enumerate(zip(percentiles_base, perc_names)):
    curves = percentiles_of_percentiles[perc_base]
    color_base = colors_base[base_idx % len(colors_base)]

    curve_median = curves[:, median_idx]

    valid_mask = ~np.isnan(curve_median)

    if np.sum(valid_mask) > 3:
        ax.plot(curve_median[valid_mask], stress_range[valid_mask],
               color=color_base, linewidth=3.5,
               label=f'{perc_name} (median curve)',
               zorder=50 + base_idx)

# FIFTH: Plot observed data on TOP
print("  Plotting observed data...")
ax.scatter(cycles_data, stress_data, c='black', s=80,
          alpha=0.9, label='Observed data', zorder=100, marker='o',
          edgecolors='white', linewidths=1.5)

cycles_min_plot = N_min * 0.1
cycles_max_plot = N_max * 10.0

ax.set_xlim([cycles_min_plot, cycles_max_plot])

ax.set_xlabel('Cycles to Failure (N)', fontsize=15, fontweight='bold')
ax.set_ylabel('Stress (Deltasigma, dimensionless)', fontsize=15, fontweight='bold')
ax.set_xscale('log')
ax.set_title(f'Percentiles of Percentiles with Uncertainty Bands ({dataset_name})\n' +
            'Bayesian Weibull Model - Castillo-Canteli Formulation',
            fontsize=15, fontweight='bold', pad=20)

# Legend
ax.legend(loc='upper right', fontsize=10, framealpha=0.98, ncol=2,
         columnspacing=1.0, handlelength=2.5,
         title='Base Percentiles & Uncertainty Bands',
         title_fontsize=11)

ax.grid(True, alpha=0.3, which='both', linestyle='-', linewidth=0.5)
ax.set_xlim([cycles_min_plot, cycles_max_plot])
ax.set_ylim([stress_min_plot - 0.01, stress_max_plot + 0.01])

plt.tight_layout()
plt.savefig('percentiles_of_percentiles_shaded.png', dpi=250, bbox_inches='tight')
plt.show()

print("‚úì Shaded plot completed!")

# ============================================================================
# PHASE 9: ALTERNATIVE PLOT - ALL CURVES
# ============================================================================

print("\nCreating alternative plot with all curves...")

fig, ax = plt.subplots(figsize=(18, 11), facecolor='white')
ax.set_facecolor('white')

# Plot shaded regions first
for base_idx, perc_base in enumerate(percentiles_base):
    curves = percentiles_of_percentiles[perc_base]
    curve_p_min = curves[:, 0]
    curve_p_max = curves[:, -1]
    valid_mask = (~np.isnan(curve_p_min)) & (~np.isnan(curve_p_max))

    if np.sum(valid_mask) > 3:
        color_shaded = colors_shaded[base_idx % len(colors_shaded)]
        ax.fill_betweenx(stress_range[valid_mask],
                        curve_p_min[valid_mask],
                        curve_p_max[valid_mask],
                        color=color_shaded,
                        alpha=0.5,
                        zorder=base_idx + 1)

# Plot ALL curves with labels
linestyles_sub = ['-', '--', '-', ':', (0, (1, 1))]
if len(percentiles_sub) > len(linestyles_sub):
    linestyles_sub = ['-'] * len(percentiles_sub)

linewidths_sub = [1.5] * len(percentiles_sub)
linewidths_sub[median_idx] = 3.5  # Median thicker

alpha_sub = [0.6] * len(percentiles_sub)
alpha_sub[median_idx] = 1.0  # Median fully opaque

curve_count = 0
for base_idx, (perc_base, perc_name_base) in enumerate(zip(percentiles_base, perc_names)):
    curves = percentiles_of_percentiles[perc_base]
    color_base = colors_base[base_idx % len(colors_base)]

    perc_names_sub = [f'P{int(p*100)}' for p in percentiles_sub]

    for sub_idx in range(len(percentiles_sub)):
        curve = curves[:, sub_idx]
        valid_mask = ~np.isnan(curve)

        if np.sum(valid_mask) > 3:
            if sub_idx == median_idx:
                label = f'{perc_name_base}-{perc_names_sub[sub_idx]} (median)'
                zorder_val = 50 + base_idx
            elif sub_idx in [0, len(percentiles_sub)-1]:
                label = f'{perc_name_base}-{perc_names_sub[sub_idx]} (bound)'
                zorder_val = 30 + base_idx
            else:
                label = f'{perc_name_base}-{perc_names_sub[sub_idx]}'
                zorder_val = 20 + base_idx

            ax.plot(curve[valid_mask], stress_range[valid_mask],
                   color=color_base, linestyle=linestyles_sub[sub_idx],
                   linewidth=linewidths_sub[sub_idx],
                   label=label, alpha=alpha_sub[sub_idx], zorder=zorder_val)
            curve_count += 1

# Plot observed data
ax.scatter(cycles_data, stress_data, c='black', s=80,
          alpha=0.9, label='Observed data', zorder=100, marker='o',
          edgecolors='white', linewidths=1.5)

total_curves = len(percentiles_base) * len(percentiles_sub)
print(f"  Total curves plotted: {curve_count}/{total_curves}")

ax.set_xlabel('Cycles to Failure (N)', fontsize=15, fontweight='bold')
ax.set_ylabel('Stress (Deltasigma, dimensionless)', fontsize=15, fontweight='bold')
ax.set_xscale('log')
ax.set_title(f'All {total_curves} Percentile Curves ({len(percentiles_base)} Base √ó {len(percentiles_sub)} Sub) - {dataset_name}\n' +
            'Complete Uncertainty Quantification',
            fontsize=15, fontweight='bold', pad=20)

ax.legend(loc='upper right', fontsize=8, framealpha=0.95, ncol=3,
         columnspacing=0.6, handlelength=2.0, handletextpad=0.5,
         title='Base-Sub Percentile Curves', title_fontsize=9,
         borderpad=0.5, labelspacing=0.3)

ax.grid(True, alpha=0.3, which='both', linestyle='-', linewidth=0.5)
ax.set_xlim([cycles_min_plot, cycles_max_plot])
ax.set_ylim([stress_min_plot - 0.01, stress_max_plot + 0.01])

plt.tight_layout()
plt.savefig('percentiles_of_percentiles_all_curves.png', dpi=250, bbox_inches='tight')
plt.show()

print("‚úì Alternative plot completed!")

# ============================================================================
# PHASE 10: SUMMARY
# ============================================================================

print("\n" + "="*70)
print("SUMMARY")
print("="*70)

print(f"\nDataset: {dataset_name}")
print(f"Observations: {len(cycles_data)}")

print("\n1. MLE ESTIMATES:")
print(f"   N‚ÇÄ = {mle_params[0]:.6f}")
print(f"   ŒîœÉ‚ÇÄ = {mle_params[1]:.6f}")
print(f"   Œ≤ = {mle_params[2]:.3f}")
print(f"   Œª = {mle_params[3]:.3f}")
print(f"   Œ¥ = {mle_params[4]:.3f}")

print("\n2. POSTERIOR MEDIANS:")
print(f"   N‚ÇÄ = {posterior['N0'].median().values:.6f}")
print(f"   ŒîœÉ‚ÇÄ = {posterior['Delta0'].median().values:.6f}")
print(f"   Œ≤ = {posterior['beta'].median().values:.3f}")
print(f"   Œª = {posterior['lambda_param'].median().values:.3f}")
print(f"   Œ¥ = {posterior['delta'].median().values:.3f}")

print("\n3. CONVERGENCE:")
print(f"   R-hat all < 1.01: {'‚úì' if all_rhat_good else '‚úó'}")
print(f"   ESS all > 1000: {'‚úì' if all_ess_good else '‚úó'}")
print(f"   Divergences: {n_divergences}")
if n_divergences > 0:
    print(f"   Divergence rate: {n_divergences/total_samples*100:.2f}%")

print("\n4. PERCENTILES OF PERCENTILES:")
print(f"   Total curves generated: {len(percentiles_base) * len(percentiles_sub)} ({len(percentiles_base)} base √ó {len(percentiles_sub)} sub)")
print(f"   Two visualization approaches:")
print(f"     ‚Ä¢ Main plot: Shaded bands with key curves")
print(f"     ‚Ä¢ Alternative: All curves individually labeled")

print("\n5. FILES CREATED:")
print("   ‚Ä¢ data_exploration_weibull.png")
print("   ‚Ä¢ mle_results.png")
print("   ‚Ä¢ mle_estimates.txt")
print("   ‚Ä¢ prior_distributions.png")
print("   ‚Ä¢ trace_plots.png")
print("   ‚Ä¢ posterior_distributions.png")
print("   ‚Ä¢ percentiles_of_percentiles_shaded.png")
print("   ‚Ä¢ percentiles_of_percentiles_all_curves.png")

print("\n" + "="*70)
print("ANALYSIS COMPLETE!")
print("="*70)

# Save results
trace.to_netcdf('fatigue_posterior_weibull.nc')
summary_table.to_csv('fatigue_summary_weibull.csv')

perc_of_perc_data = {}
for perc_base in percentiles_base:
    perc_of_perc_data[f'P{int(perc_base*100)}'] = percentiles_of_percentiles[perc_base]

np.savez('percentiles_of_percentiles.npz',
         stress_range=stress_range,
         percentiles_base=percentiles_base,
         percentiles_sub=percentiles_sub,
         **perc_of_perc_data)

print("\n‚úì Results saved:")
print("  ‚Ä¢ fatigue_posterior_weibull.nc")
print("  ‚Ä¢ fatigue_summary_weibull.csv")
print("  ‚Ä¢ percentiles_of_percentiles.npz")

# ============================================================================
# PHASE 11: SYNTHETIC DATA GENERATION FROM POSTERIOR SAMPLES
# ============================================================================

print("\n" + "="*70)
print("SYNTHETIC DATA GENERATION FROM POSTERIOR SAMPLES")
print("="*70)

print("\nWould you like to generate synthetic datasets using posterior samples?")
print("(This uses the actual MCMC samples, excluding warmup)")
generate_synthetic = input("\nGenerate synthetic data? (y/n): ").strip().lower()

if generate_synthetic == 'y':

    # Ask for number of observations per dataset
    while True:
        try:
            n_obs_per_dataset = int(input("\nHow many observations per synthetic dataset? (e.g., 75, 100, 360): ").strip())
            if n_obs_per_dataset > 0 and n_obs_per_dataset <= 20000:
                break
            else:
                print("Please enter a number between 1 and 20000")
        except ValueError:
            print("Invalid input. Please enter a number.")

    # Ask for number of datasets to generate
    while True:
        try:
            n_datasets = int(input("\nHow many synthetic datasets to generate? (e.g., 1, 10, 100): ").strip())
            if n_datasets > 0 and n_datasets <= 20000:
                break
            else:
                print("Please enter a number between 1 and 20000")
        except ValueError:
            print("Invalid input. Please enter a number.")

    print(f"\n{'='*70}")
    print(f"CONFIGURATION:")
    print(f"  Observations per dataset: {n_obs_per_dataset}")
    print(f"  Number of datasets: {n_datasets}")
    print(f"  Total posterior samples available: {len(N0_samples)}")
    print(f"{'='*70}")

    # Check if we have enough samples
    if n_datasets > len(N0_samples):
        print(f"\n‚ö† Warning: Requested {n_datasets} datasets but only {len(N0_samples)} posterior samples available.")
        print(f"  Will generate {min(n_datasets, len(N0_samples))} datasets (one per unique sample).")
        n_datasets = min(n_datasets, len(N0_samples))

    # Select random posterior samples (one per dataset)
    selected_sample_indices = np.random.choice(len(N0_samples), size=n_datasets, replace=False)

    print(f"\n‚úì Selected {n_datasets} random posterior samples")

    # Create directory for synthetic data
    import os
    synthetic_dir = f'synthetic_data_{dataset_name}'
    if not os.path.exists(synthetic_dir):
        os.makedirs(synthetic_dir)
    print(f"‚úì Created directory: {synthetic_dir}/")

    # Generate synthetic datasets
    print(f"\nGenerating {n_datasets} synthetic datasets...")

    all_synthetic_data = []

    for dataset_idx, sample_idx in enumerate(selected_sample_indices):
        if (dataset_idx + 1) % max(1, n_datasets // 10) == 0 or dataset_idx == 0:
            print(f"  Generating dataset {dataset_idx + 1}/{n_datasets}...")

        # Get parameter values from this posterior sample
        N0_synth = N0_samples[sample_idx]
        Delta0_synth = Delta0_samples[sample_idx]
        beta_synth = beta_samples[sample_idx]
        lambda_synth = lambda_samples[sample_idx]
        delta_synth = delta_samples[sample_idx]

        # Generate stress levels (uniform distribution within observed range)
        stress_synthetic = np.random.uniform(
            low=stress_data.min(),
            high=stress_data.max(),
            size=n_obs_per_dataset
        )

        # Generate cycles for each stress level using the Weibull model
        cycles_synthetic = np.zeros(n_obs_per_dataset)

        for i in range(n_obs_per_dataset):
            stress = stress_synthetic[i]

            # Weibull parameters for this stress level
            r = np.log(stress / Delta0_synth)
            mu_Y = (-lambda_synth - delta_synth) / r
            sigma_Y = delta_synth / (beta_synth * abs(r))

            # Generate random Gumbel (for minima) variable
            u = np.random.uniform(0, 1)
            z = np.log(-np.log(1 - u))  # Inverse CDF

            Y = mu_Y + sigma_Y * z
            N = N0_synth * np.exp(Y)

            cycles_synthetic[i] = N

        # Create DataFrame for this dataset
        synthetic_df = pd.DataFrame({
            'N': cycles_synthetic,
            'Deltasigma': stress_synthetic
        })

        # Sort by stress level
        synthetic_df = synthetic_df.sort_values('Deltasigma').reset_index(drop=True)

        # Store parameters used
        synthetic_df.attrs['N0'] = N0_synth
        synthetic_df.attrs['Delta0'] = Delta0_synth
        synthetic_df.attrs['beta'] = beta_synth
        synthetic_df.attrs['lambda'] = lambda_synth
        synthetic_df.attrs['delta'] = delta_synth
        synthetic_df.attrs['sample_idx'] = sample_idx

        all_synthetic_data.append(synthetic_df)

        # Save individual CSV file
        csv_filename = f'{synthetic_dir}/synthetic_{dataset_idx+1:04d}.csv'
        synthetic_df.to_csv(csv_filename, index=False)

        # Save individual R/OpenBUGS format file
        r_filename = f'{synthetic_dir}/synthetic_{dataset_idx+1:04d}.txt'
        with open(r_filename, 'w') as f:
            f.write(f"# Synthetic dataset {dataset_idx+1}/{n_datasets}\n")
            f.write(f"# Generated from posterior sample {sample_idx}\n")
            f.write(f"# Parameters: N0={N0_synth:.6f}, Delta0={Delta0_synth:.6f}, beta={beta_synth:.3f}, lambda={lambda_synth:.3f}, delta={delta_synth:.3f}\n")
            f.write(f"list(M={n_obs_per_dataset},\n")
            f.write(f"ns=50,\n")
            f.write(f"np=5,\n")
            f.write(f"percentiles=c({','.join([str(p) for p in config['percentiles_base']])}),\n")

            # Write Deltasigma
            f.write("Deltasigma=c(")
            deltasigma_str = ','.join([f"{s:.3f}" for s in synthetic_df['Deltasigma'].values])
            f.write(deltasigma_str)
            f.write("),\n")

            # Write N
            f.write("N=c(")
            n_str = ','.join([f"{n:.1f}" for n in synthetic_df['N'].values])
            f.write(n_str)
            f.write(")\n")
            f.write(")\n")

    print(f"\n‚úì Generated {n_datasets} synthetic datasets!")
    print(f"  Files saved in: {synthetic_dir}/")

    # Create summary file with parameters used
    params_summary = pd.DataFrame({
        'dataset': [f'synthetic_{i+1:04d}' for i in range(n_datasets)],
        'sample_idx': [df.attrs['sample_idx'] for df in all_synthetic_data],
        'N0': [df.attrs['N0'] for df in all_synthetic_data],
        'Delta0': [df.attrs['Delta0'] for df in all_synthetic_data],
        'beta': [df.attrs['beta'] for df in all_synthetic_data],
        'lambda': [df.attrs['lambda'] for df in all_synthetic_data],
        'delta': [df.attrs['delta'] for df in all_synthetic_data]
    })

    params_summary_file = f'{synthetic_dir}/parameters_summary.csv'
    params_summary.to_csv(params_summary_file, index=False)
    print(f"‚úì Parameters summary saved to: {params_summary_file}")

    # Create consolidated CSV with all datasets
    consolidated_data = []
    for idx, df in enumerate(all_synthetic_data):
        df_copy = df.copy()
        df_copy['dataset'] = idx + 1
        consolidated_data.append(df_copy)

    consolidated_df = pd.concat(consolidated_data, ignore_index=True)
    consolidated_file = f'{synthetic_dir}/all_synthetic_data.csv'
    consolidated_df.to_csv(consolidated_file, index=False)
    print(f"‚úì Consolidated data saved to: {consolidated_file}")

    # Summary statistics
    print("\n" + "="*70)
    print("SYNTHETIC DATA SUMMARY")
    print("="*70)

    print(f"\nGenerated datasets: {n_datasets}")
    print(f"Observations per dataset: {n_obs_per_dataset}")
    print(f"Total synthetic observations: {n_datasets * n_obs_per_dataset}")

    print(f"\nParameter ranges across datasets:")
    print(f"  N‚ÇÄ:     [{params_summary['N0'].min():.6f}, {params_summary['N0'].max():.6f}]")
    print(f"  ŒîœÉ‚ÇÄ:    [{params_summary['Delta0'].min():.6f}, {params_summary['Delta0'].max():.6f}]")
    print(f"  Œ≤:      [{params_summary['beta'].min():.3f}, {params_summary['beta'].max():.3f}]")
    print(f"  Œª:      [{params_summary['lambda'].min():.3f}, {params_summary['lambda'].max():.3f}]")
    print(f"  Œ¥:      [{params_summary['delta'].min():.3f}, {params_summary['delta'].max():.3f}]")

    # Visualize first few datasets
    print("\nVisualizing first 3 synthetic datasets vs observed data...")

    n_plots = min(3, n_datasets)
    fig, axes = plt.subplots(1, n_plots + 1, figsize=(5 * (n_plots + 1), 5))

    if n_plots == 1:
        axes = [axes]

    # Plot observed data
    ax = axes[0]
    ax.scatter(cycles_data, stress_data, c='blue', s=50, alpha=0.7,
              label='Observed', edgecolors='black', linewidths=0.5)
    ax.set_xlabel('Cycles (N)', fontsize=10, fontweight='bold')
    ax.set_ylabel('Stress (ŒîœÉ)', fontsize=10, fontweight='bold')
    ax.set_xscale('log')
    ax.set_title(f'Observed Data\n({len(cycles_data)} obs)', fontsize=11, fontweight='bold')
    ax.grid(True, alpha=0.3, which='both')
    ax.legend(fontsize=9)

    # Plot first few synthetic datasets
    colors = ['red', 'green', 'orange']
    for i in range(n_plots):
        ax = axes[i + 1]
        df = all_synthetic_data[i]
        ax.scatter(df['N'].values, df['Deltasigma'].values, c=colors[i], s=30, alpha=0.6,
                  label=f'Synthetic {i+1}', marker='^', edgecolors='black', linewidths=0.5)
        ax.set_xlabel('Cycles (N)', fontsize=10, fontweight='bold')
        ax.set_ylabel('Stress (ŒîœÉ)', fontsize=10, fontweight='bold')
        ax.set_xscale('log')
        ax.set_title(f'Synthetic Dataset {i+1}\n({len(df)} obs)', fontsize=11, fontweight='bold')
        ax.grid(True, alpha=0.3, which='both')
        ax.legend(fontsize=9)

    plt.tight_layout()
    comparison_plot = f'{synthetic_dir}/synthetic_vs_observed_comparison.png'
    plt.savefig(comparison_plot, dpi=150, bbox_inches='tight')
    plt.show()
    print(f"‚úì Comparison plot saved to: {comparison_plot}")

    # Create ZIP file with all synthetic data
    print("\nCreating ZIP archive with all synthetic data...")
    import zipfile

    zip_filename = f'{synthetic_dir}.zip'
    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
        # Add all files in the synthetic directory
        for root, dirs, files in os.walk(synthetic_dir):
            for file in files:
                file_path = os.path.join(root, file)
                arcname = os.path.relpath(file_path, os.path.dirname(synthetic_dir))
                zipf.write(file_path, arcname)

    print(f"‚úì ZIP archive created: {zip_filename}")
    print(f"  Size: {os.path.getsize(zip_filename) / (1024*1024):.2f} MB")

    # Download in Colab
    if IN_COLAB:
        print("\nDownloading ZIP file...")
        try:
            files.download(zip_filename)
            print(f"  ‚úì Downloaded: {zip_filename}")
        except Exception as e:
            print(f"  ‚úó Could not download: {e}")
            print(f"  Files are available in: {synthetic_dir}/")
    else:
        print(f"\n‚úì Files ready in: {synthetic_dir}/")
        print(f"‚úì ZIP archive: {zip_filename}")

    print("\n" + "="*70)
    print("FILES IN ZIP ARCHIVE:")
    print("="*70)
    print(f"  ‚Ä¢ synthetic_XXXX.csv ({n_datasets} files) - Individual datasets in CSV format")
    print(f"  ‚Ä¢ synthetic_XXXX.txt ({n_datasets} files) - Individual datasets in R/OpenBUGS format")
    print(f"  ‚Ä¢ all_synthetic_data.csv - All datasets combined")
    print(f"  ‚Ä¢ parameters_summary.csv - Parameters used for each dataset")
    print(f"  ‚Ä¢ synthetic_vs_observed_comparison.png - Visual comparison")

    print("\n‚úì Synthetic data generation complete!")

else:
    print("\n‚úì Skipping synthetic data generation")

# Download files in Colab
if IN_COLAB:
    print("\n" + "="*70)
    print("DOWNLOAD FILES (Google Colab)")
    print("="*70)
    print("\nDownloading all result files...")

    files_to_download = [
        'data_exploration_weibull.png',
        'mle_results.png',
        'mle_estimates.txt',
        'prior_distributions.png',
        'trace_plots.png',
        'posterior_distributions.png',
        'percentiles_of_percentiles_shaded.png',
        'percentiles_of_percentiles_all_curves.png',
        'fatigue_posterior_weibull.nc',
        'fatigue_summary_weibull.csv',
        'percentiles_of_percentiles.npz'
    ]

    for fname in files_to_download:
        if os.path.exists(fname):
            try:
                files.download(fname)
                print(f"  ‚úì Downloaded: {fname}")
            except:
                print(f"  ‚úó Could not download: {fname}")

    print("\n‚úì Download complete!")