# -*- coding: utf-8 -*-
"""PRUEBA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eeZvaAeDL4tURwXNuvjrZeUWbGBe3aFk
"""

# -*- coding: utf-8 -*-
"""FatigueExamplePythonWeibullFINAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19SkVwgmv7o5LDu8BCdrT4WaRQBcJ7dD_
"""

# ============================================================================
#   FATIGUE ANALYSIS - WEIBULL MODEL (MLE + BAYESIAN)
#   Castillo-Canteli Dimensionless Formulation
#   WITH GOOGLE COLAB FILE UPLOAD SUPPORT
# ============================================================================
# Physical justification:
# - Lower bounded problem (N > 0) ‚Üí Weibull distribution for minima
# - Gumbel is limiting case (Œ≤ ‚Üí ‚àû) naturally captured by Weibull
# ============================================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pymc as pm
import arviz as az
import pytensor.tensor as pt
from scipy import stats
from scipy.optimize import minimize, differential_evolution
import warnings
import sys
import os
import glob
import shutil
import zipfile
from datetime import datetime
import re

warnings.filterwarnings('ignore')

plt.rcParams['figure.figsize'] = [14, 8]

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

print("="*70)
print("FATIGUE ANALYSIS - WEIBULL MODEL")
print("Maximum Likelihood Estimation + Bayesian Inference")
print("="*70)
print(f"PyMC version: {pm.__version__}")
print(f"ArviZ version: {az.__version__}")

# ============================================================================
# DETECT GOOGLE COLAB ENVIRONMENT
# ============================================================================

try:
    from google.colab import files
    IN_COLAB = True
    print("\n‚úì Google Colab detected")
except:
    IN_COLAB = False
    print("\n‚úì Running in local environment")

# ============================================================================
# DATA SOURCE SELECTION
# ============================================================================

print("\n" + "="*70)
print("DATA SOURCE SELECTION")
print("="*70)

def get_user_choice():
    """Get user choice for data source."""
    print("\nChoose data source:")
    print("  1 - Use Holmen example data")
    print("  2 - Upload your own data file")

    while True:
        choice = input("\nEnter choice (1 or 2): ").strip()
        if choice in ['1', '2']:
            return choice
        print("Invalid choice. Please enter 1 or 2.")

# Determine data source
if IN_COLAB:
    try:
        data_choice = get_user_choice()
    except (EOFError, KeyboardInterrupt):
        print("\nInput not available, defaulting to file upload mode")
        data_choice = '2'
else:
    print("\nNot in Colab - defaulting to Holmen example")
    print("To use your own data, set data_choice = '2' before this section")
    data_choice = '1'

use_example = (data_choice == '1')

print(f"\nSelected mode: {'Example (Holmen)' if use_example else 'Upload file'}")

# ============================================================================
# DATA LOADING
# ============================================================================

print("\n" + "="*70)
print("DATA LOADING")
print("="*70)

if use_example:
    print("\n‚úì Using Holmen example data...")

    # Holmen example data
    stress_data = np.array([
        0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95,
        0.95, 0.95, 0.95, 0.95, 0.95,
        0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90,
        0.90, 0.90, 0.90, 0.90, 0.90,
        0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825,
        0.825, 0.825, 0.825, 0.825, 0.825,
        0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75,
        0.75, 0.75, 0.75, 0.75, 0.75,
        0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675,
        0.675, 0.675, 0.675, 0.675, 0.675,
    ])

    cycles_data = np.array([
        0.037, 0.072, 0.074, 0.076, 0.083, 0.085, 0.105, 0.109, 0.120, 0.123, 0.143,
        0.203, 0.206, 0.217, 0.257, 0.201, 0.216, 0.226, 0.252, 0.257, 0.295, 0.311,
        0.342, 0.356, 0.451, 0.457, 0.509, 0.540, 0.680, 1.129, 1.246, 1.258, 1.460,
        1.492, 2.400, 2.410, 2.590, 2.903, 3.330, 3.590, 3.847, 4.110, 4.820, 5.560,
        5.598, 6.710, 9.930, 12.600, 15.580, 16.190, 17.280, 18.620, 20.300, 24.900,
        26.260, 27.940, 36.350, 48.420, 50.090, 67.340, 102.950, 280.320, 339.830,
        366.900, 485.620, 658.960, 896.330, 1241.760, 1250.200, 1329.780, 1399.830,
        1459.140, 3294.820, 12709, 14373
    ])

    # Default configuration for Holmen
    config = {
        'n_tune': 1000,
        'n_draws': 1000,
        'n_chains': 1,
        'n_stress_points': 50,
        'n_param_samples': 1000,
        'percentiles_base': [0.01, 0.15, 0.50, 0.85, 0.99],
        'percentiles_sub': [0.01, 0.15, 0.50, 0.85, 0.99],
        'cv_N0': 0.25,
        'cv_Delta0': 0.15,
        'cv_beta': 0.30,
        'cv_lambda': 0.25,
        'cv_delta': 0.30,
        'target_accept': 0.95
    }

    dataset_name = "Holmen"

else:
    print("\nüìÅ Please upload your data file...")
    print("\nFile format requirements:")
    print("  Option 1: CSV or Excel file (.csv, .xlsx, .xls)")
    print("    - Required columns: 'N' (cycles), 'Deltasigma' (stress)")
    print("    - Optional: configuration parameters")
    print("\n  Option 2: R/OpenBUGS format (.txt, .dat)")
    print("    - list(M=..., N=c(...), Deltasigma=c(...), ...)")

    if IN_COLAB:
        print("\nUploading file...")
        uploaded = files.upload()

        if len(uploaded) == 0:
            print("No file uploaded. Using Holmen example instead.")
            use_example = True
        else:
            filename = list(uploaded.keys())[0]
            print(f"\n‚úì File uploaded: {filename}")
            print(f"  File size: {len(uploaded[filename])} bytes")

            # In Colab, the file is already in the current directory
            # Verify it exists
            if not os.path.exists(filename):
                print(f"  ‚úó Warning: File not found at {filename}")
                print(f"  Current directory: {os.getcwd()}")
                print(f"  Files in directory: {os.listdir('.')[:10]}")
            else:
                print(f"  ‚úì File verified at: {os.path.abspath(filename)}")
    else:
        filename = input("\nEnter data file path: ").strip()
        if not os.path.exists(filename):
            print(f"File not found: {filename}")
            print("Using Holmen example instead.")
            use_example = True

    if not use_example:
        # Read data file
        try:
            # Check file extension
            if filename.endswith(('.txt', '.dat')):
                # R/OpenBUGS format parser
                print("\n  Detected R/OpenBUGS format...")
                print(f"  Reading file: {filename}")

                with open(filename, 'r') as f:
                    content = f.read()

                print(f"  File size: {len(content)} characters")

                # Extract N values
                n_match = re.search(r'N\s*=\s*c\(([^)]+)\)', content)
                if not n_match:
                    raise ValueError("Could not find 'N=c(...)' in file")
                n_values_str = n_match.group(1)
                n_values = [float(x.strip()) for x in n_values_str.split(',') if x.strip()]

                # Extract Deltasigma values
                delta_match = re.search(r'Deltasigma\s*=\s*c\(([^)]+)\)', content)
                if not delta_match:
                    raise ValueError("Could not find 'Deltasigma=c(...)' in file")
                delta_values_str = delta_match.group(1)
                delta_values = [float(x.strip()) for x in delta_values_str.split(',') if x.strip()]

                if len(n_values) != len(delta_values):
                    raise ValueError(f"Length mismatch: N has {len(n_values)} values, Deltasigma has {len(delta_values)}")

                cycles_data = np.array(n_values)
                stress_data = np.array(delta_values)

                print(f"  ‚úì Parsed N: {len(cycles_data)} values")
                print(f"  ‚úì Parsed Deltasigma: {len(stress_data)} values")
                print(f"  ‚úì Created numpy arrays successfully")
                print(f"    cycles_data shape: {cycles_data.shape}")
                print(f"    stress_data shape: {stress_data.shape}")

                # Default configuration
                config = {
                    'n_tune': 1000,
                    'n_draws': 2000,
                    'n_chains': 2,
                    'n_stress_points': 50,
                    'n_param_samples': 1000,
                    'percentiles_base': [0.01, 0.15, 0.50, 0.85, 0.99],
                    'percentiles_sub': [0.01, 0.15, 0.50, 0.85, 0.99],
                    'cv_N0': 0.25,
                    'cv_Delta0': 0.15,
                    'cv_beta': 0.30,
                    'cv_lambda': 0.25,
                    'cv_delta': 0.30,
                    'target_accept': 0.95
                }

                # Try to extract optional parameters from R list
                # M (number of observations)
                m_match = re.search(r'M\s*=\s*(\d+)', content)
                if m_match:
                    M_val = int(m_match.group(1))
                    print(f"  ‚úì Found M={M_val} (verification: {len(cycles_data)} observations)")

                # ns (number of stress points)
                ns_match = re.search(r'ns\s*=\s*(\d+)', content)
                if ns_match:
                    config['n_stress_points'] = int(ns_match.group(1))
                    print(f"  ‚úì Found ns={config['n_stress_points']}")

                # np (number of percentiles)
                np_match = re.search(r'np\s*=\s*(\d+)', content)
                if np_match:
                    np_val = int(np_match.group(1))
                    print(f"  ‚úì Found np={np_val}")

                # percentiles
                perc_match = re.search(r'percentiles\s*=\s*c\(([\d.,\s]+)\)', content)
                if perc_match:
                    perc_values = [float(x.strip()) for x in perc_match.group(1).split(',')]
                    config['percentiles_base'] = perc_values
                    config['percentiles_sub'] = perc_values
                    print(f"  ‚úì Found percentiles: {perc_values}")

                # n_tune (warmup)
                tune_match = re.search(r'n_tune\s*=\s*(\d+)', content)
                if tune_match:
                    config['n_tune'] = int(tune_match.group(1))
                    print(f"  ‚úì Found n_tune={config['n_tune']}")

                # n_draws (samples)
                draws_match = re.search(r'n_draws\s*=\s*(\d+)', content)
                if draws_match:
                    config['n_draws'] = int(draws_match.group(1))
                    print(f"  ‚úì Found n_draws={config['n_draws']}")

                # n_chains
                chains_match = re.search(r'n_chains\s*=\s*(\d+)', content)
                if chains_match:
                    config['n_chains'] = int(chains_match.group(1))
                    print(f"  ‚úì Found n_chains={config['n_chains']}")

                # Coefficient of variation parameters
                cv_params = ['cv_N0', 'cv_Delta0', 'cv_beta', 'cv_lambda', 'cv_delta']
                for cv_param in cv_params:
                    cv_match = re.search(rf'{cv_param}\s*=\s*([\d.]+)', content)
                    if cv_match:
                        config[cv_param] = float(cv_match.group(1))
                        print(f"  ‚úì Found {cv_param}={config[cv_param]}")

                # target_accept
                accept_match = re.search(r'target_accept\s*=\s*([\d.]+)', content)
                if accept_match:
                    config['target_accept'] = float(accept_match.group(1))
                    print(f"  ‚úì Found target_accept={config['target_accept']}")

                dataset_name = filename.split('.')[0]

                print(f"\n  ‚úì‚úì‚úì R FORMAT FILE LOADED SUCCESSFULLY")
                print(f"      Dataset: {dataset_name}")
                print(f"      use_example = {use_example}")
                print(f"      Variables 'cycles_data' and 'stress_data' created")

            elif filename.endswith('.csv'):
                df = pd.read_csv(filename)
            elif filename.endswith(('.xlsx', '.xls')):
                df = pd.read_excel(filename)
            else:
                raise ValueError("Unsupported file format. Use .csv, .xlsx, .xls, .txt, or .dat")

            # CSV/Excel format (if not R format)
            if filename.endswith(('.csv', '.xlsx', '.xls')):
                # Extract required columns
                if 'N' not in df.columns or 'Deltasigma' not in df.columns:
                    raise ValueError("File must contain 'N' and 'Deltasigma' columns")

                cycles_data = df['N'].values
                stress_data = df['Deltasigma'].values

                # Remove NaN values
                valid_mask = ~(np.isnan(cycles_data) | np.isnan(stress_data))
                cycles_data = cycles_data[valid_mask]
                stress_data = stress_data[valid_mask]

                print(f"\n‚úì Data loaded: {len(cycles_data)} observations")

                # Try to read configuration parameters
                config = {
                    'n_tune': 1000,
                    'n_draws': 2000,
                    'n_chains': 2,
                    'n_stress_points': 50,
                    'n_param_samples': 1000,
                    'percentiles_base': [0.01, 0.15, 0.50, 0.85, 0.99],
                    'percentiles_sub': [0.01, 0.15, 0.50, 0.85, 0.99],
                    'cv_N0': 0.25,
                    'cv_Delta0': 0.15,
                    'cv_beta': 0.30,
                    'cv_lambda': 0.25,
                    'cv_delta': 0.30,
                    'target_accept': 0.95
                }

                # Check for configuration in the file
                config_params = {
                    'n_tune': ['n_tune', 'tune', 'warmup'],
                    'n_draws': ['n_draws', 'draws', 'samples'],
                    'n_chains': ['n_chains', 'chains'],
                    'n_stress_points': ['n_stress_points', 'stress_points'],
                    'n_param_samples': ['n_param_samples', 'param_samples'],
                    'cv_N0': ['cv_N0', 'cv_n0'],
                    'cv_Delta0': ['cv_Delta0', 'cv_delta0'],
                    'cv_beta': ['cv_beta'],
                    'cv_lambda': ['cv_lambda'],
                    'cv_delta': ['cv_delta'],
                    'target_accept': ['target_accept', 'accept_rate']
                }

                for param, possible_names in config_params.items():
                    for name in possible_names:
                        if name in df.columns:
                            value = df[name].dropna().iloc[0]
                            config[param] = float(value) if param.startswith('cv_') or param == 'target_accept' else int(value)
                            print(f"  Found config: {param} = {config[param]}")
                            break

                # Try to read percentiles
                if 'percentiles_base' in df.columns:
                    perc_base = df['percentiles_base'].dropna().values
                    config['percentiles_base'] = perc_base.tolist()
                    print(f"  Found percentiles_base: {config['percentiles_base']}")

                if 'percentiles_sub' in df.columns:
                    perc_sub = df['percentiles_sub'].dropna().values
                    config['percentiles_sub'] = perc_sub.tolist()
                    print(f"  Found percentiles_sub: {config['percentiles_sub']}")

                dataset_name = filename.split('.')[0]

        except Exception as e:
            print(f"\n‚úó Error reading file: {e}")
            import traceback
            print("\nFull error traceback:")
            traceback.print_exc()
            print("\nUsing Holmen example instead.")
            use_example = True

# Final check - if reverted to example OR if data wasn't loaded, use Holmen
if use_example or 'stress_data' not in locals() or 'cycles_data' not in locals():
    if not use_example:
        print("\n‚ö† Data not loaded properly, using Holmen example instead.")
    # Load Holmen data (same as above)
    stress_data = np.array([
        0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95,
        0.95, 0.95, 0.95, 0.95, 0.95,
        0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90,
        0.90, 0.90, 0.90, 0.90, 0.90,
        0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825,
        0.825, 0.825, 0.825, 0.825, 0.825,
        0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75,
        0.75, 0.75, 0.75, 0.75, 0.75,
        0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675,
        0.675, 0.675, 0.675, 0.675, 0.675,
    ])
    cycles_data = np.array([
        0.037, 0.072, 0.074, 0.076, 0.083, 0.085, 0.105, 0.109, 0.120, 0.123, 0.143,
        0.203, 0.206, 0.217, 0.257, 0.201, 0.216, 0.226, 0.252, 0.257, 0.295, 0.311,
        0.342, 0.356, 0.451, 0.457, 0.509, 0.540, 0.680, 1.129, 1.246, 1.258, 1.460,
        1.492, 2.400, 2.410, 2.590, 2.903, 3.330, 3.590, 3.847, 4.110, 4.820, 5.560,
        5.598, 6.710, 9.930, 12.600, 15.580, 16.190, 17.280, 18.620, 20.300, 24.900,
        26.260, 27.940, 36.350, 48.420, 50.090, 67.340, 102.950, 280.320, 339.830,
        366.900, 485.620, 658.960, 896.330, 1241.760, 1250.200, 1329.780, 1399.830,
        1459.140, 3294.820, 12709, 14373
    ])
    config = {
        'n_tune': 1000,
        'n_draws': 2000,
        'n_chains': 2,
        'n_stress_points': 50,
        'n_param_samples': 1000,
        'percentiles_base': [0.01, 0.15, 0.50, 0.85, 0.99],
        'percentiles_sub': [0.01, 0.15, 0.50, 0.85, 0.99],
        'cv_N0': 0.25,
        'cv_Delta0': 0.15,
        'cv_beta': 0.30,
        'cv_lambda': 0.25,
        'cv_delta': 0.30,
        'target_accept': 0.95
    }
    dataset_name = "Holmen"

N_min = cycles_data.min()
N_max = cycles_data.max()

print(f"\n{'='*70}")
print(f"DATASET: {dataset_name}")
print(f"{'='*70}")
print(f"Data source: {'Holmen example (built-in)' if use_example else 'User-uploaded file'}")
print(f"Observations: {len(cycles_data)}")
print(f"Stress range: [{stress_data.min():.3f}, {stress_data.max():.3f}] (dimensionless)")
print(f"Cycles range: [{N_min:.4f}, {N_max:.2f}]")
print(f"\nFirst 5 N values: {cycles_data[:5]}")
print(f"First 5 Deltasigma values: {stress_data[:5]}")

print(f"\n{'='*70}")
print("ANALYSIS CONFIGURATION")
print(f"{'='*70}")
print(f"MCMC Settings:")
print(f"  Tune (warmup): {config['n_tune']}")
print(f"  Draws (samples): {config['n_draws']}")
print(f"  Chains: {config['n_chains']}")
print(f"  Target acceptance: {config['target_accept']}")
print(f"\nPercentiles:")
print(f"  Stress points: {config['n_stress_points']}")
print(f"  Parameter samples: {config['n_param_samples']}")
print(f"  Base percentiles: {[int(p*100) for p in config['percentiles_base']]}%")
print(f"  Sub-percentiles: {[int(p*100) for p in config['percentiles_sub']]}%")
print(f"\nPrior Coefficients of Variation (CV):")
print(f"  CV(N‚ÇÄ) = {config['cv_N0']:.2f}")
print(f"  CV(ŒîœÉ‚ÇÄ) = {config['cv_Delta0']:.2f}")
print(f"  CV(Œ≤) = {config['cv_beta']:.2f}")
print(f"  CV(Œª) = {config['cv_lambda']:.2f}")
print(f"  CV(Œ¥) = {config['cv_delta']:.2f}")

# ============================================================================
# DATA VISUALIZATION
# ============================================================================

print("\n" + "="*70)
print("DATA VISUALIZATION")
print("="*70)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

ax = axes[0]
# Use different colors for each unique stress level
unique_stresses = np.unique(stress_data)
colors_plot = plt.cm.rainbow(np.linspace(0, 1, len(unique_stresses)))

for i, stress_level in enumerate(unique_stresses):
    mask = stress_data == stress_level
    ax.scatter(cycles_data[mask], [stress_level]*np.sum(mask),
              alpha=0.7, s=50, label=f'{stress_level:.3f}',
              color=colors_plot[i], edgecolors='black', linewidths=0.5)

ax.set_xlabel('Cycles to Failure (N)', fontsize=12, fontweight='bold')
ax.set_ylabel('Stress (Deltasigma, dimensionless)', fontsize=12, fontweight='bold')
ax.set_xscale('log')
ax.set_title(f'S-N Data ({dataset_name}) - Weibull Model', fontsize=13, fontweight='bold')
if len(unique_stresses) <= 10:
    ax.legend(fontsize=9, loc='upper right')
ax.grid(True, alpha=0.3, which='both')

ax = axes[1]
ax.scatter(np.log(cycles_data), np.log(stress_data), alpha=0.6, s=40, color='darkblue')
ax.set_xlabel('ln(Cycles)', fontsize=12, fontweight='bold')
ax.set_ylabel('ln(Stress)', fontsize=12, fontweight='bold')
ax.set_title('Log-Log Space', fontsize=13, fontweight='bold')
ax.grid(True, alpha=0.3)

plt.tight_layout()
data_exploration_file = 'data_exploration_weibull.png'
plt.savefig(data_exploration_file, dpi=150, bbox_inches='tight')
plt.show()

print("‚úì Data visualization completed!")

# ============================================================================
# PHASE 1: MAXIMUM LIKELIHOOD ESTIMATION (WEIBULL)
# ============================================================================

print("\n" + "="*70)
print("PHASE 1: MAXIMUM LIKELIHOOD ESTIMATION")
print("="*70)

def weibull_log_likelihood(params, stress, cycles):
    """
    Weibull log-likelihood for fatigue data.
    Castillo-Canteli dimensionless formulation.
    """
    N0, Delta0, beta, lambda_param, delta = params

    # Validations
    if N0 <= 0 or Delta0 <= 0 or beta <= 0 or delta <= 0:
        return -np.inf

    # Dimensionless transformation
    log_N_dimensionless = np.log(cycles) - np.log(N0)
    r = np.log(stress) - np.log(Delta0)

    # Check for valid r values
    if np.any(np.abs(r) < 1e-10):
        return -np.inf

    # Weibull parameters for minima
    mu_Y = (-lambda_param - delta) / r
    sigma_Y = delta / (beta * np.abs(r))

    # Check valid sigma
    if np.any(sigma_Y <= 0):
        return -np.inf

    # Standardized variable for Weibull (Gumbel for minima parametrization)
    z = (log_N_dimensionless - mu_Y) / sigma_Y

    # Weibull log-likelihood for minima
    log_lik = -np.log(sigma_Y) + z - np.exp(z)

    # Check for invalid values
    if not np.all(np.isfinite(log_lik)):
        return -np.inf

    return np.sum(log_lik)

def negative_log_likelihood(params, stress, cycles):
    """Negative log-likelihood for minimization."""
    return -weibull_log_likelihood(params, stress, cycles)

# Initial guess based on physical reasoning
N0_init = N_min * 0.5
Delta0_init = stress_data.min() * 0.7
beta_init = 3.0
lambda_init = -8.0
delta_init = 2.0

initial_params = np.array([N0_init, Delta0_init, beta_init, lambda_init, delta_init])

print("\nInitial guess:")
print(f"  N‚ÇÄ = {N0_init:.4f}")
print(f"  ŒîœÉ‚ÇÄ = {Delta0_init:.4f}")
print(f"  Œ≤ = {beta_init:.4f}")
print(f"  Œª = {lambda_init:.4f}")
print(f"  Œ¥ = {delta_init:.4f}")

# Parameter bounds for optimization
bounds = [
    (0.001, N_min * 0.9),
    (stress_data.min() * 0.4, stress_data.min() * 0.99),
    (0.5, 15.0),
    (-12.0, -4.0),
    (0.5, 5.0)
]

print("\nRunning global optimization (differential evolution)...")
print("This may take a few minutes...")

result_global = differential_evolution(
    negative_log_likelihood,
    bounds=bounds,
    args=(stress_data, cycles_data),
    seed=RANDOM_SEED,
    maxiter=1000,
    popsize=30,
    tol=1e-7,
    atol=1e-7,
    workers=1,
    updating='deferred',
    polish=True
)

print("\n‚úì Global optimization completed!")
print(f"  Success: {result_global.success}")
print(f"  Log-likelihood: {-result_global.fun:.2f}")
print(f"  Iterations: {result_global.nit}")

mle_params = result_global.x

print("\n" + "="*70)
print("MLE ESTIMATES (WEIBULL MODEL)")
print("="*70)
print(f"  N‚ÇÄ (reference cycles) = {mle_params[0]:.6f}")
print(f"  ŒîœÉ‚ÇÄ (reference stress) = {mle_params[1]:.6f}")
print(f"  Œ≤ (shape parameter)    = {mle_params[2]:.6f}")
print(f"  Œª (location param)     = {mle_params[3]:.6f}")
print(f"  Œ¥ (scale param)        = {mle_params[4]:.6f}")
print(f"\n  Log-likelihood = {-result_global.fun:.2f}")

# Save MLE results
mle_estimates_file = 'mle_estimates.txt'
with open(mle_estimates_file, 'w') as f:
    f.write(f"MLE ESTIMATES - WEIBULL MODEL - {dataset_name}\n")
    f.write("="*50 + "\n")
    f.write(f"N0 = {mle_params[0]:.8f}\n")
    f.write(f"Delta0 = {mle_params[1]:.8f}\n")
    f.write(f"beta = {mle_params[2]:.8f}\n")
    f.write(f"lambda = {mle_params[3]:.8f}\n")
    f.write(f"delta = {mle_params[4]:.8f}\n")
    f.write(f"Log-likelihood = {-result_global.fun:.8f}\n")

print("\n‚úì MLE estimates saved to 'mle_estimates.txt'")

# Visualize MLE fit
print("\nVisualizing MLE fit...")

fig, ax = plt.subplots(figsize=(14, 8))

# Plot observed data
ax.scatter(cycles_data, stress_data, c='black', s=80,
          alpha=0.9, label='Observed data', zorder=100, marker='o',
          edgecolors='white', linewidths=1.5)

# Plot MLE curves for different percentiles
stress_range_plot = np.linspace(stress_data.min() * 0.97, stress_data.max() * 1.03, 100)
percentiles_mle = [0.01, 0.10, 0.50, 0.90, 0.99]
colors_mle = ['#8B0000', '#FF8C00', '#228B22', '#4169E1', '#8B008B']

N0_mle, Delta0_mle, beta_mle, lambda_mle, delta_mle = mle_params

for perc, color in zip(percentiles_mle, colors_mle):
    N_perc = []
    for stress in stress_range_plot:
        if stress > 0 and Delta0_mle > 0:
            r = np.log(stress / Delta0_mle)
            if abs(r) > 1e-10:
                mu_Y = (-lambda_mle - delta_mle) / r
                sigma_Y = delta_mle / (beta_mle * abs(r))

                # Weibull quantile for minima (Gumbel parametrization)
                z_p = np.log(-np.log(1 - perc))
                Y_p = mu_Y + sigma_Y * z_p
                N_p = N0_mle * np.exp(Y_p)

                if N_p > 0 and np.isfinite(N_p):
                    N_perc.append(N_p)
                else:
                    N_perc.append(np.nan)
            else:
                N_perc.append(np.nan)
        else:
            N_perc.append(np.nan)

    valid_mask = ~np.isnan(N_perc)
    if np.sum(valid_mask) > 0:
        ax.plot(np.array(N_perc)[valid_mask], stress_range_plot[valid_mask],
               color=color, linewidth=2.5, label=f'P{int(perc*100)} MLE',
               alpha=0.8)

ax.set_xlabel('Cycles to Failure (N)', fontsize=13, fontweight='bold')
ax.set_ylabel('Stress (Deltasigma, dimensionless)', fontsize=13, fontweight='bold')
ax.set_xscale('log')
ax.set_title(f'MLE Fit - Weibull Model ({dataset_name})',
            fontsize=14, fontweight='bold')
ax.legend(fontsize=10, loc='upper right')
ax.grid(True, alpha=0.3, which='both')
ax.set_xlim([N_min * 0.1, N_max * 10])

plt.tight_layout()
mle_results_file = 'mle_results.png'
plt.savefig(mle_results_file, dpi=150, bbox_inches='tight')
plt.show()

print("‚úì MLE visualization completed!")

# ============================================================================
# PHASE 2: DEFINE INFORMATIVE PRIORS FROM MLE
# ============================================================================

print("\n" + "="*70)
print("PHASE 2: DEFINING BAYESIAN PRIORS FROM MLE")
print("="*70)

# Use MLE estimates to define informative Normal priors
N0_mle_val = mle_params[0]
Delta0_mle_val = mle_params[1]
beta_mle_val = mle_params[2]
lambda_mle_val = mle_params[3]
delta_mle_val = mle_params[4]

# Prior standard deviations using CV from config
N0_std = N0_mle_val * config['cv_N0']
Delta0_std = Delta0_mle_val * config['cv_Delta0']
beta_std = beta_mle_val * config['cv_beta']
lambda_std = abs(lambda_mle_val) * config['cv_lambda']
delta_std = delta_mle_val * config['cv_delta']

print("\nInformative Normal Priors (centered on MLE):")
print(f"  N‚ÇÄ     ~ Normal({N0_mle_val:.4f}, {N0_std:.4f})  [CV={config['cv_N0']:.2f}]")
print(f"  ŒîœÉ‚ÇÄ    ~ Normal({Delta0_mle_val:.6f}, {Delta0_std:.6f})  [CV={config['cv_Delta0']:.2f}]")
print(f"  Œ≤      ~ Normal({beta_mle_val:.4f}, {beta_std:.4f})  [CV={config['cv_beta']:.2f}]")
print(f"  Œª      ~ Normal({lambda_mle_val:.4f}, {lambda_std:.4f})  [CV={config['cv_lambda']:.2f}]")
print(f"  Œ¥      ~ Normal({delta_mle_val:.4f}, {delta_std:.4f})  [CV={config['cv_delta']:.2f}]")

print("\nThese priors will help achieve faster convergence in Bayesian inference.")

# ============================================================================
# PHASE 3: BAYESIAN INFERENCE WITH WEIBULL MODEL
# ============================================================================

print("\n" + "="*70)
print("PHASE 3: BAYESIAN INFERENCE")
print("="*70)

initial_values = {
    'N0': N0_mle_val,
    'Delta0': Delta0_mle_val,
    'beta': beta_mle_val,
    'lambda_param': lambda_mle_val,
    'delta': delta_mle_val
}

with pm.Model() as fatigue_model:
    # PRIORS - Informative Normal distributions based on MLE
    N0 = pm.TruncatedNormal('N0', mu=N0_mle_val, sigma=N0_std,
                            lower=0.001, upper=N_min)
    Delta0 = pm.TruncatedNormal('Delta0', mu=Delta0_mle_val, sigma=Delta0_std,
                                lower=stress_data.min() * 0.3, upper=stress_data.min())
    beta = pm.TruncatedNormal('beta', mu=beta_mle_val, sigma=beta_std,
                             lower=0.5, upper=20.0)
    lambda_param = pm.Normal('lambda_param', mu=lambda_mle_val, sigma=lambda_std)
    delta = pm.TruncatedNormal('delta', mu=delta_mle_val, sigma=delta_std,
                              lower=0.1, upper=10.0)

    # Transform to dimensionless log-space
    log_N_dimensionless = pt.log(cycles_data) - pt.log(N0)
    r = pt.log(stress_data) - pt.log(Delta0)

    # Weibull parameters for minima
    mu_Y = (-lambda_param - delta) / r
    sigma_Y = delta / (beta * pt.abs(r) + 1e-8)

    # Standardized variable
    z = (log_N_dimensionless - mu_Y) / (sigma_Y + 1e-8)

    # Log-likelihood (Weibull for minima, Gumbel parametrization)
    log_lik = -pt.log(sigma_Y + 1e-8) + z - pt.exp(z)

    # Total likelihood
    likelihood = pm.Potential('likelihood', pt.sum(log_lik))

print("‚úì Bayesian model defined with informative priors")

# ============================================================================
# PHASE 4: PRIOR PREDICTIVE CHECK
# ============================================================================

print("\n" + "="*70)
print("PRIOR PREDICTIVE CHECK")
print("="*70)

print("\nSampling from prior predictive distribution...")

with fatigue_model:
    prior_predictive = pm.sample_prior_predictive(
        samples=500,
        random_seed=RANDOM_SEED
    )

print("‚úì Prior predictive samples generated")

# Visualize prior distributions
print("\nVisualizing prior distributions...")

fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

var_names = ['N0', 'Delta0', 'beta', 'lambda_param', 'delta']
var_labels = ['N‚ÇÄ (reference cycles)', 'ŒîœÉ‚ÇÄ (reference stress)', 'Œ≤', 'Œª', 'Œ¥']

for ax, var, label in zip(axes[:5], var_names, var_labels):
    samples = prior_predictive.prior[var].values.flatten()

    ax.hist(samples, bins=50, alpha=0.7, color='steelblue', edgecolor='black')
    ax.set_xlabel(label, fontsize=11, fontweight='bold')
    ax.set_ylabel('Frequency', fontsize=11)
    ax.set_title(f'Prior: {label}', fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3)

    ax.axvline(np.median(samples), color='red', linestyle='--', linewidth=2, label='Median')

    # Add MLE value line
    if var == 'N0':
        ax.axvline(N0_mle_val, color='green', linestyle=':', linewidth=2, label='MLE')
    elif var == 'Delta0':
        ax.axvline(Delta0_mle_val, color='green', linestyle=':', linewidth=2, label='MLE')
    elif var == 'beta':
        ax.axvline(beta_mle_val, color='green', linestyle=':', linewidth=2, label='MLE')
    elif var == 'lambda_param':
        ax.axvline(lambda_mle_val, color='green', linestyle=':', linewidth=2, label='MLE')
    elif var == 'delta':
        ax.axvline(delta_mle_val, color='green', linestyle=':', linewidth=2, label='MLE')

    ax.legend()

axes[5].axis('off')

plt.suptitle(f'Prior Distributions (Centered on MLE) - {dataset_name}', fontsize=14, fontweight='bold')
plt.tight_layout()
prior_distributions_file = 'prior_distributions.png'
plt.savefig(prior_distributions_file, dpi=150, bbox_inches='tight')
plt.show()

print("‚úì Prior distributions visualized")

# ============================================================================
# PHASE 5: SAMPLE FROM POSTERIOR
# ============================================================================

print("\n" + "="*70)
print("SAMPLING FROM POSTERIOR DISTRIBUTION")
print("="*70)

print("\nSampling strategy:")
print(f"  ‚Ä¢ Using informative priors from MLE")
print(f"  ‚Ä¢ Tune: {config['n_tune']}, Draws: {config['n_draws']}, Chains: {config['n_chains']}")
print(f"  ‚Ä¢ Target acceptance = {config['target_accept']}")
print("\nThis may take 5-15 minutes depending on configuration...\n")

with fatigue_model:
    trace = pm.sample(
        draws=config['n_draws'],
        tune=config['n_tune'],
        chains=config['n_chains'],
        cores=1,
        random_seed=RANDOM_SEED,
        return_inferencedata=True,
        target_accept=config['target_accept'],
        init='adapt_diag',
        initvals=initial_values
    )

print("\n‚úì Sampling completed!")

# ============================================================================
# PHASE 6: CONVERGENCE DIAGNOSTICS
# ============================================================================

print("\n" + "="*70)
print("CONVERGENCE DIAGNOSTICS")
print("="*70)

summary_table = az.summary(
    trace,
    var_names=['N0', 'Delta0', 'beta', 'lambda_param', 'delta'],
    hdi_prob=0.95
)
print("\n--- Posterior Summary ---")
print(summary_table)

rhat_values = az.rhat(trace)
print(f"\n--- R-hat Diagnostic (should be < 1.01) ---")
all_rhat_good = True
for var in ['N0', 'Delta0', 'beta', 'lambda_param', 'delta']:
    rhat_val = rhat_values[var].values
    status = "‚úì" if rhat_val < 1.01 else "‚úó"
    if rhat_val >= 1.01:
        all_rhat_good = False
    print(f"{var:15s}: {rhat_val:.4f} {status}")

ess_values = az.ess(trace)
print(f"\n--- Effective Sample Size (should be > 1000) ---")
all_ess_good = True
for var in ['N0', 'Delta0', 'beta', 'lambda_param', 'delta']:
    ess_val = ess_values[var].values
    status = "‚úì" if ess_val > 1000 else "‚úó"
    if ess_val <= 1000:
        all_ess_good = False
    print(f"{var:15s}: {ess_val:.0f} {status}")

n_divergences = trace.sample_stats.diverging.sum().values
total_samples = config['n_draws'] * config['n_chains']
print(f"\n--- Divergence Check ---")
print(f"Number of divergent transitions: {n_divergences}")
if n_divergences > 0:
    print(f"‚ö† Warning: {n_divergences} divergences detected")
    print(f"  Divergence rate: {n_divergences / total_samples:.2%}")
else:
    print("‚úì No divergences detected!")

print(f"\n--- Overall Convergence Assessment ---")
if all_rhat_good and all_ess_good and n_divergences == 0:
    print("‚úì‚úì‚úì EXCELLENT: Model has converged successfully!")
elif all_rhat_good and n_divergences < 50:
    print("‚úì‚úì GOOD: Model convergence is acceptable")
else:
    print("‚úó WARNING: Model may not have converged properly")

# Trace plots
print("\nGenerating trace plots...")
fig, axes = plt.subplots(5, 2, figsize=(14, 16))
az.plot_trace(
    trace,
    var_names=['N0', 'Delta0', 'beta', 'lambda_param', 'delta'],
    compact=False,
    axes=axes
)
plt.suptitle(f'Trace Plots and Posterior Distributions - {dataset_name}',
            fontsize=14, fontweight='bold', y=1.001)
plt.tight_layout()
trace_plots_file = 'trace_plots.png'
plt.savefig(trace_plots_file, dpi=150, bbox_inches='tight')
plt.show()

# Posterior distributions
fig = plt.figure(figsize=(15, 10))
az.plot_posterior(
    trace,
    var_names=['N0', 'Delta0', 'beta', 'lambda_param', 'delta'],
    hdi_prob=0.95,
    figsize=(15, 10)
)
plt.suptitle(f'Posterior Distributions with 95% HDI - {dataset_name}',
            fontsize=14, fontweight='bold')
plt.tight_layout()
posterior_distributions_file = 'posterior_distributions.png'
plt.savefig(posterior_distributions_file, dpi=150, bbox_inches='tight')
plt.show()

print("\n‚úì Diagnostics completed!")

# ============================================================================
# PHASE 7: PERCENTILES OF PERCENTILES
# ============================================================================

print("\n" + "="*70)
print("COMPUTING PERCENTILES OF PERCENTILES")
print("="*70)

posterior = trace.posterior

# Define stress range for plotting
stress_margin = 0.03
stress_min_plot = stress_data.min() - stress_margin
stress_max_plot = stress_data.max() + stress_margin
stress_min_plot = max(stress_min_plot, 0.01)

stress_range = np.linspace(stress_min_plot, stress_max_plot, config['n_stress_points'])

percentiles_base = config['percentiles_base']
percentiles_sub = config['percentiles_sub']

print(f"\nConfiguration:")
print(f"  Stress points: {config['n_stress_points']}")
print(f"  Parameter samples: {config['n_param_samples']}")
print(f"  Base percentiles: {[int(p*100) for p in percentiles_base]}%")
print(f"  Sub-percentiles: {[int(p*100) for p in percentiles_sub]}%")

def compute_percentile(stress, N0, Delta0, beta, lambda_p, delta, prob):
    """Compute N_p for given stress and failure probability (Weibull)."""
    if stress <= 0 or N0 <= 0 or Delta0 <= 0 or beta <= 0 or delta <= 0:
        return np.nan
    if prob <= 0 or prob >= 1:
        return np.nan

    try:
        r = np.log(stress / Delta0)
        if abs(r) < 1e-10:
            return np.nan

        mu_Y = (-lambda_p - delta) / r
        sigma_Y = delta / (beta * abs(r))

        # Weibull quantile for minima (Gumbel parametrization)
        z_p = np.log(-np.log(1 - prob))
        Y_p = mu_Y + sigma_Y * z_p
        N_p = N0 * np.exp(Y_p)

        if not np.isfinite(N_p) or N_p <= 0:
            return np.nan

        if N_p < 1e-6 or N_p > 1e10:
            return np.nan

        return N_p
    except:
        return np.nan

# Extract posterior samples
N0_samples = posterior['N0'].values.flatten()
Delta0_samples = posterior['Delta0'].values.flatten()
beta_samples = posterior['beta'].values.flatten()
lambda_samples = posterior['lambda_param'].values.flatten()
delta_samples = posterior['delta'].values.flatten()

total_samples = len(N0_samples)
sample_indices = np.random.choice(total_samples, size=min(config['n_param_samples'], total_samples), replace=False)

# Storage for percentiles of percentiles
percentiles_of_percentiles = {}

print("\nComputing percentiles of percentiles...")

for perc_base_idx, perc_base in enumerate(percentiles_base):
    print(f"\n  Processing base percentile P{int(perc_base*100)}...")

    percentile_matrix = np.zeros((config['n_stress_points'], len(sample_indices)))

    for i, stress in enumerate(stress_range):
        if i % max(1, config['n_stress_points']//5) == 0:
            print(f"    Stress point {i+1}/{config['n_stress_points']}...")

        for j, idx in enumerate(sample_indices):
            N0_s = N0_samples[idx]
            Delta0_s = Delta0_samples[idx]
            beta_s = beta_samples[idx]
            lambda_s = lambda_samples[idx]
            delta_s = delta_samples[idx]

            N_p = compute_percentile(stress, N0_s, Delta0_s, beta_s, lambda_s, delta_s, perc_base)

            if not np.isnan(N_p):
                percentile_matrix[i, j] = N_p
            else:
                percentile_matrix[i, j] = np.nan

    n_valid = np.sum(~np.isnan(percentile_matrix), axis=1)
    print(f"    Valid values per stress: min={n_valid.min()}, max={n_valid.max()}, mean={n_valid.mean():.1f}")

    # Sort each row
    percentile_matrix_sorted = np.sort(percentile_matrix, axis=1)

    # Extract sub-percentiles
    percentile_indices = [int(p * (len(sample_indices) - 1)) for p in percentiles_sub]
    perc_of_perc_curves = percentile_matrix_sorted[:, percentile_indices]

    percentiles_of_percentiles[perc_base] = perc_of_perc_curves

print("\n‚úì Percentiles of percentiles computed!")

# ============================================================================
# PHASE 8: PLOT WITH SHADED REGIONS
# ============================================================================

print("\nPlotting percentiles of percentiles with shaded regions...")

fig, ax = plt.subplots(figsize=(18, 11), facecolor='white')
ax.set_facecolor('white')

# Colors for shading and lines
colors_base = ['#8B0000', '#FF8C00', '#228B22', '#4169E1', '#8B008B']
colors_shaded = ['#FFB6B9', '#FFCC80', '#A5D6A7', '#90CAF9', '#CE93D8']

# Generate labels for percentiles
perc_names = [f'P{int(p*100)}' for p in percentiles_base]

# Ensure we have enough colors
if len(percentiles_base) > len(colors_base):
    colors_base = plt.cm.rainbow(np.linspace(0, 1, len(percentiles_base)))
    colors_shaded = plt.cm.rainbow(np.linspace(0, 1, len(percentiles_base)))

# FIRST: Plot shaded regions
print("  Plotting shaded uncertainty bands...")
for base_idx, (perc_base, perc_name) in enumerate(zip(percentiles_base, perc_names)):
    curves = percentiles_of_percentiles[perc_base]

    curve_p_min = curves[:, 0]  # First sub-percentile
    curve_p_max = curves[:, -1]  # Last sub-percentile

    valid_mask = (~np.isnan(curve_p_min)) & (~np.isnan(curve_p_max))

    if np.sum(valid_mask) > 3:
        color_shaded = colors_shaded[base_idx % len(colors_shaded)]
        ax.fill_betweenx(stress_range[valid_mask],
                curve_p_min[valid_mask],
                curve_p_max[valid_mask],
                color=color_shaded,
                alpha=0.85,
                label=f'{perc_name} uncertainty band',
                zorder=base_idx + 1)

# SECOND: Plot intermediate sub-percentile curves
print("  Plotting intermediate sub-percentile curves...")
if len(percentiles_sub) > 2:
    for base_idx, (perc_base, perc_name) in enumerate(zip(percentiles_base, perc_names)):
        curves = percentiles_of_percentiles[perc_base]
        color_base = colors_base[base_idx % len(colors_base)]

        for sub_idx in range(1, len(percentiles_sub)-1):
            curve = curves[:, sub_idx]
            valid_mask = ~np.isnan(curve)
            if np.sum(valid_mask) > 3:
                linestyle = '--' if sub_idx == 1 else ':'
                ax.plot(curve[valid_mask], stress_range[valid_mask],
                       color=color_base, linestyle=linestyle, linewidth=1.2,
                       alpha=0.5, zorder=10 + base_idx)

# THIRD: Plot extreme sub-percentiles
print("  Plotting extreme sub-percentile curves...")
for base_idx, (perc_base, perc_name) in enumerate(zip(percentiles_base, perc_names)):
    curves = percentiles_of_percentiles[perc_base]
    color_base = colors_base[base_idx % len(colors_base)]

    curve_p_min = curves[:, 0]
    valid_mask = ~np.isnan(curve_p_min)
    if np.sum(valid_mask) > 3:
        ax.plot(curve_p_min[valid_mask], stress_range[valid_mask],
               color=color_base, linestyle=(0, (5, 2)), linewidth=1.5,
               alpha=0.7, zorder=15 + base_idx)

    curve_p_max = curves[:, -1]
    valid_mask = ~np.isnan(curve_p_max)
    if np.sum(valid_mask) > 3:
        ax.plot(curve_p_max[valid_mask], stress_range[valid_mask],
               color=color_base, linestyle=(0, (1, 1)), linewidth=1.5,
               alpha=0.7, zorder=15 + base_idx)

# FOURTH: Plot median curves - THICK
print("  Plotting median curves...")
median_idx = len(percentiles_sub) // 2
for base_idx, (perc_base, perc_name) in enumerate(zip(percentiles_base, perc_names)):
    curves = percentiles_of_percentiles[perc_base]
    color_base = colors_base[base_idx % len(colors_base)]

    curve_median = curves[:, median_idx]

    valid_mask = ~np.isnan(curve_median)

    if np.sum(valid_mask) > 3:
        ax.plot(curve_median[valid_mask], stress_range[valid_mask],
               color=color_base, linewidth=3.5,
               label=f'{perc_name} (median curve)',
               zorder=50 + base_idx)

# FIFTH: Plot observed data on TOP
print("  Plotting observed data...")
ax.scatter(cycles_data, stress_data, c='black', s=80,
          alpha=0.9, label='Observed data', zorder=100, marker='o',
          edgecolors='white', linewidths=1.5)

cycles_min_plot = N_min * 0.1
cycles_max_plot = N_max * 10.0

ax.set_xlim([cycles_min_plot, cycles_max_plot])

ax.set_xlabel('Cycles to Failure (N)', fontsize=15, fontweight='bold')
ax.set_ylabel('Stress (Deltasigma, dimensionless)', fontsize=15, fontweight='bold')
ax.set_xscale('log')
ax.set_title(f'Percentiles of Percentiles with Uncertainty Bands ({dataset_name})\n' +
            'Bayesian Weibull Model - Castillo-Canteli Formulation',
            fontsize=15, fontweight='bold', pad=20)

# Legend
ax.legend(loc='upper right', fontsize=10, framealpha=0.98, ncol=2,
         columnspacing=1.0, handlelength=2.5,
         title='Base Percentiles & Uncertainty Bands',
         title_fontsize=11)

ax.grid(True, alpha=0.3, which='both', linestyle='-', linewidth=0.5)
ax.set_xlim([cycles_min_plot, cycles_max_plot])
ax.set_ylim([stress_min_plot - 0.01, stress_max_plot + 0.01])

plt.tight_layout()
percentiles_shaded_file = 'percentiles_of_percentiles_shaded.png'
plt.savefig(percentiles_shaded_file, dpi=250, bbox_inches='tight')
plt.show()

print("‚úì Shaded plot completed!")

# ============================================================================
# PHASE 9: ALTERNATIVE PLOT - ALL CURVES
# ============================================================================

print("\nCreating alternative plot with all curves...")

fig, ax = plt.subplots(figsize=(18, 11), facecolor='white')
ax.set_facecolor('white')

# Plot shaded regions first
for base_idx, perc_base in enumerate(percentiles_base):
    curves = percentiles_of_percentiles[perc_base]
    curve_p_min = curves[:, 0]
    curve_p_max = curves[:, -1]
    valid_mask = (~np.isnan(curve_p_min)) & (~np.isnan(curve_p_max))

    if np.sum(valid_mask) > 3:
        color_shaded = colors_shaded[base_idx % len(colors_shaded)]
        ax.fill_betweenx(stress_range[valid_mask],
                        curve_p_min[valid_mask],
                        curve_p_max[valid_mask],
                        color=color_shaded,
                        alpha=0.5,
                        zorder=base_idx + 1)

# Plot ALL curves with labels
linestyles_sub = ['-', '--', '-', ':', (0, (1, 1))]
if len(percentiles_sub) > len(linestyles_sub):
    linestyles_sub = ['-'] * len(percentiles_sub)

linewidths_sub = [1.5] * len(percentiles_sub)
linewidths_sub[median_idx] = 3.5  # Median thicker

alpha_sub = [0.6] * len(percentiles_sub)
alpha_sub[median_idx] = 1.0  # Median fully opaque

curve_count = 0
for base_idx, (perc_base, perc_name_base) in enumerate(zip(percentiles_base, perc_names)):
    curves = percentiles_of_percentiles[perc_base]
    color_base = colors_base[base_idx % len(colors_base)]

    perc_names_sub = [f'P{int(p*100)}' for p in percentiles_sub]

    for sub_idx in range(len(percentiles_sub)):
        curve = curves[:, sub_idx]
        valid_mask = ~np.isnan(curve)

        if np.sum(valid_mask) > 3:
            if sub_idx == median_idx:
                label = f'{perc_name_base}-{perc_names_sub[sub_idx]} (median)'
                zorder_val = 50 + base_idx
            elif sub_idx in [0, len(percentiles_sub)-1]:
                label = f'{perc_name_base}-{perc_names_sub[sub_idx]} (bound)'
                zorder_val = 30 + base_idx
            else:
                label = f'{perc_name_base}-{perc_names_sub[sub_idx]}'
                zorder_val = 20 + base_idx

            ax.plot(curve[valid_mask], stress_range[valid_mask],
                   color=color_base, linestyle=linestyles_sub[sub_idx],
                   linewidth=linewidths_sub[sub_idx],
                   label=label, alpha=alpha_sub[sub_idx], zorder=zorder_val)
            curve_count += 1

# Plot observed data
ax.scatter(cycles_data, stress_data, c='black', s=80,
          alpha=0.9, label='Observed data', zorder=100, marker='o',
          edgecolors='white', linewidths=1.5)

total_curves = len(percentiles_base) * len(percentiles_sub)
print(f"  Total curves plotted: {curve_count}/{total_curves}")

ax.set_xlabel('Cycles to Failure (N)', fontsize=15, fontweight='bold')
ax.set_ylabel('Stress (Deltasigma, dimensionless)', fontsize=15, fontweight='bold')
ax.set_xscale('log')
ax.set_title(f'All {total_curves} Percentile Curves ({len(percentiles_base)} Base √ó {len(percentiles_sub)} Sub) - {dataset_name}\n' +
            'Complete Uncertainty Quantification',
            fontsize=15, fontweight='bold', pad=20)

ax.legend(loc='upper right', fontsize=8, framealpha=0.95, ncol=3,
         columnspacing=0.6, handlelength=2.0, handletextpad=0.5,
         title='Base-Sub Percentile Curves', title_fontsize=9,
         borderpad=0.5, labelspacing=0.3)

ax.grid(True, alpha=0.3, which='both', linestyle='-', linewidth=0.5)
ax.set_xlim([cycles_min_plot, cycles_max_plot])
ax.set_ylim([stress_min_plot - 0.01, stress_max_plot + 0.01])

plt.tight_layout()
percentiles_all_curves_file = 'percentiles_of_percentiles_all_curves.png'
plt.savefig(percentiles_all_curves_file, dpi=250, bbox_inches='tight')
plt.show()

print("‚úì Alternative plot completed!")

# ============================================================================
# PHASE 10: SUMMARY
# ============================================================================

print("\n" + "="*70)
print("SUMMARY")
print("="*70)

print(f"\nDataset: {dataset_name}")
print(f"Observations: {len(cycles_data)}")

print("\n1. MLE ESTIMATES:")
print(f"   N‚ÇÄ = {mle_params[0]:.6f}")
print(f"   ŒîœÉ‚ÇÄ = {mle_params[1]:.6f}")
print(f"   Œ≤ = {mle_params[2]:.3f}")
print(f"   Œª = {mle_params[3]:.3f}")
print(f"   Œ¥ = {mle_params[4]:.3f}")

print("\n2. POSTERIOR MEDIANS:")
print(f"   N‚ÇÄ = {posterior['N0'].median().values:.6f}")
print(f"   ŒîœÉ‚ÇÄ = {posterior['Delta0'].median().values:.6f}")
print(f"   Œ≤ = {posterior['beta'].median().values:.3f}")
print(f"   Œª = {posterior['lambda_param'].median().values:.3f}")
print(f"   Œ¥ = {posterior['delta'].median().values:.3f}")

print("\n3. CONVERGENCE:")
print(f"   R-hat all < 1.01: {'‚úì' if all_rhat_good else '‚úó'}")
print(f"   ESS all > 1000: {'‚úì' if all_ess_good else '‚úó'}")
print(f"   Divergences: {n_divergences}")
if n_divergences > 0:
    print(f"   Divergence rate: {n_divergences/total_samples*100:.2f}%")

print("\n4. PERCENTILES OF PERCENTILES:")
print(f"   Total curves generated: {len(percentiles_base) * len(percentiles_sub)} ({len(percentiles_base)} base √ó {len(percentiles_sub)} sub)")
print(f"   Two visualization approaches:")
print(f"     ‚Ä¢ Main plot: Shaded bands with key curves")
print(f"     ‚Ä¢ Alternative: All curves individually labeled")

print("\n5. FILES CREATED:")
print("   ‚Ä¢ data_exploration_weibull.png")
print("   ‚Ä¢ mle_results.png")
print("   ‚Ä¢ mle_estimates.txt")
print("   ‚Ä¢ prior_distributions.png")
print("   ‚Ä¢ trace_plots.png")
print("   ‚Ä¢ posterior_distributions.png")
print("   ‚Ä¢ percentiles_of_percentiles_shaded.png")
print("   ‚Ä¢ percentiles_of_percentiles_all_curves.png")

print("\n" + "="*70)
print("ANALYSIS COMPLETE!")
print("="*70)

# Save results
fatigue_posterior_file = 'fatigue_posterior_weibull.nc'
trace.to_netcdf(fatigue_posterior_file)

fatigue_summary_file = 'fatigue_summary_weibull.csv'
summary_table.to_csv(fatigue_summary_file)

perc_of_perc_data = {}
for perc_base in percentiles_base:
    perc_of_perc_data[f'P{int(perc_base*100)}'] = percentiles_of_percentiles[perc_base]

percentiles_file = 'percentiles_of_percentiles.npz'
np.savez(percentiles_file,
         stress_range=stress_range,
         percentiles_base=percentiles_base,
         percentiles_sub=percentiles_sub,
         **perc_of_perc_data)

print("\n‚úì Results saved:")
print("  ‚Ä¢ fatigue_posterior_weibull.nc")
print("  ‚Ä¢ fatigue_summary_weibull.csv")
print("  ‚Ä¢ percentiles_of_percentiles.npz")

# ============================================================================
# PHASE 11: SYNTHETIC DATA GENERATION FROM POSTERIOR SAMPLES
# ============================================================================

print("\n" + "="*70)
print("SYNTHETIC DATA GENERATION FROM POSTERIOR SAMPLES")
print("="*70)

print("\nWould you like to generate synthetic datasets using posterior samples?")
print("(This uses the actual MCMC samples, excluding warmup)")
generate_synthetic = input("\nGenerate synthetic data? (y/n): ").strip().lower()

synthetic_dir = None
synthetic_files = []

if generate_synthetic == 'y':

    # Ask for number of observations per dataset
    while True:
        try:
            n_obs_per_dataset = int(input("\nHow many observations per synthetic dataset? (e.g., 75, 100, 360): ").strip())
            if n_obs_per_dataset > 0 and n_obs_per_dataset <= 20000:
                break
            else:
                print("Please enter a number between 1 and 20000")
        except ValueError:
            print("Invalid input. Please enter a number.")

    # Ask for number of datasets to generate
    while True:
        try:
            n_datasets = int(input("\nHow many synthetic datasets to generate? (e.g., 1, 10, 100): ").strip())
            if n_datasets > 0 and n_datasets <= 20000:
                break
            else:
                print("Please enter a number between 1 and 20000")
        except ValueError:
            print("Invalid input. Please enter a number.")

    print(f"\n{'='*70}")
    print(f"CONFIGURATION:")
    print(f"  Observations per dataset: {n_obs_per_dataset}")
    print(f"  Number of datasets: {n_datasets}")
    print(f"  Total posterior samples available: {len(N0_samples)}")
    print(f"{'='*70}")

    # Check if we have enough samples
    if n_datasets > len(N0_samples):
        print(f"\n‚ö† Warning: Requested {n_datasets} datasets but only {len(N0_samples)} posterior samples available.")
        print(f"  Will generate {min(n_datasets, len(N0_samples))} datasets (one per unique sample).")
        n_datasets = min(n_datasets, len(N0_samples))

    # Select random posterior samples (one per dataset)
    selected_sample_indices = np.random.choice(len(N0_samples), size=n_datasets, replace=False)

    print(f"\n‚úì Selected {n_datasets} random posterior samples")

    # Create directory for synthetic data
    synthetic_dir = f'synthetic_data_{dataset_name}'
    if not os.path.exists(synthetic_dir):
        os.makedirs(synthetic_dir)
    print(f"‚úì Created directory: {synthetic_dir}/")

    # Generate synthetic datasets
    print(f"\nGenerating {n_datasets} synthetic datasets...")

    all_synthetic_data = []

    for dataset_idx, sample_idx in enumerate(selected_sample_indices):
        if (dataset_idx + 1) % max(1, n_datasets // 10) == 0 or dataset_idx == 0:
            print(f"  Generating dataset {dataset_idx + 1}/{n_datasets}...")

        # Get parameter values from this posterior sample
        N0_synth = N0_samples[sample_idx]
        Delta0_synth = Delta0_samples[sample_idx]
        beta_synth = beta_samples[sample_idx]
        lambda_synth = lambda_samples[sample_idx]
        delta_synth = delta_samples[sample_idx]

        # Generate stress levels (uniform distribution within observed range)
        stress_synthetic = np.random.uniform(
            low=stress_data.min(),
            high=stress_data.max(),
            size=n_obs_per_dataset
        )

        # Generate cycles for each stress level using the Weibull model
        cycles_synthetic = np.zeros(n_obs_per_dataset)

        for i in range(n_obs_per_dataset):
            stress = stress_synthetic[i]

            # Weibull parameters for this stress level
            r = np.log(stress / Delta0_synth)
            mu_Y = (-lambda_synth - delta_synth) / r
            sigma_Y = delta_synth / (beta_synth * abs(r))

            # Generate random Gumbel (for minima) variable
            u = np.random.uniform(0, 1)
            z = np.log(-np.log(1 - u))  # Inverse CDF

            Y = mu_Y + sigma_Y * z
            N = N0_synth * np.exp(Y)

            cycles_synthetic[i] = N

        # Create DataFrame for this dataset
        synthetic_df = pd.DataFrame({
            'N': cycles_synthetic,
            'Deltasigma': stress_synthetic
        })

        # Sort by stress level
        synthetic_df = synthetic_df.sort_values('Deltasigma').reset_index(drop=True)

        # Store parameters used
        synthetic_df.attrs['N0'] = N0_synth
        synthetic_df.attrs['Delta0'] = Delta0_synth
        synthetic_df.attrs['beta'] = beta_synth
        synthetic_df.attrs['lambda'] = lambda_synth
        synthetic_df.attrs['delta'] = delta_synth
        synthetic_df.attrs['sample_idx'] = sample_idx

        all_synthetic_data.append(synthetic_df)

        # Save individual CSV file
        csv_filename = f'{synthetic_dir}/synthetic_{dataset_idx+1:04d}.csv'
        synthetic_df.to_csv(csv_filename, index=False)
        synthetic_files.append(csv_filename)

        # Save individual R/OpenBUGS format file
        r_filename = f'{synthetic_dir}/synthetic_{dataset_idx+1:04d}.txt'
        with open(r_filename, 'w') as f:
            f.write(f"# Synthetic dataset {dataset_idx+1}/{n_datasets}\n")
            f.write(f"# Generated from posterior sample {sample_idx}\n")
            f.write(f"# Parameters: N0={N0_synth:.6f}, Delta0={Delta0_synth:.6f}, beta={beta_synth:.3f}, lambda={lambda_synth:.3f}, delta={delta_synth:.3f}\n")
            f.write(f"list(M={n_obs_per_dataset},\n")
            f.write(f"ns=50,\n")
            f.write(f"np=5,\n")
            f.write(f"percentiles=c({','.join([str(p) for p in config['percentiles_base']])}),\n")

            # Write Deltasigma
            f.write("Deltasigma=c(")
            deltasigma_str = ','.join([f"{s:.3f}" for s in synthetic_df['Deltasigma'].values])
            f.write(deltasigma_str)
            f.write("),\n")

            # Write N
            f.write("N=c(")
            n_str = ','.join([f"{n:.1f}" for n in synthetic_df['N'].values])
            f.write(n_str)
            f.write(")\n")
            f.write(")\n")
        synthetic_files.append(r_filename)

    print(f"\n‚úì Generated {n_datasets} synthetic datasets!")
    print(f"  Files saved in: {synthetic_dir}/")

    # Create summary file with parameters used
    params_summary = pd.DataFrame({
        'dataset': [f'synthetic_{i+1:04d}' for i in range(n_datasets)],
        'sample_idx': [df.attrs['sample_idx'] for df in all_synthetic_data],
        'N0': [df.attrs['N0'] for df in all_synthetic_data],
        'Delta0': [df.attrs['Delta0'] for df in all_synthetic_data],
        'beta': [df.attrs['beta'] for df in all_synthetic_data],
        'lambda': [df.attrs['lambda'] for df in all_synthetic_data],
        'delta': [df.attrs['delta'] for df in all_synthetic_data]
    })

    params_summary_file = f'{synthetic_dir}/parameters_summary.csv'
    params_summary.to_csv(params_summary_file, index=False)
    synthetic_files.append(params_summary_file)
    print(f"‚úì Parameters summary saved to: {params_summary_file}")

    # Create consolidated CSV with all datasets
    consolidated_data = []
    for idx, df in enumerate(all_synthetic_data):
        df_copy = df.copy()
        df_copy['dataset'] = idx + 1
        consolidated_data.append(df_copy)

    consolidated_df = pd.concat(consolidated_data, ignore_index=True)
    consolidated_file = f'{synthetic_dir}/all_synthetic_data.csv'
    consolidated_df.to_csv(consolidated_file, index=False)
    synthetic_files.append(consolidated_file)
    print(f"‚úì Consolidated data saved to: {consolidated_file}")

    # Summary statistics
    print("\n" + "="*70)
    print("SYNTHETIC DATA SUMMARY")
    print("="*70)

    print(f"\nGenerated datasets: {n_datasets}")
    print(f"Observations per dataset: {n_obs_per_dataset}")
    print(f"Total synthetic observations: {n_datasets * n_obs_per_dataset}")

    print(f"\nParameter ranges across datasets:")
    print(f"  N‚ÇÄ:     [{params_summary['N0'].min():.6f}, {params_summary['N0'].max():.6f}]")
    print(f"  ŒîœÉ‚ÇÄ:    [{params_summary['Delta0'].min():.6f}, {params_summary['Delta0'].max():.6f}]")
    print(f"  Œ≤:      [{params_summary['beta'].min():.3f}, {params_summary['beta'].max():.3f}]")
    print(f"  Œª:      [{params_summary['lambda'].min():.3f}, {params_summary['lambda'].max():.3f}]")
    print(f"  Œ¥:      [{params_summary['delta'].min():.3f}, {params_summary['delta'].max():.3f}]")

    # Visualize first few datasets
    print("\nVisualizing first 3 synthetic datasets vs observed data...")

    n_plots = min(3, n_datasets)
    fig, axes = plt.subplots(1, n_plots + 1, figsize=(5 * (n_plots + 1), 5))

    if n_plots == 1:
        axes = [axes]

    # Definir l√≠mites comunes basados en los datos observados
    common_x_min = cycles_data.min() * 0.1
    common_x_max = cycles_data.max() * 10.0
    common_y_min = max(stress_data.min() * 0.95, 0.1)
    common_y_max = min(stress_data.max() * 1.05, 1.0)

    # Plot observed data
    ax = axes[0]
    ax.scatter(cycles_data, stress_data, c='blue', s=50, alpha=0.7,
              label='Observed', edgecolors='black', linewidths=0.5)
    ax.set_xlabel('Cycles (N)', fontsize=10, fontweight='bold')
    ax.set_ylabel('Stress (ŒîœÉ)', fontsize=10, fontweight='bold')
    ax.set_xscale('log')
    ax.set_xlim([common_x_min, common_x_max])
    ax.set_ylim([common_y_min, common_y_max])
    ax.set_title(f'Observed Data\n({len(cycles_data)} obs)', fontsize=11, fontweight='bold')
    ax.grid(True, alpha=0.3, which='both')
    ax.legend(fontsize=9)

    # Plot first few synthetic datasets
    colors = ['red', 'green', 'orange']
    for i in range(n_plots):
        ax = axes[i + 1]
        df = all_synthetic_data[i]
        ax.scatter(df['N'].values, df['Deltasigma'].values, c=colors[i], s=30, alpha=0.6,
                  label=f'Synthetic {i+1}', marker='^', edgecolors='black', linewidths=0.5)
        ax.set_xlabel('Cycles (N)', fontsize=10, fontweight='bold')
        ax.set_ylabel('Stress (ŒîœÉ)', fontsize=10, fontweight='bold')
        ax.set_xscale('log')
        # Usar los mismos l√≠mites en todos los gr√°ficos
        ax.set_xlim([common_x_min, common_x_max])
        ax.set_ylim([common_y_min, common_y_max])
        ax.set_title(f'Synthetic Dataset {i+1}\n({len(df)} obs)', fontsize=11, fontweight='bold')
        ax.grid(True, alpha=0.3, which='both')
        ax.legend(fontsize=9)

    plt.tight_layout()
    comparison_plot = f'{synthetic_dir}/synthetic_vs_observed_comparison.png'
    plt.savefig(comparison_plot, dpi=150, bbox_inches='tight')
    plt.show()
    synthetic_files.append(comparison_plot)
    print(f"‚úì Comparison plot saved to: {comparison_plot}")

    # Create ZIP file with all synthetic data
    print("\nCreating ZIP archive with all synthetic data...")

    zip_filename = f'{synthetic_dir}.zip'
    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
        # Add all files in the synthetic directory
        for root, dirs, files in os.walk(synthetic_dir):
            for file in files:
                file_path = os.path.join(root, file)
                arcname = os.path.relpath(file_path, os.path.dirname(synthetic_dir))
                zipf.write(file_path, arcname)

    print(f"‚úì ZIP archive created: {zip_filename}")
    print(f"  Size: {os.path.getsize(zip_filename) / (1024*1024):.2f} MB")
    synthetic_files.append(zip_filename)

    # Download in Colab
    if IN_COLAB:
        print("\nDownloading ZIP file...")
        try:
            files.download(zip_filename)
            print(f"  ‚úì Downloaded: {zip_filename}")
        except Exception as e:
            print(f"  ‚úó Could not download: {e}")
            print(f"  Files are available in: {synthetic_dir}/")
    else:
        print(f"\n‚úì Files ready in: {synthetic_dir}/")
        print(f"‚úì ZIP archive: {zip_filename}")

    print("\n" + "="*70)
    print("FILES IN ZIP ARCHIVE:")
    print("="*70)
    print(f"  ‚Ä¢ synthetic_XXXX.csv ({n_datasets} files) - Individual datasets in CSV format")
    print(f"  ‚Ä¢ synthetic_XXXX.txt ({n_datasets} files) - Individual datasets in R/OpenBUGS format")
    print(f"  ‚Ä¢ all_synthetic_data.csv - All datasets combined")
    print(f"  ‚Ä¢ parameters_summary.csv - Parameters used for each dataset")
    print(f"  ‚Ä¢ synthetic_vs_observed_comparison.png - Visual comparison")

    print("\n‚úì Synthetic data generation complete!")

else:
    print("\n‚úì Skipping synthetic data generation")

# ============================================================================
# CREATE ZIP WITH ALL RESULTS (OPCIONAL) - INCLUYE ARCHIVO .PY
# ============================================================================

print("\n" + "="*70)
print("CREATE ZIP ARCHIVE WITH ALL RESULTS")
print("="*70)

create_zip = input("\n¬øCrear archivo ZIP con todos los resultados? (y/n): ").strip().lower()

zip_files_created = []
py_script_filename = None
synthetic_zip_created = None  # Para rastrear si se cre√≥ un ZIP de datos sint√©ticos

if create_zip == 'y':
    # Nombre corto del ZIP general
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    zip_filename = f"fatigue_results_{dataset_name[:15]}_{timestamp}.zip"

    # Crear archivo .py COMPLETO con informaci√≥n
    py_script_filename = f"analysis_info_{dataset_name[:10]}_{timestamp}.py"
    print(f"\nüíæ Creando archivo .py con informaci√≥n completa...")

    # Crear contenido COMPLETO para el archivo .py
    full_code = f'''# -*- coding: utf-8 -*-
"""
FATIGUE ANALYSIS - WEIBULL MODEL
================================
Dataset: {dataset_name}
Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Observations: {len(cycles_data)}

SUMMARY
-------
This analysis performed Maximum Likelihood Estimation (MLE) and Bayesian
inference using the Weibull model with Castillo-Canteli dimensionless
formulation for fatigue data analysis.

RESULTS
-------
MLE PARAMETERS:
  N‚ÇÄ (reference cycles) = {mle_params[0]:.6f}
  ŒîœÉ‚ÇÄ (reference stress) = {mle_params[1]:.6f}
  Œ≤ (shape parameter)    = {mle_params[2]:.6f}
  Œª (location param)     = {mle_params[3]:.6f}
  Œ¥ (scale param)        = {mle_params[4]:.6f}
  Log-likelihood         = {-result_global.fun:.2f}

BAYESIAN POSTERIOR MEDIANS:
  N‚ÇÄ = {posterior['N0'].median().values:.6f}
  ŒîœÉ‚ÇÄ = {posterior['Delta0'].median().values:.6f}
  Œ≤ = {posterior['beta'].median().values:.3f}
  Œª = {posterior['lambda_param'].median().values:.3f}
  Œ¥ = {posterior['delta'].median().values:.3f}

MCMC CONFIGURATION:
  Tune (warmup): {config['n_tune']}
  Draws (samples): {config['n_draws']}
  Chains: {config['n_chains']}
  Target acceptance: {config['target_accept']}

CONVERGENCE DIAGNOSTICS:
  R-hat all < 1.01: {'‚úì' if all_rhat_good else '‚úó'}
  ESS all > 1000: {'‚úì' if all_ess_good else '‚úó'}
  Divergences: {n_divergences}
  Divergence rate: {n_divergences/total_samples*100:.2f}%

DATA CHARACTERISTICS:
  Stress range: [{stress_data.min():.3f}, {stress_data.max():.3f}]
  Cycles range: [{N_min:.4f}, {N_max:.2f}]

PERCENTILES ANALYZED:
  Base percentiles: {[int(p*100) for p in config['percentiles_base']]}%
  Sub-percentiles: {[int(p*100) for p in config['percentiles_sub']]}%
  Stress points: {config['n_stress_points']}
  Parameter samples: {config['n_param_samples']}

PRIOR DISTRIBUTIONS (CV):
  CV(N‚ÇÄ) = {config['cv_N0']:.2f}
  CV(ŒîœÉ‚ÇÄ) = {config['cv_Delta0']:.2f}
  CV(Œ≤) = {config['cv_beta']:.2f}
  CV(Œª) = {config['cv_lambda']:.2f}
  CV(Œ¥) = {config['cv_delta']:.2f}

FILES INCLUDED IN THIS ZIP:
---------------------------
IMAGES:
1. data_exploration.png - Initial data visualization
2. mle_results.png - MLE fit with percentile curves
3. prior_distributions.png - Prior distributions
4. trace_plots.png - MCMC trace plots
5. posterior_distributions.png - Posterior distributions
6. percentiles_shaded.png - Percentiles with uncertainty bands
7. percentiles_all.png - All percentile curves

DATA FILES:
8. mle_estimates.txt - MLE parameter estimates
9. fatigue_posterior.nc - MCMC samples (NetCDF format)
10. fatigue_summary.csv - Statistical summary
11. percentiles_data.npz - Percentile data (NumPy format)

DOCUMENTATION:
12. analysis_info.py - This file with analysis information
{f"13. synthetic_data/ - Synthetic datasets directory ({n_datasets} datasets)" if 'n_datasets' in locals() else ""}

ANALYSIS STEPS:
---------------
1. Data loading and exploration
2. Maximum Likelihood Estimation (differential evolution)
3. Bayesian model definition with informative priors
4. MCMC sampling (PyMC/ArviZ)
5. Convergence diagnostics
6. Percentiles of percentiles computation
7. Visualization and results export

SOFTWARE VERSIONS:
------------------
- Python
- NumPy
- PyMC
- ArviZ
- SciPy
- Matplotlib
- Pandas

NOTES:
------
‚Ä¢ This analysis uses the Weibull distribution for minima (fatigue data)
‚Ä¢ Castillo-Canteli dimensionless formulation
‚Ä¢ Informative priors centered on MLE estimates
‚Ä¢ Bayesian uncertainty quantification

To reproduce this analysis, ensure you have the required Python packages
and run the original analysis script/notebook.
"""

print("="*70)
print("FATIGUE ANALYSIS - WEIBULL MODEL - COMPLETE REPORT")
print("="*70)
print(f"Dataset: {dataset_name}")
print(f"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"Observations: {len(cycles_data)}")

print("\\n" + "="*70)
print("MLE RESULTS")
print("="*70)
print(f"N‚ÇÄ (reference cycles) = {mle_params[0]:.6f}")
print(f"ŒîœÉ‚ÇÄ (reference stress) = {mle_params[1]:.6f}")
print(f"Œ≤ (shape parameter)    = {mle_params[2]:.6f}")
print(f"Œª (location param)     = {mle_params[3]:.6f}")
print(f"Œ¥ (scale param)        = {mle_params[4]:.6f}")
print(f"Log-likelihood         = {-result_global.fun:.2f}")

print("\\n" + "="*70)
print("BAYESIAN RESULTS (POSTERIOR MEDIANS)")
print("="*70)
print(f"N‚ÇÄ = {posterior['N0'].median().values:.6f}")
print(f"ŒîœÉ‚ÇÄ = {posterior['Delta0'].median().values:.6f}")
print(f"Œ≤ = {posterior['beta'].median().values:.3f}")
print(f"Œª = {posterior['lambda_param'].median().values:.3f}")
print(f"Œ¥ = {posterior['delta'].median().values:.3f}")

print("\\n" + "="*70)
print("CONVERGENCE DIAGNOSTICS")
print("="*70)
print(f"R-hat all < 1.01: {'‚úì PASS' if all_rhat_good else '‚úó FAIL'}")
print(f"ESS all > 1000: {'‚úì PASS' if all_ess_good else '‚úó FAIL'}")
print(f"Divergences: {n_divergences}")
print(f"Divergence rate: {n_divergences/total_samples*100:.2f}%")

print("\\n" + "="*70)
print("FILES INCLUDED")
print("="*70)
print("IMAGES:")
print("1. data_exploration.png")
print("2. mle_results.png")
print("3. prior_distributions.png")
print("4. trace_plots.png")
print("5. posterior_distributions.png")
print("6. percentiles_shaded.png")
print("7. percentiles_all.png")

print("\\nDATA FILES:")
print("8. mle_estimates.txt")
print("9. fatigue_posterior.nc")
print("10. fatigue_summary.csv")
print("11. percentiles_data.npz")
print("12. analysis_info.py (this file)")

if 'n_datasets' in locals():
    print(f"\\nSYNTHETIC DATA:")
    print(f"13. synthetic_data/ directory ({n_datasets} datasets)")

print("\\n" + "="*70)
print("END OF REPORT")
print("="*70)

# Utility function to load summary
def load_summary():
    """Load and display the statistical summary."""
    try:
        import pandas as pd
        summary = pd.read_csv('fatigue_summary.csv')
        print("\\nSTATISTICAL SUMMARY:")
        print(summary.to_string())
    except FileNotFoundError:
        print("\\nNote: File 'fatigue_summary.csv' not found in current directory.")
    except Exception:
        print("\\nError loading summary: unable to load file")
if __name__ == "__main__":
    print("\\nUtility functions available:")
    print("‚Ä¢ load_summary() - Loads and displays statistical summary")
if __name__ == "__main__":
    print("\\nUtility functions available:")
    print("‚Ä¢ load_summary() - Loads and displays statistical summary")
'''

    # Guardar el archivo .py COMPLETO
    with open(py_script_filename, 'w', encoding='utf-8') as f:
        f.write(full_code)

    print(f"‚úì Python file created (complete): {py_script_filename}")

    # Lista de archivos a incluir en el ZIP (con nombres cortos)
    main_files = [
        (data_exploration_file, 'data_exploration.png'),
        (mle_results_file, 'mle_results.png'),
        (mle_estimates_file, 'mle_estimates.txt'),
        (prior_distributions_file, 'prior_distributions.png'),
        (trace_plots_file, 'trace_plots.png'),
        (posterior_distributions_file, 'posterior_distributions.png'),
        (percentiles_shaded_file, 'percentiles_shaded.png'),
        (percentiles_all_curves_file, 'percentiles_all.png'),
        (fatigue_posterior_file, 'fatigue_posterior.nc'),
        (fatigue_summary_file, 'fatigue_summary.csv'),
        (percentiles_file, 'percentiles_data.npz'),
        (py_script_filename, 'analysis_info.py')
    ]

    # Filtrar solo archivos que existen
    files_to_zip = []
    for original_file, short_name in main_files:
        if os.path.exists(original_file):
            files_to_zip.append((original_file, short_name))

    # A√±adir directorio de datos sint√©ticos SI EXISTE y NO se cre√≥ ZIP separado
    # (Quitamos la creaci√≥n del ZIP separado de datos sint√©ticos)
    if synthetic_dir and os.path.exists(synthetic_dir):
        # Incluir todo el directorio sint√©tico en el ZIP general
        for root, dirs, files in os.walk(synthetic_dir):
            for file in files:
                file_path = os.path.join(root, file)
                # Crear ruta relativa para el ZIP
                rel_path = os.path.relpath(file_path, synthetic_dir)
                zip_path = f"synthetic_data/{rel_path}"
                files_to_zip.append((file_path, zip_path))

    if files_to_zip:
        print(f"\nüì¶ Files to compress ({len(files_to_zip)}):")

        # Mostrar categor√≠as
        images = sum(1 for _, name in files_to_zip if name.endswith('.png'))
        data_files = sum(1 for _, name in files_to_zip if name.endswith(('.csv', '.txt', '.nc', '.npz')))
        py_files = sum(1 for _, name in files_to_zip if name.endswith('.py'))
        synth_files = sum(1 for _, name in files_to_zip if 'synthetic_data' in name)

        print(f"  üì∏ Images: {images}")
        print(f"  üìä Data files: {data_files}")
        print(f"  üêç Python files: {py_files}")
        if synth_files > 0:
            print(f"  üî¢ Synthetic data files: {synth_files}")

        # Crear ZIP GENERAL
        total_size = 0
        with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for original_file, zip_name in files_to_zip:
                if os.path.exists(original_file):
                    zipf.write(original_file, zip_name)
                    file_size = os.path.getsize(original_file)
                    total_size += file_size

        print(f"\n‚úÖ MAIN ZIP created: {zip_filename}")
        print(f"   üì¶ Total size: {total_size/(1024*1024):.2f} MB")
        print(f"   üìÑ Total files: {len(files_to_zip)}")

        zip_files_created.append(zip_filename)

        # MOSTRAR CONTENIDO DETALLADO
        print("\n" + "="*60)
        print("DETAILED ZIP CONTENTS:")
        print("="*60)
        with zipfile.ZipFile(zip_filename, 'r') as zipf:
            file_list = zipf.namelist()

            # Primero definir todas las categor√≠as excepto 'Other'
            categories = {
                'Images (.png)': [f for f in file_list if f.endswith('.png')],
                'Data files': [f for f in file_list if f.endswith(('.csv', '.txt', '.nc', '.npz'))],
                'Python files': [f for f in file_list if f.endswith('.py')],
                'Synthetic data': [f for f in file_list if 'synthetic_data' in f]
            }

            # Ahora calcular 'Other' usando las categor√≠as ya definidas
            all_categorized = []
            for cat_files in categories.values():
                all_categorized.extend(cat_files)

            categories['Other'] = [f for f in file_list if f not in all_categorized]

            file_count = 0
            for category, files in categories.items():
                if files:
                    print(f"\n{category.upper()}:")
                    for filename in sorted(files):
                        file_count += 1
                        file_info = zipf.getinfo(filename)
                        size_kb = file_info.file_size / 1024
                        print(f"  {file_count:3d}. {filename:<45} {size_kb:6.1f} KB")

        # DESCARGAR EL ZIP GENERAL (¬°ESTO ES LO QUE FALTABA!)
        if IN_COLAB:
            print("\n" + "="*70)
            print("DOWNLOADING MAIN ZIP FILE...")
            print("="*70)
            try:
                files.download(zip_filename)
                print(f"‚úÖ ZIP file downloaded: {zip_filename}")
                print(f"   Size: {total_size/(1024*1024):.2f} MB")
                print(f"   Files: {len(file_list)}")
            except Exception as e:
                print(f"‚ö† Could not download automatically: {e}")
                print(f"üìç File available at: {os.path.abspath(zip_filename)}")
        else:
            print(f"\nüìç ZIP file available at: {os.path.abspath(zip_filename)}")
            print(f"   Size: {total_size/(1024*1024):.2f} MB")
            print(f"   Files: {len(file_list)}")

    else:
        print("‚ö† No files found to compress")

    print("\n" + "="*70)
else:
    print("\n‚úì Skipping ZIP creation")

# ============================================================================
# ELIMINAR ARCHIVOS GENERADOS (pero NO el ZIP si el usuario quiere)
# ============================================================================

print("\n" + "="*70)
print("CLEANING UP GENERATED FILES")
print("="*70)

# Lista de archivos generados por el programa
generated_files = [
    data_exploration_file,
    mle_results_file,
    mle_estimates_file,
    prior_distributions_file,
    trace_plots_file,
    posterior_distributions_file,
    percentiles_shaded_file,
    percentiles_all_curves_file,
    fatigue_posterior_file,
    fatigue_summary_file,
    percentiles_file,
]

# A√±adir archivo .py si se cre√≥
if py_script_filename and os.path.exists(py_script_filename):
    generated_files.append(py_script_filename)

# A√±adir directorio de datos sint√©ticos si existe
if synthetic_dir and os.path.exists(synthetic_dir):
    generated_files.append(synthetic_dir)

# ELIMINAR cualquier ZIP de datos sint√©ticos que se haya creado
# (porque ya est√°n incluidos en el ZIP general)
if 'zip_filename' in locals() and os.path.exists(zip_filename):
    # Buscar y eliminar ZIPs de datos sint√©ticos separados
    synth_zip_pattern = f"synthetic_data_{dataset_name}*.zip"
    for synth_zip in glob.glob(synth_zip_pattern):
        try:
            os.remove(synth_zip)
            print(f"  ‚úì Removed duplicate synthetic ZIP: {os.path.basename(synth_zip)}")
        except:
            pass

# Preguntar si mantener el ZIP GENERAL
if zip_files_created:
    print(f"\nMain ZIP file created: {os.path.basename(zip_files_created[0])}")
    keep_main_zip = input("Keep the main ZIP file? (y/n): ").strip().lower()
    if keep_main_zip == 'y':
        print("‚úì Main ZIP file will be kept.")
        # Remover el ZIP de la lista de archivos a eliminar
        for zip_file in zip_files_created:
            if zip_file in generated_files:
                generated_files.remove(zip_file)
    else:
        print("‚úì Main ZIP file will be deleted.")
        generated_files.extend(zip_files_created)

# Filtrar solo archivos que existen
existing_files = [f for f in generated_files if os.path.exists(f)]

if existing_files:
    print(f"\nFound {len(existing_files)} generated files/directories:")

    for f in existing_files:
        if os.path.isdir(f):
            # Contar archivos en el directorio
            file_count = 0
            dir_size = 0
            for root, dirs, files in os.walk(f):
                file_count += len(files)
                for file in files:
                    file_path = os.path.join(root, file)
                    if os.path.exists(file_path):
                        dir_size += os.path.getsize(file_path)
            print(f"  üìÅ {f}/ ({file_count} files, {dir_size/1024:.1f} KB)")
        else:
            size = os.path.getsize(f) if os.path.exists(f) else 0
            print(f"  üìÑ {os.path.basename(f)} ({size/1024:.1f} KB)")

    # Preguntar por confirmaci√≥n
    print("\n" + "="*70)
    respuesta = input("Delete ALL generated files and directories? (y/n): ").strip().lower()

    if respuesta == 'y':
        removed_count = 0
        removed_size = 0

        for filepath in existing_files:
            try:
                if os.path.isdir(filepath):
                    # Calcular tama√±o
                    dir_size = 0
                    for root, dirs, files in os.walk(filepath):
                        for file in files:
                            file_path = os.path.join(root, file)
                            if os.path.exists(file_path):
                                dir_size += os.path.getsize(file_path)

                    # Eliminar
                    shutil.rmtree(filepath)
                    removed_count += 1
                    removed_size += dir_size
                    print(f"  ‚úì Deleted directory: {filepath}/ ({dir_size/1024:.1f} KB)")
                else:
                    file_size = os.path.getsize(filepath)
                    os.remove(filepath)
                    removed_count += 1
                    removed_size += file_size
                    print(f"  ‚úì Deleted: {os.path.basename(filepath)} ({file_size/1024:.1f} KB)")
            except Exception as e:
                print(f"  ‚úó Error deleting {filepath}: {e}")

        print(f"\n‚úÖ Cleanup completed:")
        print(f"   ‚Ä¢ Files/directories deleted: {removed_count}")
        print(f"   ‚Ä¢ Space freed: {removed_size/(1024*1024):.2f} MB")

    else:
        print("\n‚úÖ Operation cancelled. Files kept.")

else:
    print("  ‚úÖ No generated files found.")

print("\n" + "="*70)
print("ANALYSIS COMPLETED SUCCESSFULLY!")
print("="*70)

# -*- coding: utf-8 -*-
"""FatigueExamplePythonWeibullFINAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19SkVwgmv7o5LDu8BCdrT4WaRQBcJ7dD_
"""

# ============================================================================
#   FATIGUE ANALYSIS - WEIBULL MODEL (MLE + BAYESIAN)
#   Castillo-Canteli Dimensionless Formulation
#   WITH GOOGLE COLAB FILE UPLOAD SUPPORT
# ============================================================================
# Physical justification:
# - Lower bounded problem (N > 0) ‚Üí Weibull distribution for minima
# - Gumbel is limiting case (Œ≤ ‚Üí ‚àû) naturally captured by Weibull
# ============================================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pymc as pm
import arviz as az
import pytensor.tensor as pt
from scipy import stats
from scipy.optimize import minimize, differential_evolution
import warnings
import sys
import os
import glob
import shutil
import zipfile
from datetime import datetime
import re

warnings.filterwarnings('ignore')

plt.rcParams['figure.figsize'] = [14, 8]

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

print("="*70)
print("FATIGUE ANALYSIS - WEIBULL MODEL")
print("Maximum Likelihood Estimation + Bayesian Inference")
print("="*70)
print(f"PyMC version: {pm.__version__}")
print(f"ArviZ version: {az.__version__}")

# ============================================================================
# DETECT GOOGLE COLAB ENVIRONMENT
# ============================================================================

try:
    from google.colab import files
    IN_COLAB = True
    print("\n‚úì Google Colab detected")
except:
    IN_COLAB = False
    print("\n‚úì Running in local environment")

# ============================================================================
# DATA SOURCE SELECTION
# ============================================================================

print("\n" + "="*70)
print("DATA SOURCE SELECTION")
print("="*70)

def get_user_choice():
    """Get user choice for data source."""
    print("\nChoose data source:")
    print("  1 - Use Holmen example data")
    print("  2 - Upload your own data file")

    while True:
        choice = input("\nEnter choice (1 or 2): ").strip()
        if choice in ['1', '2']:
            return choice
        print("Invalid choice. Please enter 1 or 2.")

# Determine data source
if IN_COLAB:
    try:
        data_choice = get_user_choice()
    except (EOFError, KeyboardInterrupt):
        print("\nInput not available, defaulting to file upload mode")
        data_choice = '2'
else:
    print("\nNot in Colab - defaulting to Holmen example")
    print("To use your own data, set data_choice = '2' before this section")
    data_choice = '1'

use_example = (data_choice == '1')

print(f"\nSelected mode: {'Example (Holmen)' if use_example else 'Upload file'}")

# ============================================================================
# DATA LOADING
# ============================================================================

print("\n" + "="*70)
print("DATA LOADING")
print("="*70)

if use_example:
    print("\n‚úì Using Holmen example data...")

    # Holmen example data
    stress_data = np.array([
        0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95,
        0.95, 0.95, 0.95, 0.95, 0.95,
        0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90,
        0.90, 0.90, 0.90, 0.90, 0.90,
        0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825,
        0.825, 0.825, 0.825, 0.825, 0.825,
        0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75,
        0.75, 0.75, 0.75, 0.75, 0.75,
        0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675,
        0.675, 0.675, 0.675, 0.675, 0.675,
    ])

    cycles_data = np.array([
        0.037, 0.072, 0.074, 0.076, 0.083, 0.085, 0.105, 0.109, 0.120, 0.123, 0.143,
        0.203, 0.206, 0.217, 0.257, 0.201, 0.216, 0.226, 0.252, 0.257, 0.295, 0.311,
        0.342, 0.356, 0.451, 0.457, 0.509, 0.540, 0.680, 1.129, 1.246, 1.258, 1.460,
        1.492, 2.400, 2.410, 2.590, 2.903, 3.330, 3.590, 3.847, 4.110, 4.820, 5.560,
        5.598, 6.710, 9.930, 12.600, 15.580, 16.190, 17.280, 18.620, 20.300, 24.900,
        26.260, 27.940, 36.350, 48.420, 50.090, 67.340, 102.950, 280.320, 339.830,
        366.900, 485.620, 658.960, 896.330, 1241.760, 1250.200, 1329.780, 1399.830,
        1459.140, 3294.820, 12709, 14373
    ])

    # Default configuration for Holmen
    config = {
        'n_tune': 1000,
        'n_draws': 1000,
        'n_chains': 1,
        'n_stress_points': 50,
        'n_param_samples': 1000,
        'percentiles_base': [0.01, 0.15, 0.50, 0.85, 0.99],
        'percentiles_sub': [0.01, 0.15, 0.50, 0.85, 0.99],
        'cv_N0': 0.25,
        'cv_Delta0': 0.15,
        'cv_beta': 0.30,
        'cv_lambda': 0.25,
        'cv_delta': 0.30,
        'target_accept': 0.95
    }

    dataset_name = "Holmen"

else:
    print("\nüìÅ Please upload your data file...")
    print("\nFile format requirements:")
    print("  Option 1: CSV or Excel file (.csv, .xlsx, .xls)")
    print("    - Required columns: 'N' (cycles), 'Deltasigma' (stress)")
    print("    - Optional: configuration parameters")
    print("\n  Option 2: R/OpenBUGS format (.txt, .dat)")
    print("    - list(M=..., N=c(...), Deltasigma=c(...), ...)")

    if IN_COLAB:
        print("\nUploading file...")
        uploaded = files.upload()

        if len(uploaded) == 0:
            print("No file uploaded. Using Holmen example instead.")
            use_example = True
        else:
            filename = list(uploaded.keys())[0]
            print(f"\n‚úì File uploaded: {filename}")
            print(f"  File size: {len(uploaded[filename])} bytes")

            # In Colab, the file is already in the current directory
            # Verify it exists
            if not os.path.exists(filename):
                print(f"  ‚úó Warning: File not found at {filename}")
                print(f"  Current directory: {os.getcwd()}")
                print(f"  Files in directory: {os.listdir('.')[:10]}")
            else:
                print(f"  ‚úì File verified at: {os.path.abspath(filename)}")
    else:
        filename = input("\nEnter data file path: ").strip()
        if not os.path.exists(filename):
            print(f"File not found: {filename}")
            print("Using Holmen example instead.")
            use_example = True

    if not use_example:
        # Read data file
        try:
            # Check file extension
            if filename.endswith(('.txt', '.dat')):
                # R/OpenBUGS format parser
                print("\n  Detected R/OpenBUGS format...")
                print(f"  Reading file: {filename}")

                with open(filename, 'r') as f:
                    content = f.read()

                print(f"  File size: {len(content)} characters")

                # Extract N values
                n_match = re.search(r'N\s*=\s*c\(([^)]+)\)', content)
                if not n_match:
                    raise ValueError("Could not find 'N=c(...)' in file")
                n_values_str = n_match.group(1)
                n_values = [float(x.strip()) for x in n_values_str.split(',') if x.strip()]

                # Extract Deltasigma values
                delta_match = re.search(r'Deltasigma\s*=\s*c\(([^)]+)\)', content)
                if not delta_match:
                    raise ValueError("Could not find 'Deltasigma=c(...)' in file")
                delta_values_str = delta_match.group(1)
                delta_values = [float(x.strip()) for x in delta_values_str.split(',') if x.strip()]

                if len(n_values) != len(delta_values):
                    raise ValueError(f"Length mismatch: N has {len(n_values)} values, Deltasigma has {len(delta_values)}")

                cycles_data = np.array(n_values)
                stress_data = np.array(delta_values)

                print(f"  ‚úì Parsed N: {len(cycles_data)} values")
                print(f"  ‚úì Parsed Deltasigma: {len(stress_data)} values")
                print(f"  ‚úì Created numpy arrays successfully")
                print(f"    cycles_data shape: {cycles_data.shape}")
                print(f"    stress_data shape: {stress_data.shape}")

                # Default configuration
                config = {
                    'n_tune': 1000,
                    'n_draws': 2000,
                    'n_chains': 2,
                    'n_stress_points': 50,
                    'n_param_samples': 1000,
                    'percentiles_base': [0.01, 0.15, 0.50, 0.85, 0.99],
                    'percentiles_sub': [0.01, 0.15, 0.50, 0.85, 0.99],
                    'cv_N0': 0.25,
                    'cv_Delta0': 0.15,
                    'cv_beta': 0.30,
                    'cv_lambda': 0.25,
                    'cv_delta': 0.30,
                    'target_accept': 0.95
                }

                # Try to extract optional parameters from R list
                # M (number of observations)
                m_match = re.search(r'M\s*=\s*(\d+)', content)
                if m_match:
                    M_val = int(m_match.group(1))
                    print(f"  ‚úì Found M={M_val} (verification: {len(cycles_data)} observations)")

                # ns (number of stress points)
                ns_match = re.search(r'ns\s*=\s*(\d+)', content)
                if ns_match:
                    config['n_stress_points'] = int(ns_match.group(1))
                    print(f"  ‚úì Found ns={config['n_stress_points']}")

                # np (number of percentiles)
                np_match = re.search(r'np\s*=\s*(\d+)', content)
                if np_match:
                    np_val = int(np_match.group(1))
                    print(f"  ‚úì Found np={np_val}")

                # percentiles
                perc_match = re.search(r'percentiles\s*=\s*c\(([\d.,\s]+)\)', content)
                if perc_match:
                    perc_values = [float(x.strip()) for x in perc_match.group(1).split(',')]
                    config['percentiles_base'] = perc_values
                    config['percentiles_sub'] = perc_values
                    print(f"  ‚úì Found percentiles: {perc_values}")

                # n_tune (warmup)
                tune_match = re.search(r'n_tune\s*=\s*(\d+)', content)
                if tune_match:
                    config['n_tune'] = int(tune_match.group(1))
                    print(f"  ‚úì Found n_tune={config['n_tune']}")

                # n_draws (samples)
                draws_match = re.search(r'n_draws\s*=\s*(\d+)', content)
                if draws_match:
                    config['n_draws'] = int(draws_match.group(1))
                    print(f"  ‚úì Found n_draws={config['n_draws']}")

                # n_chains
                chains_match = re.search(r'n_chains\s*=\s*(\d+)', content)
                if chains_match:
                    config['n_chains'] = int(chains_match.group(1))
                    print(f"  ‚úì Found n_chains={config['n_chains']}")

                # Coefficient of variation parameters
                cv_params = ['cv_N0', 'cv_Delta0', 'cv_beta', 'cv_lambda', 'cv_delta']
                for cv_param in cv_params:
                    cv_match = re.search(rf'{cv_param}\s*=\s*([\d.]+)', content)
                    if cv_match:
                        config[cv_param] = float(cv_match.group(1))
                        print(f"  ‚úì Found {cv_param}={config[cv_param]}")

                # target_accept
                accept_match = re.search(r'target_accept\s*=\s*([\d.]+)', content)
                if accept_match:
                    config['target_accept'] = float(accept_match.group(1))
                    print(f"  ‚úì Found target_accept={config['target_accept']}")

                dataset_name = filename.split('.')[0]

                print(f"\n  ‚úì‚úì‚úì R FORMAT FILE LOADED SUCCESSFULLY")
                print(f"      Dataset: {dataset_name}")
                print(f"      use_example = {use_example}")
                print(f"      Variables 'cycles_data' and 'stress_data' created")

            elif filename.endswith('.csv'):
                df = pd.read_csv(filename)
            elif filename.endswith(('.xlsx', '.xls')):
                df = pd.read_excel(filename)
            else:
                raise ValueError("Unsupported file format. Use .csv, .xlsx, .xls, .txt, or .dat")

            # CSV/Excel format (if not R format)
            if filename.endswith(('.csv', '.xlsx', '.xls')):
                # Extract required columns
                if 'N' not in df.columns or 'Deltasigma' not in df.columns:
                    raise ValueError("File must contain 'N' and 'Deltasigma' columns")

                cycles_data = df['N'].values
                stress_data = df['Deltasigma'].values

                # Remove NaN values
                valid_mask = ~(np.isnan(cycles_data) | np.isnan(stress_data))
                cycles_data = cycles_data[valid_mask]
                stress_data = stress_data[valid_mask]

                print(f"\n‚úì Data loaded: {len(cycles_data)} observations")

                # Try to read configuration parameters
                config = {
                    'n_tune': 1000,
                    'n_draws': 2000,
                    'n_chains': 2,
                    'n_stress_points': 50,
                    'n_param_samples': 1000,
                    'percentiles_base': [0.01, 0.15, 0.50, 0.85, 0.99],
                    'percentiles_sub': [0.01, 0.15, 0.50, 0.85, 0.99],
                    'cv_N0': 0.25,
                    'cv_Delta0': 0.15,
                    'cv_beta': 0.30,
                    'cv_lambda': 0.25,
                    'cv_delta': 0.30,
                    'target_accept': 0.95
                }

                # Check for configuration in the file
                config_params = {
                    'n_tune': ['n_tune', 'tune', 'warmup'],
                    'n_draws': ['n_draws', 'draws', 'samples'],
                    'n_chains': ['n_chains', 'chains'],
                    'n_stress_points': ['n_stress_points', 'stress_points'],
                    'n_param_samples': ['n_param_samples', 'param_samples'],
                    'cv_N0': ['cv_N0', 'cv_n0'],
                    'cv_Delta0': ['cv_Delta0', 'cv_delta0'],
                    'cv_beta': ['cv_beta'],
                    'cv_lambda': ['cv_lambda'],
                    'cv_delta': ['cv_delta'],
                    'target_accept': ['target_accept', 'accept_rate']
                }

                for param, possible_names in config_params.items():
                    for name in possible_names:
                        if name in df.columns:
                            value = df[name].dropna().iloc[0]
                            config[param] = float(value) if param.startswith('cv_') or param == 'target_accept' else int(value)
                            print(f"  Found config: {param} = {config[param]}")
                            break

                # Try to read percentiles
                if 'percentiles_base' in df.columns:
                    perc_base = df['percentiles_base'].dropna().values
                    config['percentiles_base'] = perc_base.tolist()
                    print(f"  Found percentiles_base: {config['percentiles_base']}")

                if 'percentiles_sub' in df.columns:
                    perc_sub = df['percentiles_sub'].dropna().values
                    config['percentiles_sub'] = perc_sub.tolist()
                    print(f"  Found percentiles_sub: {config['percentiles_sub']}")

                dataset_name = filename.split('.')[0]

        except Exception as e:
            print(f"\n‚úó Error reading file: {e}")
            import traceback
            print("\nFull error traceback:")
            traceback.print_exc()
            print("\nUsing Holmen example instead.")
            use_example = True

# Final check - if reverted to example OR if data wasn't loaded, use Holmen
if use_example or 'stress_data' not in locals() or 'cycles_data' not in locals():
    if not use_example:
        print("\n‚ö† Data not loaded properly, using Holmen example instead.")
    # Load Holmen data (same as above)
    stress_data = np.array([
        0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95,
        0.95, 0.95, 0.95, 0.95, 0.95,
        0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90, 0.90,
        0.90, 0.90, 0.90, 0.90, 0.90,
        0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825, 0.825,
        0.825, 0.825, 0.825, 0.825, 0.825,
        0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75,
        0.75, 0.75, 0.75, 0.75, 0.75,
        0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675, 0.675,
        0.675, 0.675, 0.675, 0.675, 0.675,
    ])
    cycles_data = np.array([
        0.037, 0.072, 0.074, 0.076, 0.083, 0.085, 0.105, 0.109, 0.120, 0.123, 0.143,
        0.203, 0.206, 0.217, 0.257, 0.201, 0.216, 0.226, 0.252, 0.257, 0.295, 0.311,
        0.342, 0.356, 0.451, 0.457, 0.509, 0.540, 0.680, 1.129, 1.246, 1.258, 1.460,
        1.492, 2.400, 2.410, 2.590, 2.903, 3.330, 3.590, 3.847, 4.110, 4.820, 5.560,
        5.598, 6.710, 9.930, 12.600, 15.580, 16.190, 17.280, 18.620, 20.300, 24.900,
        26.260, 27.940, 36.350, 48.420, 50.090, 67.340, 102.950, 280.320, 339.830,
        366.900, 485.620, 658.960, 896.330, 1241.760, 1250.200, 1329.780, 1399.830,
        1459.140, 3294.820, 12709, 14373
    ])
    config = {
        'n_tune': 1000,
        'n_draws': 2000,
        'n_chains': 2,
        'n_stress_points': 50,
        'n_param_samples': 1000,
        'percentiles_base': [0.01, 0.15, 0.50, 0.85, 0.99],
        'percentiles_sub': [0.01, 0.15, 0.50, 0.85, 0.99],
        'cv_N0': 0.25,
        'cv_Delta0': 0.15,
        'cv_beta': 0.30,
        'cv_lambda': 0.25,
        'cv_delta': 0.30,
        'target_accept': 0.95
    }
    dataset_name = "Holmen"

N_min = cycles_data.min()
N_max = cycles_data.max()

print(f"\n{'='*70}")
print(f"DATASET: {dataset_name}")
print(f"{'='*70}")
print(f"Data source: {'Holmen example (built-in)' if use_example else 'User-uploaded file'}")
print(f"Observations: {len(cycles_data)}")
print(f"Stress range: [{stress_data.min():.3f}, {stress_data.max():.3f}] (dimensionless)")
print(f"Cycles range: [{N_min:.4f}, {N_max:.2f}]")
print(f"\nFirst 5 N values: {cycles_data[:5]}")
print(f"First 5 Deltasigma values: {stress_data[:5]}")

print(f"\n{'='*70}")
print("ANALYSIS CONFIGURATION")
print(f"{'='*70}")
print(f"MCMC Settings:")
print(f"  Tune (warmup): {config['n_tune']}")
print(f"  Draws (samples): {config['n_draws']}")
print(f"  Chains: {config['n_chains']}")
print(f"  Target acceptance: {config['target_accept']}")
print(f"\nPercentiles:")
print(f"  Stress points: {config['n_stress_points']}")
print(f"  Parameter samples: {config['n_param_samples']}")
print(f"  Base percentiles: {[int(p*100) for p in config['percentiles_base']]}%")
print(f"  Sub-percentiles: {[int(p*100) for p in config['percentiles_sub']]}%")
print(f"\nPrior Coefficients of Variation (CV):")
print(f"  CV(N‚ÇÄ) = {config['cv_N0']:.2f}")
print(f"  CV(ŒîœÉ‚ÇÄ) = {config['cv_Delta0']:.2f}")
print(f"  CV(Œ≤) = {config['cv_beta']:.2f}")
print(f"  CV(Œª) = {config['cv_lambda']:.2f}")
print(f"  CV(Œ¥) = {config['cv_delta']:.2f}")

# ============================================================================
# DATA VISUALIZATION
# ============================================================================

print("\n" + "="*70)
print("DATA VISUALIZATION")
print("="*70)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

ax = axes[0]
# Use different colors for each unique stress level
unique_stresses = np.unique(stress_data)
colors_plot = plt.cm.rainbow(np.linspace(0, 1, len(unique_stresses)))

for i, stress_level in enumerate(unique_stresses):
    mask = stress_data == stress_level
    ax.scatter(cycles_data[mask], [stress_level]*np.sum(mask),
              alpha=0.7, s=50, label=f'{stress_level:.3f}',
              color=colors_plot[i], edgecolors='black', linewidths=0.5)

ax.set_xlabel('Cycles to Failure (N)', fontsize=12, fontweight='bold')
ax.set_ylabel('Stress (Deltasigma, dimensionless)', fontsize=12, fontweight='bold')
ax.set_xscale('log')
ax.set_title(f'S-N Data ({dataset_name}) - Weibull Model', fontsize=13, fontweight='bold')
if len(unique_stresses) <= 10:
    ax.legend(fontsize=9, loc='upper right')
ax.grid(True, alpha=0.3, which='both')

ax = axes[1]
ax.scatter(np.log(cycles_data), np.log(stress_data), alpha=0.6, s=40, color='darkblue')
ax.set_xlabel('ln(Cycles)', fontsize=12, fontweight='bold')
ax.set_ylabel('ln(Stress)', fontsize=12, fontweight='bold')
ax.set_title('Log-Log Space', fontsize=13, fontweight='bold')
ax.grid(True, alpha=0.3)

plt.tight_layout()
data_exploration_file = 'data_exploration_weibull.png'
plt.savefig(data_exploration_file, dpi=150, bbox_inches='tight')
plt.show()

print("‚úì Data visualization completed!")

# ============================================================================
# PHASE 1: MAXIMUM LIKELIHOOD ESTIMATION (WEIBULL)
# ============================================================================

print("\n" + "="*70)
print("PHASE 1: MAXIMUM LIKELIHOOD ESTIMATION")
print("="*70)

def weibull_log_likelihood(params, stress, cycles):
    """
    Weibull log-likelihood for fatigue data.
    Castillo-Canteli dimensionless formulation.
    """
    N0, Delta0, beta, lambda_param, delta = params

    # Validations
    if N0 <= 0 or Delta0 <= 0 or beta <= 0 or delta <= 0:
        return -np.inf

    # Dimensionless transformation
    log_N_dimensionless = np.log(cycles) - np.log(N0)
    r = np.log(stress) - np.log(Delta0)

    # Check for valid r values
    if np.any(np.abs(r) < 1e-10):
        return -np.inf

    # Weibull parameters for minima
    mu_Y = (-lambda_param - delta) / r
    sigma_Y = delta / (beta * np.abs(r))

    # Check valid sigma
    if np.any(sigma_Y <= 0):
        return -np.inf

    # Standardized variable for Weibull (Gumbel for minima parametrization)
    z = (log_N_dimensionless - mu_Y) / sigma_Y

    # Weibull log-likelihood for minima
    log_lik = -np.log(sigma_Y) + z - np.exp(z)

    # Check for invalid values
    if not np.all(np.isfinite(log_lik)):
        return -np.inf

    return np.sum(log_lik)

def negative_log_likelihood(params, stress, cycles):
    """Negative log-likelihood for minimization."""
    return -weibull_log_likelihood(params, stress, cycles)

# Initial guess based on physical reasoning
N0_init = N_min * 0.5
Delta0_init = stress_data.min() * 0.7
beta_init = 3.0
lambda_init = -8.0
delta_init = 2.0

initial_params = np.array([N0_init, Delta0_init, beta_init, lambda_init, delta_init])

print("\nInitial guess:")
print(f"  N‚ÇÄ = {N0_init:.4f}")
print(f"  ŒîœÉ‚ÇÄ = {Delta0_init:.4f}")
print(f"  Œ≤ = {beta_init:.4f}")
print(f"  Œª = {lambda_init:.4f}")
print(f"  Œ¥ = {delta_init:.4f}")

# Parameter bounds for optimization
bounds = [
    (0.001, N_min * 0.9),
    (stress_data.min() * 0.4, stress_data.min() * 0.99),
    (0.5, 15.0),
    (-12.0, -4.0),
    (0.5, 5.0)
]

print("\nRunning global optimization (differential evolution)...")
print("This may take a few minutes...")

result_global = differential_evolution(
    negative_log_likelihood,
    bounds=bounds,
    args=(stress_data, cycles_data),
    seed=RANDOM_SEED,
    maxiter=1000,
    popsize=30,
    tol=1e-7,
    atol=1e-7,
    workers=1,
    updating='deferred',
    polish=True
)

print("\n‚úì Global optimization completed!")
print(f"  Success: {result_global.success}")
print(f"  Log-likelihood: {-result_global.fun:.2f}")
print(f"  Iterations: {result_global.nit}")

mle_params = result_global.x

print("\n" + "="*70)
print("MLE ESTIMATES (WEIBULL MODEL)")
print("="*70)
print(f"  N‚ÇÄ (reference cycles) = {mle_params[0]:.6f}")
print(f"  ŒîœÉ‚ÇÄ (reference stress) = {mle_params[1]:.6f}")
print(f"  Œ≤ (shape parameter)    = {mle_params[2]:.6f}")
print(f"  Œª (location param)     = {mle_params[3]:.6f}")
print(f"  Œ¥ (scale param)        = {mle_params[4]:.6f}")
print(f"\n  Log-likelihood = {-result_global.fun:.2f}")

# Save MLE results
mle_estimates_file = 'mle_estimates.txt'
with open(mle_estimates_file, 'w') as f:
    f.write(f"MLE ESTIMATES - WEIBULL MODEL - {dataset_name}\n")
    f.write("="*50 + "\n")
    f.write(f"N0 = {mle_params[0]:.8f}\n")
    f.write(f"Delta0 = {mle_params[1]:.8f}\n")
    f.write(f"beta = {mle_params[2]:.8f}\n")
    f.write(f"lambda = {mle_params[3]:.8f}\n")
    f.write(f"delta = {mle_params[4]:.8f}\n")
    f.write(f"Log-likelihood = {-result_global.fun:.8f}\n")

print("\n‚úì MLE estimates saved to 'mle_estimates.txt'")

# Visualize MLE fit
print("\nVisualizing MLE fit...")

fig, ax = plt.subplots(figsize=(14, 8))

# Plot observed data
ax.scatter(cycles_data, stress_data, c='black', s=80,
          alpha=0.9, label='Observed data', zorder=100, marker='o',
          edgecolors='white', linewidths=1.5)

# Plot MLE curves for different percentiles
stress_range_plot = np.linspace(stress_data.min() * 0.97, stress_data.max() * 1.03, 100)
percentiles_mle = [0.01, 0.10, 0.50, 0.90, 0.99]
colors_mle = ['#8B0000', '#FF8C00', '#228B22', '#4169E1', '#8B008B']

N0_mle, Delta0_mle, beta_mle, lambda_mle, delta_mle = mle_params

for perc, color in zip(percentiles_mle, colors_mle):
    N_perc = []
    for stress in stress_range_plot:
        if stress > 0 and Delta0_mle > 0:
            r = np.log(stress / Delta0_mle)
            if abs(r) > 1e-10:
                mu_Y = (-lambda_mle - delta_mle) / r
                sigma_Y = delta_mle / (beta_mle * abs(r))

                # Weibull quantile for minima (Gumbel parametrization)
                z_p = np.log(-np.log(1 - perc))
                Y_p = mu_Y + sigma_Y * z_p
                N_p = N0_mle * np.exp(Y_p)

                if N_p > 0 and np.isfinite(N_p):
                    N_perc.append(N_p)
                else:
                    N_perc.append(np.nan)
            else:
                N_perc.append(np.nan)
        else:
            N_perc.append(np.nan)

    valid_mask = ~np.isnan(N_perc)
    if np.sum(valid_mask) > 0:
        ax.plot(np.array(N_perc)[valid_mask], stress_range_plot[valid_mask],
               color=color, linewidth=2.5, label=f'P{int(perc*100)} MLE',
               alpha=0.8)

ax.set_xlabel('Cycles to Failure (N)', fontsize=13, fontweight='bold')
ax.set_ylabel('Stress (Deltasigma, dimensionless)', fontsize=13, fontweight='bold')
ax.set_xscale('log')
ax.set_title(f'MLE Fit - Weibull Model ({dataset_name})',
            fontsize=14, fontweight='bold')
ax.legend(fontsize=10, loc='upper right')
ax.grid(True, alpha=0.3, which='both')
ax.set_xlim([N_min * 0.1, N_max * 10])

plt.tight_layout()
mle_results_file = 'mle_results.png'
plt.savefig(mle_results_file, dpi=150, bbox_inches='tight')
plt.show()

print("‚úì MLE visualization completed!")

# ============================================================================
# PHASE 2: DEFINE INFORMATIVE PRIORS FROM MLE
# ============================================================================

print("\n" + "="*70)
print("PHASE 2: DEFINING BAYESIAN PRIORS FROM MLE")
print("="*70)

# Use MLE estimates to define informative Normal priors
N0_mle_val = mle_params[0]
Delta0_mle_val = mle_params[1]
beta_mle_val = mle_params[2]
lambda_mle_val = mle_params[3]
delta_mle_val = mle_params[4]

# Prior standard deviations using CV from config
N0_std = N0_mle_val * config['cv_N0']
Delta0_std = Delta0_mle_val * config['cv_Delta0']
beta_std = beta_mle_val * config['cv_beta']
lambda_std = abs(lambda_mle_val) * config['cv_lambda']
delta_std = delta_mle_val * config['cv_delta']

print("\nInformative Normal Priors (centered on MLE):")
print(f"  N‚ÇÄ     ~ Normal({N0_mle_val:.4f}, {N0_std:.4f})  [CV={config['cv_N0']:.2f}]")
print(f"  ŒîœÉ‚ÇÄ    ~ Normal({Delta0_mle_val:.6f}, {Delta0_std:.6f})  [CV={config['cv_Delta0']:.2f}]")
print(f"  Œ≤      ~ Normal({beta_mle_val:.4f}, {beta_std:.4f})  [CV={config['cv_beta']:.2f}]")
print(f"  Œª      ~ Normal({lambda_mle_val:.4f}, {lambda_std:.4f})  [CV={config['cv_lambda']:.2f}]")
print(f"  Œ¥      ~ Normal({delta_mle_val:.4f}, {delta_std:.4f})  [CV={config['cv_delta']:.2f}]")

print("\nThese priors will help achieve faster convergence in Bayesian inference.")

# ============================================================================
# PHASE 3: BAYESIAN INFERENCE WITH WEIBULL MODEL
# ============================================================================

print("\n" + "="*70)
print("PHASE 3: BAYESIAN INFERENCE")
print("="*70)

initial_values = {
    'N0': N0_mle_val,
    'Delta0': Delta0_mle_val,
    'beta': beta_mle_val,
    'lambda_param': lambda_mle_val,
    'delta': delta_mle_val
}

with pm.Model() as fatigue_model:
    # PRIORS - Informative Normal distributions based on MLE
    N0 = pm.TruncatedNormal('N0', mu=N0_mle_val, sigma=N0_std,
                            lower=0.001, upper=N_min)
    Delta0 = pm.TruncatedNormal('Delta0', mu=Delta0_mle_val, sigma=Delta0_std,
                                lower=stress_data.min() * 0.3, upper=stress_data.min())
    beta = pm.TruncatedNormal('beta', mu=beta_mle_val, sigma=beta_std,
                             lower=0.5, upper=20.0)
    lambda_param = pm.Normal('lambda_param', mu=lambda_mle_val, sigma=lambda_std)
    delta = pm.TruncatedNormal('delta', mu=delta_mle_val, sigma=delta_std,
                              lower=0.1, upper=10.0)

    # Transform to dimensionless log-space
    log_N_dimensionless = pt.log(cycles_data) - pt.log(N0)
    r = pt.log(stress_data) - pt.log(Delta0)

    # Weibull parameters for minima
    mu_Y = (-lambda_param - delta) / r
    sigma_Y = delta / (beta * pt.abs(r) + 1e-8)

    # Standardized variable
    z = (log_N_dimensionless - mu_Y) / (sigma_Y + 1e-8)

    # Log-likelihood (Weibull for minima, Gumbel parametrization)
    log_lik = -pt.log(sigma_Y + 1e-8) + z - pt.exp(z)

    # Total likelihood
    likelihood = pm.Potential('likelihood', pt.sum(log_lik))

print("‚úì Bayesian model defined with informative priors")

# ============================================================================
# PHASE 4: PRIOR PREDICTIVE CHECK
# ============================================================================

print("\n" + "="*70)
print("PRIOR PREDICTIVE CHECK")
print("="*70)

print("\nSampling from prior predictive distribution...")

with fatigue_model:
    prior_predictive = pm.sample_prior_predictive(
        samples=500,
        random_seed=RANDOM_SEED
    )

print("‚úì Prior predictive samples generated")

# Visualize prior distributions
print("\nVisualizing prior distributions...")

fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.flatten()

var_names = ['N0', 'Delta0', 'beta', 'lambda_param', 'delta']
var_labels = ['N‚ÇÄ (reference cycles)', 'ŒîœÉ‚ÇÄ (reference stress)', 'Œ≤', 'Œª', 'Œ¥']

for ax, var, label in zip(axes[:5], var_names, var_labels):
    samples = prior_predictive.prior[var].values.flatten()

    ax.hist(samples, bins=50, alpha=0.7, color='steelblue', edgecolor='black')
    ax.set_xlabel(label, fontsize=11, fontweight='bold')
    ax.set_ylabel('Frequency', fontsize=11)
    ax.set_title(f'Prior: {label}', fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3)

    ax.axvline(np.median(samples), color='red', linestyle='--', linewidth=2, label='Median')

    # Add MLE value line
    if var == 'N0':
        ax.axvline(N0_mle_val, color='green', linestyle=':', linewidth=2, label='MLE')
    elif var == 'Delta0':
        ax.axvline(Delta0_mle_val, color='green', linestyle=':', linewidth=2, label='MLE')
    elif var == 'beta':
        ax.axvline(beta_mle_val, color='green', linestyle=':', linewidth=2, label='MLE')
    elif var == 'lambda_param':
        ax.axvline(lambda_mle_val, color='green', linestyle=':', linewidth=2, label='MLE')
    elif var == 'delta':
        ax.axvline(delta_mle_val, color='green', linestyle=':', linewidth=2, label='MLE')

    ax.legend()

axes[5].axis('off')

plt.suptitle(f'Prior Distributions (Centered on MLE) - {dataset_name}', fontsize=14, fontweight='bold')
plt.tight_layout()
prior_distributions_file = 'prior_distributions.png'
plt.savefig(prior_distributions_file, dpi=150, bbox_inches='tight')
plt.show()

print("‚úì Prior distributions visualized")

# ============================================================================
# PHASE 5: SAMPLE FROM POSTERIOR
# ============================================================================

print("\n" + "="*70)
print("SAMPLING FROM POSTERIOR DISTRIBUTION")
print("="*70)

print("\nSampling strategy:")
print(f"  ‚Ä¢ Using informative priors from MLE")
print(f"  ‚Ä¢ Tune: {config['n_tune']}, Draws: {config['n_draws']}, Chains: {config['n_chains']}")
print(f"  ‚Ä¢ Target acceptance = {config['target_accept']}")
print("\nThis may take 5-15 minutes depending on configuration...\n")

with fatigue_model:
    trace = pm.sample(
        draws=config['n_draws'],
        tune=config['n_tune'],
        chains=config['n_chains'],
        cores=1,
        random_seed=RANDOM_SEED,
        return_inferencedata=True,
        target_accept=config['target_accept'],
        init='adapt_diag',
        initvals=initial_values
    )

print("\n‚úì Sampling completed!")

# ============================================================================
# PHASE 6: CONVERGENCE DIAGNOSTICS
# ============================================================================

print("\n" + "="*70)
print("CONVERGENCE DIAGNOSTICS")
print("="*70)

summary_table = az.summary(
    trace,
    var_names=['N0', 'Delta0', 'beta', 'lambda_param', 'delta'],
    hdi_prob=0.95
)
print("\n--- Posterior Summary ---")
print(summary_table)

rhat_values = az.rhat(trace)
print(f"\n--- R-hat Diagnostic (should be < 1.01) ---")
all_rhat_good = True
for var in ['N0', 'Delta0', 'beta', 'lambda_param', 'delta']:
    rhat_val = rhat_values[var].values
    status = "‚úì" if rhat_val < 1.01 else "‚úó"
    if rhat_val >= 1.01:
        all_rhat_good = False
    print(f"{var:15s}: {rhat_val:.4f} {status}")

ess_values = az.ess(trace)
print(f"\n--- Effective Sample Size (should be > 1000) ---")
all_ess_good = True
for var in ['N0', 'Delta0', 'beta', 'lambda_param', 'delta']:
    ess_val = ess_values[var].values
    status = "‚úì" if ess_val > 1000 else "‚úó"
    if ess_val <= 1000:
        all_ess_good = False
    print(f"{var:15s}: {ess_val:.0f} {status}")

n_divergences = trace.sample_stats.diverging.sum().values
total_samples = config['n_draws'] * config['n_chains']
print(f"\n--- Divergence Check ---")
print(f"Number of divergent transitions: {n_divergences}")
if n_divergences > 0:
    print(f"‚ö† Warning: {n_divergences} divergences detected")
    print(f"  Divergence rate: {n_divergences / total_samples:.2%}")
else:
    print("‚úì No divergences detected!")

print(f"\n--- Overall Convergence Assessment ---")
if all_rhat_good and all_ess_good and n_divergences == 0:
    print("‚úì‚úì‚úì EXCELLENT: Model has converged successfully!")
elif all_rhat_good and n_divergences < 50:
    print("‚úì‚úì GOOD: Model convergence is acceptable")
else:
    print("‚úó WARNING: Model may not have converged properly")

# Trace plots
print("\nGenerating trace plots...")
fig, axes = plt.subplots(5, 2, figsize=(14, 16))
az.plot_trace(
    trace,
    var_names=['N0', 'Delta0', 'beta', 'lambda_param', 'delta'],
    compact=False,
    axes=axes
)
plt.suptitle(f'Trace Plots and Posterior Distributions - {dataset_name}',
            fontsize=14, fontweight='bold', y=1.001)
plt.tight_layout()
trace_plots_file = 'trace_plots.png'
plt.savefig(trace_plots_file, dpi=150, bbox_inches='tight')
plt.show()

# Posterior distributions
fig = plt.figure(figsize=(15, 10))
az.plot_posterior(
    trace,
    var_names=['N0', 'Delta0', 'beta', 'lambda_param', 'delta'],
    hdi_prob=0.95,
    figsize=(15, 10)
)
plt.suptitle(f'Posterior Distributions with 95% HDI - {dataset_name}',
            fontsize=14, fontweight='bold')
plt.tight_layout()
posterior_distributions_file = 'posterior_distributions.png'
plt.savefig(posterior_distributions_file, dpi=150, bbox_inches='tight')
plt.show()

print("\n‚úì Diagnostics completed!")

# ============================================================================
# PHASE 7: PERCENTILES OF PERCENTILES
# ============================================================================

print("\n" + "="*70)
print("COMPUTING PERCENTILES OF PERCENTILES")
print("="*70)

posterior = trace.posterior

# Define stress range for plotting
stress_margin = 0.03
stress_min_plot = stress_data.min() - stress_margin
stress_max_plot = stress_data.max() + stress_margin
stress_min_plot = max(stress_min_plot, 0.01)

stress_range = np.linspace(stress_min_plot, stress_max_plot, config['n_stress_points'])

percentiles_base = config['percentiles_base']
percentiles_sub = config['percentiles_sub']

print(f"\nConfiguration:")
print(f"  Stress points: {config['n_stress_points']}")
print(f"  Parameter samples: {config['n_param_samples']}")
print(f"  Base percentiles: {[int(p*100) for p in percentiles_base]}%")
print(f"  Sub-percentiles: {[int(p*100) for p in percentiles_sub]}%")

def compute_percentile(stress, N0, Delta0, beta, lambda_p, delta, prob):
    """Compute N_p for given stress and failure probability (Weibull)."""
    if stress <= 0 or N0 <= 0 or Delta0 <= 0 or beta <= 0 or delta <= 0:
        return np.nan
    if prob <= 0 or prob >= 1:
        return np.nan

    try:
        r = np.log(stress / Delta0)
        if abs(r) < 1e-10:
            return np.nan

        mu_Y = (-lambda_p - delta) / r
        sigma_Y = delta / (beta * abs(r))

        # Weibull quantile for minima (Gumbel parametrization)
        z_p = np.log(-np.log(1 - prob))
        Y_p = mu_Y + sigma_Y * z_p
        N_p = N0 * np.exp(Y_p)

        if not np.isfinite(N_p) or N_p <= 0:
            return np.nan

        if N_p < 1e-6 or N_p > 1e10:
            return np.nan

        return N_p
    except:
        return np.nan

# Extract posterior samples
N0_samples = posterior['N0'].values.flatten()
Delta0_samples = posterior['Delta0'].values.flatten()
beta_samples = posterior['beta'].values.flatten()
lambda_samples = posterior['lambda_param'].values.flatten()
delta_samples = posterior['delta'].values.flatten()

total_samples = len(N0_samples)
sample_indices = np.random.choice(total_samples, size=min(config['n_param_samples'], total_samples), replace=False)

# Storage for percentiles of percentiles
percentiles_of_percentiles = {}

print("\nComputing percentiles of percentiles...")

for perc_base_idx, perc_base in enumerate(percentiles_base):
    print(f"\n  Processing base percentile P{int(perc_base*100)}...")

    percentile_matrix = np.zeros((config['n_stress_points'], len(sample_indices)))

    for i, stress in enumerate(stress_range):
        if i % max(1, config['n_stress_points']//5) == 0:
            print(f"    Stress point {i+1}/{config['n_stress_points']}...")

        for j, idx in enumerate(sample_indices):
            N0_s = N0_samples[idx]
            Delta0_s = Delta0_samples[idx]
            beta_s = beta_samples[idx]
            lambda_s = lambda_samples[idx]
            delta_s = delta_samples[idx]

            N_p = compute_percentile(stress, N0_s, Delta0_s, beta_s, lambda_s, delta_s, perc_base)

            if not np.isnan(N_p):
                percentile_matrix[i, j] = N_p
            else:
                percentile_matrix[i, j] = np.nan

    n_valid = np.sum(~np.isnan(percentile_matrix), axis=1)
    print(f"    Valid values per stress: min={n_valid.min()}, max={n_valid.max()}, mean={n_valid.mean():.1f}")

    # Sort each row
    percentile_matrix_sorted = np.sort(percentile_matrix, axis=1)

    # Extract sub-percentiles
    percentile_indices = [int(p * (len(sample_indices) - 1)) for p in percentiles_sub]
    perc_of_perc_curves = percentile_matrix_sorted[:, percentile_indices]

    percentiles_of_percentiles[perc_base] = perc_of_perc_curves

print("\n‚úì Percentiles of percentiles computed!")

# ============================================================================
# PHASE 8: PLOT WITH SHADED REGIONS
# ============================================================================

print("\nPlotting percentiles of percentiles with shaded regions...")

fig, ax = plt.subplots(figsize=(18, 11), facecolor='white')
ax.set_facecolor('white')

# Colors for shading and lines
colors_base = ['#8B0000', '#FF8C00', '#228B22', '#4169E1', '#8B008B']
colors_shaded = ['#FFB6B9', '#FFCC80', '#A5D6A7', '#90CAF9', '#CE93D8']

# Generate labels for percentiles
perc_names = [f'P{int(p*100)}' for p in percentiles_base]

# Ensure we have enough colors
if len(percentiles_base) > len(colors_base):
    colors_base = plt.cm.rainbow(np.linspace(0, 1, len(percentiles_base)))
    colors_shaded = plt.cm.rainbow(np.linspace(0, 1, len(percentiles_base)))

# FIRST: Plot shaded regions
print("  Plotting shaded uncertainty bands...")
for base_idx, (perc_base, perc_name) in enumerate(zip(percentiles_base, perc_names)):
    curves = percentiles_of_percentiles[perc_base]

    curve_p_min = curves[:, 0]  # First sub-percentile
    curve_p_max = curves[:, -1]  # Last sub-percentile

    valid_mask = (~np.isnan(curve_p_min)) & (~np.isnan(curve_p_max))

    if np.sum(valid_mask) > 3:
        color_shaded = colors_shaded[base_idx % len(colors_shaded)]
        ax.fill_betweenx(stress_range[valid_mask],
                curve_p_min[valid_mask],
                curve_p_max[valid_mask],
                color=color_shaded,
                alpha=0.85,
                label=f'{perc_name} uncertainty band',
                zorder=base_idx + 1)

# SECOND: Plot intermediate sub-percentile curves
print("  Plotting intermediate sub-percentile curves...")
if len(percentiles_sub) > 2:
    for base_idx, (perc_base, perc_name) in enumerate(zip(percentiles_base, perc_names)):
        curves = percentiles_of_percentiles[perc_base]
        color_base = colors_base[base_idx % len(colors_base)]

        for sub_idx in range(1, len(percentiles_sub)-1):
            curve = curves[:, sub_idx]
            valid_mask = ~np.isnan(curve)
            if np.sum(valid_mask) > 3:
                linestyle = '--' if sub_idx == 1 else ':'
                ax.plot(curve[valid_mask], stress_range[valid_mask],
                       color=color_base, linestyle=linestyle, linewidth=1.2,
                       alpha=0.5, zorder=10 + base_idx)

# THIRD: Plot extreme sub-percentiles
print("  Plotting extreme sub-percentile curves...")
for base_idx, (perc_base, perc_name) in enumerate(zip(percentiles_base, perc_names)):
    curves = percentiles_of_percentiles[perc_base]
    color_base = colors_base[base_idx % len(colors_base)]

    curve_p_min = curves[:, 0]
    valid_mask = ~np.isnan(curve_p_min)
    if np.sum(valid_mask) > 3:
        ax.plot(curve_p_min[valid_mask], stress_range[valid_mask],
               color=color_base, linestyle=(0, (5, 2)), linewidth=1.5,
               alpha=0.7, zorder=15 + base_idx)

    curve_p_max = curves[:, -1]
    valid_mask = ~np.isnan(curve_p_max)
    if np.sum(valid_mask) > 3:
        ax.plot(curve_p_max[valid_mask], stress_range[valid_mask],
               color=color_base, linestyle=(0, (1, 1)), linewidth=1.5,
               alpha=0.7, zorder=15 + base_idx)

# FOURTH: Plot median curves - THICK
print("  Plotting median curves...")
median_idx = len(percentiles_sub) // 2
for base_idx, (perc_base, perc_name) in enumerate(zip(percentiles_base, perc_names)):
    curves = percentiles_of_percentiles[perc_base]
    color_base = colors_base[base_idx % len(colors_base)]

    curve_median = curves[:, median_idx]

    valid_mask = ~np.isnan(curve_median)

    if np.sum(valid_mask) > 3:
        ax.plot(curve_median[valid_mask], stress_range[valid_mask],
               color=color_base, linewidth=3.5,
               label=f'{perc_name} (median curve)',
               zorder=50 + base_idx)

# FIFTH: Plot observed data on TOP
print("  Plotting observed data...")
ax.scatter(cycles_data, stress_data, c='black', s=80,
          alpha=0.9, label='Observed data', zorder=100, marker='o',
          edgecolors='white', linewidths=1.5)

cycles_min_plot = N_min * 0.1
cycles_max_plot = N_max * 10.0

ax.set_xlim([cycles_min_plot, cycles_max_plot])

ax.set_xlabel('Cycles to Failure (N)', fontsize=15, fontweight='bold')
ax.set_ylabel('Stress (Deltasigma, dimensionless)', fontsize=15, fontweight='bold')
ax.set_xscale('log')
ax.set_title(f'Percentiles of Percentiles with Uncertainty Bands ({dataset_name})\n' +
            'Bayesian Weibull Model - Castillo-Canteli Formulation',
            fontsize=15, fontweight='bold', pad=20)

# Legend
ax.legend(loc='upper right', fontsize=10, framealpha=0.98, ncol=2,
         columnspacing=1.0, handlelength=2.5,
         title='Base Percentiles & Uncertainty Bands',
         title_fontsize=11)

ax.grid(True, alpha=0.3, which='both', linestyle='-', linewidth=0.5)
ax.set_xlim([cycles_min_plot, cycles_max_plot])
ax.set_ylim([stress_min_plot - 0.01, stress_max_plot + 0.01])

plt.tight_layout()
percentiles_shaded_file = 'percentiles_of_percentiles_shaded.png'
plt.savefig(percentiles_shaded_file, dpi=250, bbox_inches='tight')
plt.show()

print("‚úì Shaded plot completed!")

# ============================================================================
# PHASE 9: ALTERNATIVE PLOT - ALL CURVES
# ============================================================================

print("\nCreating alternative plot with all curves...")

fig, ax = plt.subplots(figsize=(18, 11), facecolor='white')
ax.set_facecolor('white')

# Plot shaded regions first
for base_idx, perc_base in enumerate(percentiles_base):
    curves = percentiles_of_percentiles[perc_base]
    curve_p_min = curves[:, 0]
    curve_p_max = curves[:, -1]
    valid_mask = (~np.isnan(curve_p_min)) & (~np.isnan(curve_p_max))

    if np.sum(valid_mask) > 3:
        color_shaded = colors_shaded[base_idx % len(colors_shaded)]
        ax.fill_betweenx(stress_range[valid_mask],
                        curve_p_min[valid_mask],
                        curve_p_max[valid_mask],
                        color=color_shaded,
                        alpha=0.5,
                        zorder=base_idx + 1)

# Plot ALL curves with labels
linestyles_sub = ['-', '--', '-', ':', (0, (1, 1))]
if len(percentiles_sub) > len(linestyles_sub):
    linestyles_sub = ['-'] * len(percentiles_sub)

linewidths_sub = [1.5] * len(percentiles_sub)
linewidths_sub[median_idx] = 3.5  # Median thicker

alpha_sub = [0.6] * len(percentiles_sub)
alpha_sub[median_idx] = 1.0  # Median fully opaque

curve_count = 0
for base_idx, (perc_base, perc_name_base) in enumerate(zip(percentiles_base, perc_names)):
    curves = percentiles_of_percentiles[perc_base]
    color_base = colors_base[base_idx % len(colors_base)]

    perc_names_sub = [f'P{int(p*100)}' for p in percentiles_sub]

    for sub_idx in range(len(percentiles_sub)):
        curve = curves[:, sub_idx]
        valid_mask = ~np.isnan(curve)

        if np.sum(valid_mask) > 3:
            if sub_idx == median_idx:
                label = f'{perc_name_base}-{perc_names_sub[sub_idx]} (median)'
                zorder_val = 50 + base_idx
            elif sub_idx in [0, len(percentiles_sub)-1]:
                label = f'{perc_name_base}-{perc_names_sub[sub_idx]} (bound)'
                zorder_val = 30 + base_idx
            else:
                label = f'{perc_name_base}-{perc_names_sub[sub_idx]}'
                zorder_val = 20 + base_idx

            ax.plot(curve[valid_mask], stress_range[valid_mask],
                   color=color_base, linestyle=linestyles_sub[sub_idx],
                   linewidth=linewidths_sub[sub_idx],
                   label=label, alpha=alpha_sub[sub_idx], zorder=zorder_val)
            curve_count += 1

# Plot observed data
ax.scatter(cycles_data, stress_data, c='black', s=80,
          alpha=0.9, label='Observed data', zorder=100, marker='o',
          edgecolors='white', linewidths=1.5)

total_curves = len(percentiles_base) * len(percentiles_sub)
print(f"  Total curves plotted: {curve_count}/{total_curves}")

ax.set_xlabel('Cycles to Failure (N)', fontsize=15, fontweight='bold')
ax.set_ylabel('Stress (Deltasigma, dimensionless)', fontsize=15, fontweight='bold')
ax.set_xscale('log')
ax.set_title(f'All {total_curves} Percentile Curves ({len(percentiles_base)} Base √ó {len(percentiles_sub)} Sub) - {dataset_name}\n' +
            'Complete Uncertainty Quantification',
            fontsize=15, fontweight='bold', pad=20)

ax.legend(loc='upper right', fontsize=8, framealpha=0.95, ncol=3,
         columnspacing=0.6, handlelength=2.0, handletextpad=0.5,
         title='Base-Sub Percentile Curves', title_fontsize=9,
         borderpad=0.5, labelspacing=0.3)

ax.grid(True, alpha=0.3, which='both', linestyle='-', linewidth=0.5)
ax.set_xlim([cycles_min_plot, cycles_max_plot])
ax.set_ylim([stress_min_plot - 0.01, stress_max_plot + 0.01])

plt.tight_layout()
percentiles_all_curves_file = 'percentiles_of_percentiles_all_curves.png'
plt.savefig(percentiles_all_curves_file, dpi=250, bbox_inches='tight')
plt.show()

print("‚úì Alternative plot completed!")

# ============================================================================
# PHASE 10: SUMMARY
# ============================================================================

print("\n" + "="*70)
print("SUMMARY")
print("="*70)

print(f"\nDataset: {dataset_name}")
print(f"Observations: {len(cycles_data)}")

print("\n1. MLE ESTIMATES:")
print(f"   N‚ÇÄ = {mle_params[0]:.6f}")
print(f"   ŒîœÉ‚ÇÄ = {mle_params[1]:.6f}")
print(f"   Œ≤ = {mle_params[2]:.3f}")
print(f"   Œª = {mle_params[3]:.3f}")
print(f"   Œ¥ = {mle_params[4]:.3f}")

print("\n2. POSTERIOR MEDIANS:")
print(f"   N‚ÇÄ = {posterior['N0'].median().values:.6f}")
print(f"   ŒîœÉ‚ÇÄ = {posterior['Delta0'].median().values:.6f}")
print(f"   Œ≤ = {posterior['beta'].median().values:.3f}")
print(f"   Œª = {posterior['lambda_param'].median().values:.3f}")
print(f"   Œ¥ = {posterior['delta'].median().values:.3f}")

print("\n3. CONVERGENCE:")
print(f"   R-hat all < 1.01: {'‚úì' if all_rhat_good else '‚úó'}")
print(f"   ESS all > 1000: {'‚úì' if all_ess_good else '‚úó'}")
print(f"   Divergences: {n_divergences}")
if n_divergences > 0:
    print(f"   Divergence rate: {n_divergences/total_samples*100:.2f}%")

print("\n4. PERCENTILES OF PERCENTILES:")
print(f"   Total curves generated: {len(percentiles_base) * len(percentiles_sub)} ({len(percentiles_base)} base √ó {len(percentiles_sub)} sub)")
print(f"   Two visualization approaches:")
print(f"     ‚Ä¢ Main plot: Shaded bands with key curves")
print(f"     ‚Ä¢ Alternative: All curves individually labeled")

print("\n5. FILES CREATED:")
print("   ‚Ä¢ data_exploration_weibull.png")
print("   ‚Ä¢ mle_results.png")
print("   ‚Ä¢ mle_estimates.txt")
print("   ‚Ä¢ prior_distributions.png")
print("   ‚Ä¢ trace_plots.png")
print("   ‚Ä¢ posterior_distributions.png")
print("   ‚Ä¢ percentiles_of_percentiles_shaded.png")
print("   ‚Ä¢ percentiles_of_percentiles_all_curves.png")

print("\n" + "="*70)
print("ANALYSIS COMPLETE!")
print("="*70)

# Save results
fatigue_posterior_file = 'fatigue_posterior_weibull.nc'
trace.to_netcdf(fatigue_posterior_file)

fatigue_summary_file = 'fatigue_summary_weibull.csv'
summary_table.to_csv(fatigue_summary_file)

perc_of_perc_data = {}
for perc_base in percentiles_base:
    perc_of_perc_data[f'P{int(perc_base*100)}'] = percentiles_of_percentiles[perc_base]

percentiles_file = 'percentiles_of_percentiles.npz'
np.savez(percentiles_file,
         stress_range=stress_range,
         percentiles_base=percentiles_base,
         percentiles_sub=percentiles_sub,
         **perc_of_perc_data)

print("\n‚úì Results saved:")
print("  ‚Ä¢ fatigue_posterior_weibull.nc")
print("  ‚Ä¢ fatigue_summary_weibull.csv")
print("  ‚Ä¢ percentiles_of_percentiles.npz")

# ============================================================================
# PHASE 11: SYNTHETIC DATA GENERATION FROM POSTERIOR SAMPLES
# ============================================================================

print("\n" + "="*70)
print("SYNTHETIC DATA GENERATION FROM POSTERIOR SAMPLES")
print("="*70)

print("\nWould you like to generate synthetic datasets using posterior samples?")
print("(This uses the actual MCMC samples, excluding warmup)")
generate_synthetic = input("\nGenerate synthetic data? (y/n): ").strip().lower()

synthetic_dir = None
synthetic_files = []

if generate_synthetic == 'y':

    # Ask for number of observations per dataset
    while True:
        try:
            n_obs_per_dataset = int(input("\nHow many observations per synthetic dataset? (e.g., 75, 100, 360): ").strip())
            if n_obs_per_dataset > 0 and n_obs_per_dataset <= 20000:
                break
            else:
                print("Please enter a number between 1 and 20000")
        except ValueError:
            print("Invalid input. Please enter a number.")

    # Ask for number of datasets to generate
    while True:
        try:
            n_datasets = int(input("\nHow many synthetic datasets to generate? (e.g., 1, 10, 100): ").strip())
            if n_datasets > 0 and n_datasets <= 20000:
                break
            else:
                print("Please enter a number between 1 and 20000")
        except ValueError:
            print("Invalid input. Please enter a number.")

    print(f"\n{'='*70}")
    print(f"CONFIGURATION:")
    print(f"  Observations per dataset: {n_obs_per_dataset}")
    print(f"  Number of datasets: {n_datasets}")
    print(f"  Total posterior samples available: {len(N0_samples)}")
    print(f"{'='*70}")

    # Check if we have enough samples
    if n_datasets > len(N0_samples):
        print(f"\n‚ö† Warning: Requested {n_datasets} datasets but only {len(N0_samples)} posterior samples available.")
        print(f"  Will generate {min(n_datasets, len(N0_samples))} datasets (one per unique sample).")
        n_datasets = min(n_datasets, len(N0_samples))

    # Select random posterior samples (one per dataset)
    selected_sample_indices = np.random.choice(len(N0_samples), size=n_datasets, replace=False)

    print(f"\n‚úì Selected {n_datasets} random posterior samples")

    # Create directory for synthetic data
    synthetic_dir = f'synthetic_data_{dataset_name}'
    if not os.path.exists(synthetic_dir):
        os.makedirs(synthetic_dir)
    print(f"‚úì Created directory: {synthetic_dir}/")

    # Generate synthetic datasets
    print(f"\nGenerating {n_datasets} synthetic datasets...")

    all_synthetic_data = []

    for dataset_idx, sample_idx in enumerate(selected_sample_indices):
        if (dataset_idx + 1) % max(1, n_datasets // 10) == 0 or dataset_idx == 0:
            print(f"  Generating dataset {dataset_idx + 1}/{n_datasets}...")

        # Get parameter values from this posterior sample
        N0_synth = N0_samples[sample_idx]
        Delta0_synth = Delta0_samples[sample_idx]
        beta_synth = beta_samples[sample_idx]
        lambda_synth = lambda_samples[sample_idx]
        delta_synth = delta_samples[sample_idx]

        # Generate stress levels (uniform distribution within observed range)
        stress_synthetic = np.random.uniform(
            low=stress_data.min(),
            high=stress_data.max(),
            size=n_obs_per_dataset
        )

        # Generate cycles for each stress level using the Weibull model
        cycles_synthetic = np.zeros(n_obs_per_dataset)

        for i in range(n_obs_per_dataset):
            stress = stress_synthetic[i]

            # Weibull parameters for this stress level
            r = np.log(stress / Delta0_synth)
            mu_Y = (-lambda_synth - delta_synth) / r
            sigma_Y = delta_synth / (beta_synth * abs(r))

            # Generate random Gumbel (for minima) variable
            u = np.random.uniform(0, 1)
            z = np.log(-np.log(1 - u))  # Inverse CDF

            Y = mu_Y + sigma_Y * z
            N = N0_synth * np.exp(Y)

            cycles_synthetic[i] = N

        # Create DataFrame for this dataset
        synthetic_df = pd.DataFrame({
            'N': cycles_synthetic,
            'Deltasigma': stress_synthetic
        })

        # Sort by stress level
        synthetic_df = synthetic_df.sort_values('Deltasigma').reset_index(drop=True)

        # Store parameters used
        synthetic_df.attrs['N0'] = N0_synth
        synthetic_df.attrs['Delta0'] = Delta0_synth
        synthetic_df.attrs['beta'] = beta_synth
        synthetic_df.attrs['lambda'] = lambda_synth
        synthetic_df.attrs['delta'] = delta_synth
        synthetic_df.attrs['sample_idx'] = sample_idx

        all_synthetic_data.append(synthetic_df)

        # Save individual CSV file
        csv_filename = f'{synthetic_dir}/synthetic_{dataset_idx+1:04d}.csv'
        synthetic_df.to_csv(csv_filename, index=False)
        synthetic_files.append(csv_filename)

        # Save individual R/OpenBUGS format file
        r_filename = f'{synthetic_dir}/synthetic_{dataset_idx+1:04d}.txt'
        with open(r_filename, 'w') as f:
            f.write(f"# Synthetic dataset {dataset_idx+1}/{n_datasets}\n")
            f.write(f"# Generated from posterior sample {sample_idx}\n")
            f.write(f"# Parameters: N0={N0_synth:.6f}, Delta0={Delta0_synth:.6f}, beta={beta_synth:.3f}, lambda={lambda_synth:.3f}, delta={delta_synth:.3f}\n")
            f.write(f"list(M={n_obs_per_dataset},\n")
            f.write(f"ns=50,\n")
            f.write(f"np=5,\n")
            f.write(f"percentiles=c({','.join([str(p) for p in config['percentiles_base']])}),\n")

            # Write Deltasigma
            f.write("Deltasigma=c(")
            deltasigma_str = ','.join([f"{s:.3f}" for s in synthetic_df['Deltasigma'].values])
            f.write(deltasigma_str)
            f.write("),\n")

            # Write N
            f.write("N=c(")
            n_str = ','.join([f"{n:.1f}" for n in synthetic_df['N'].values])
            f.write(n_str)
            f.write(")\n")
            f.write(")\n")
        synthetic_files.append(r_filename)

    print(f"\n‚úì Generated {n_datasets} synthetic datasets!")
    print(f"  Files saved in: {synthetic_dir}/")

    # Create summary file with parameters used
    params_summary = pd.DataFrame({
        'dataset': [f'synthetic_{i+1:04d}' for i in range(n_datasets)],
        'sample_idx': [df.attrs['sample_idx'] for df in all_synthetic_data],
        'N0': [df.attrs['N0'] for df in all_synthetic_data],
        'Delta0': [df.attrs['Delta0'] for df in all_synthetic_data],
        'beta': [df.attrs['beta'] for df in all_synthetic_data],
        'lambda': [df.attrs['lambda'] for df in all_synthetic_data],
        'delta': [df.attrs['delta'] for df in all_synthetic_data]
    })

    params_summary_file = f'{synthetic_dir}/parameters_summary.csv'
    params_summary.to_csv(params_summary_file, index=False)
    synthetic_files.append(params_summary_file)
    print(f"‚úì Parameters summary saved to: {params_summary_file}")

    # Create consolidated CSV with all datasets
    consolidated_data = []
    for idx, df in enumerate(all_synthetic_data):
        df_copy = df.copy()
        df_copy['dataset'] = idx + 1
        consolidated_data.append(df_copy)

    consolidated_df = pd.concat(consolidated_data, ignore_index=True)
    consolidated_file = f'{synthetic_dir}/all_synthetic_data.csv'
    consolidated_df.to_csv(consolidated_file, index=False)
    synthetic_files.append(consolidated_file)
    print(f"‚úì Consolidated data saved to: {consolidated_file}")

    # Summary statistics
    print("\n" + "="*70)
    print("SYNTHETIC DATA SUMMARY")
    print("="*70)

    print(f"\nGenerated datasets: {n_datasets}")
    print(f"Observations per dataset: {n_obs_per_dataset}")
    print(f"Total synthetic observations: {n_datasets * n_obs_per_dataset}")

    print(f"\nParameter ranges across datasets:")
    print(f"  N‚ÇÄ:     [{params_summary['N0'].min():.6f}, {params_summary['N0'].max():.6f}]")
    print(f"  ŒîœÉ‚ÇÄ:    [{params_summary['Delta0'].min():.6f}, {params_summary['Delta0'].max():.6f}]")
    print(f"  Œ≤:      [{params_summary['beta'].min():.3f}, {params_summary['beta'].max():.3f}]")
    print(f"  Œª:      [{params_summary['lambda'].min():.3f}, {params_summary['lambda'].max():.3f}]")
    print(f"  Œ¥:      [{params_summary['delta'].min():.3f}, {params_summary['delta'].max():.3f}]")

    # Visualize first few datasets
    print("\nVisualizing first 3 synthetic datasets vs observed data...")

    n_plots = min(3, n_datasets)
    fig, axes = plt.subplots(1, n_plots + 1, figsize=(5 * (n_plots + 1), 5))

    if n_plots == 1:
        axes = [axes]

    # Definir l√≠mites comunes basados en los datos observados
    common_x_min = cycles_data.min() * 0.1
    common_x_max = cycles_data.max() * 10.0
    common_y_min = max(stress_data.min() * 0.95, 0.1)
    common_y_max = min(stress_data.max() * 1.05, 1.0)

    # Plot observed data
    ax = axes[0]
    ax.scatter(cycles_data, stress_data, c='blue', s=50, alpha=0.7,
              label='Observed', edgecolors='black', linewidths=0.5)
    ax.set_xlabel('Cycles (N)', fontsize=10, fontweight='bold')
    ax.set_ylabel('Stress (ŒîœÉ)', fontsize=10, fontweight='bold')
    ax.set_xscale('log')
    ax.set_xlim([common_x_min, common_x_max])
    ax.set_ylim([common_y_min, common_y_max])
    ax.set_title(f'Observed Data\n({len(cycles_data)} obs)', fontsize=11, fontweight='bold')
    ax.grid(True, alpha=0.3, which='both')
    ax.legend(fontsize=9)

    # Plot first few synthetic datasets
    colors = ['red', 'green', 'orange']
    for i in range(n_plots):
        ax = axes[i + 1]
        df = all_synthetic_data[i]
        ax.scatter(df['N'].values, df['Deltasigma'].values, c=colors[i], s=30, alpha=0.6,
                  label=f'Synthetic {i+1}', marker='^', edgecolors='black', linewidths=0.5)
        ax.set_xlabel('Cycles (N)', fontsize=10, fontweight='bold')
        ax.set_ylabel('Stress (ŒîœÉ)', fontsize=10, fontweight='bold')
        ax.set_xscale('log')
        # Usar los mismos l√≠mites en todos los gr√°ficos
        ax.set_xlim([common_x_min, common_x_max])
        ax.set_ylim([common_y_min, common_y_max])
        ax.set_title(f'Synthetic Dataset {i+1}\n({len(df)} obs)', fontsize=11, fontweight='bold')
        ax.grid(True, alpha=0.3, which='both')
        ax.legend(fontsize=9)

    plt.tight_layout()
    comparison_plot = f'{synthetic_dir}/synthetic_vs_observed_comparison.png'
    plt.savefig(comparison_plot, dpi=150, bbox_inches='tight')
    plt.show()
    synthetic_files.append(comparison_plot)
    print(f"‚úì Comparison plot saved to: {comparison_plot}")

    # Create ZIP file with all synthetic data
    print("\nCreating ZIP archive with all synthetic data...")

    zip_filename = f'{synthetic_dir}.zip'
    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
        # Add all files in the synthetic directory
        for root, dirs, files in os.walk(synthetic_dir):
            for file in files:
                file_path = os.path.join(root, file)
                arcname = os.path.relpath(file_path, os.path.dirname(synthetic_dir))
                zipf.write(file_path, arcname)

    print(f"‚úì ZIP archive created: {zip_filename}")
    print(f"  Size: {os.path.getsize(zip_filename) / (1024*1024):.2f} MB")
    synthetic_files.append(zip_filename)

    # Download in Colab
    if IN_COLAB:
        print("\nDownloading ZIP file...")
        try:
            files.download(zip_filename)
            print(f"  ‚úì Downloaded: {zip_filename}")
        except Exception as e:
            print(f"  ‚úó Could not download: {e}")
            print(f"  Files are available in: {synthetic_dir}/")
    else:
        print(f"\n‚úì Files ready in: {synthetic_dir}/")
        print(f"‚úì ZIP archive: {zip_filename}")

    print("\n" + "="*70)
    print("FILES IN ZIP ARCHIVE:")
    print("="*70)
    print(f"  ‚Ä¢ synthetic_XXXX.csv ({n_datasets} files) - Individual datasets in CSV format")
    print(f"  ‚Ä¢ synthetic_XXXX.txt ({n_datasets} files) - Individual datasets in R/OpenBUGS format")
    print(f"  ‚Ä¢ all_synthetic_data.csv - All datasets combined")
    print(f"  ‚Ä¢ parameters_summary.csv - Parameters used for each dataset")
    print(f"  ‚Ä¢ synthetic_vs_observed_comparison.png - Visual comparison")

    print("\n‚úì Synthetic data generation complete!")

else:
    print("\n‚úì Skipping synthetic data generation")

# ============================================================================
# CREATE ZIP WITH ALL RESULTS (INCLUDING FULL PYTHON SCRIPT) - VERSI√ìN CORREGIDA
# ============================================================================

print("\n" + "="*70)
print("CREATE ZIP ARCHIVE WITH ALL RESULTS")
print("="*70)

create_zip = input("\n¬øCrear archivo ZIP con todos los resultados? (y/n): ").strip().lower()

zip_files_created = []
py_script_filename = None
full_script_filename = None
synthetic_zip_created = None

if create_zip == 'y':
    # Nombre corto del ZIP general
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    zip_filename = f"fatigue_results_{dataset_name[:15]}_{timestamp}.zip"

    # ========================================================================
    # 1. CREAR ARCHIVO .PY INFORMATIVO (RESUMEN)
    # ========================================================================
    py_script_filename = f"analysis_info_{dataset_name[:10]}_{timestamp}.py"
    print(f"\nüíæ Creando archivo .py con informaci√≥n completa...")

    # Crear contenido COMPLETO para el archivo .py INFORMATIVO
    info_code = f'''# -*- coding: utf-8 -*-
"""
FATIGUE ANALYSIS - WEIBULL MODEL
================================
Dataset: {dataset_name}
Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Observations: {len(cycles_data)}

SUMMARY
-------
This analysis performed Maximum Likelihood Estimation (MLE) and Bayesian
inference using the Weibull model with Castillo-Canteli dimensionless
formulation for fatigue data analysis.

RESULTS
-------
MLE PARAMETERS:
  N‚ÇÄ (reference cycles) = {mle_params[0]:.6f}
  ŒîœÉ‚ÇÄ (reference stress) = {mle_params[1]:.6f}
  Œ≤ (shape parameter)    = {mle_params[2]:.6f}
  Œª (location param)     = {mle_params[3]:.6f}
  Œ¥ (scale param)        = {mle_params[4]:.6f}
  Log-likelihood         = {-result_global.fun:.2f}

BAYESIAN POSTERIOR MEDIANS:
  N‚ÇÄ = {posterior['N0'].median().values:.6f}
  ŒîœÉ‚ÇÄ = {posterior['Delta0'].median().values:.6f}
  Œ≤ = {posterior['beta'].median().values:.3f}
  Œª = {posterior['lambda_param'].median().values:.3f}
  Œ¥ = {posterior['delta'].median().values:.3f}

MCMC CONFIGURATION:
  Tune (warmup): {config['n_tune']}
  Draws (samples): {config['n_draws']}
  Chains: {config['n_chains']}
  Target acceptance: {config['target_accept']}

CONVERGENCE DIAGNOSTICS:
  R-hat all < 1.01: {'‚úì' if all_rhat_good else '‚úó'}
  ESS all > 1000: {'‚úì' if all_ess_good else '‚úó'}
  Divergences: {n_divergences}
  Divergence rate: {n_divergences/total_samples*100:.2f}%

DATA CHARACTERISTICS:
  Stress range: [{stress_data.min():.3f}, {stress_data.max():.3f}]
  Cycles range: [{N_min:.4f}, {N_max:.2f}]

PERCENTILES ANALYZED:
  Base percentiles: {[int(p*100) for p in config['percentiles_base']]}%
  Sub-percentiles: {[int(p*100) for p in config['percentiles_sub']]}%
  Stress points: {config['n_stress_points']}
  Parameter samples: {config['n_param_samples']}

PRIOR DISTRIBUTIONS (CV):
  CV(N‚ÇÄ) = {config['cv_N0']:.2f}
  CV(ŒîœÉ‚ÇÄ) = {config['cv_Delta0']:.2f}
  CV(Œ≤) = {config['cv_beta']:.2f}
  CV(Œª) = {config['cv_lambda']:.2f}
  CV(Œ¥) = {config['cv_delta']:.2f}

FILES INCLUDED IN THIS ZIP:
---------------------------
IMAGES:
1. data_exploration.png - Initial data visualization
2. mle_results.png - MLE fit with percentile curves
3. prior_distributions.png - Prior distributions
4. trace_plots.png - MCMC trace plots
5. posterior_distributions.png - Posterior distributions
6. percentiles_shaded.png - Percentiles with uncertainty bands
7. percentiles_all.png - All percentile curves

DATA FILES:
8. mle_estimates.txt - MLE parameter estimates
9. fatigue_posterior.nc - MCMC samples (NetCDF format)
10. fatigue_summary.csv - Statistical summary
11. percentiles_data.npz - Percentile data (NumPy format)

DOCUMENTATION:
12. analysis_info.py - This file with analysis information
13. fatigue_analysis_FULL.py - Complete Python script used for analysis
{f"14. synthetic_data/ - Synthetic datasets directory ({n_datasets} datasets)" if 'n_datasets' in locals() else ""}

ANALYSIS STEPS:
---------------
1. Data loading and exploration
2. Maximum Likelihood Estimation (differential evolution)
3. Bayesian model definition with informative priors
4. MCMC sampling (PyMC/ArviZ)
5. Convergence diagnostics
6. Percentiles of percentiles computation
7. Visualization and results export

SOFTWARE VERSIONS:
------------------
- Python
- NumPy
- PyMC
- ArviZ
- SciPy
- Matplotlib
- Pandas

NOTES:
------
‚Ä¢ This analysis uses the Weibull distribution for minima (fatigue data)
‚Ä¢ Castillo-Canteli dimensionless formulation
‚Ä¢ Informative priors centered on MLE estimates
‚Ä¢ Bayesian uncertainty quantification

To reproduce this analysis, ensure you have the required Python packages
and run the original analysis script/notebook.
"""

print("="*70)
print("FATIGUE ANALYSIS - WEIBULL MODEL - COMPLETE REPORT")
print("="*70)
print(f"Dataset: {dataset_name}")
print(f"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"Observations: {len(cycles_data)}")

print("\\n" + "="*70)
print("MLE RESULTS")
print("="*70)
print(f"N‚ÇÄ (reference cycles) = {mle_params[0]:.6f}")
print(f"ŒîœÉ‚ÇÄ (reference stress) = {mle_params[1]:.6f}")
print(f"Œ≤ (shape parameter)    = {mle_params[2]:.6f}")
print(f"Œª (location param)     = {mle_params[3]:.6f}")
print(f"Œ¥ (scale param)        = {mle_params[4]:.6f}")
print(f"Log-likelihood         = {-result_global.fun:.2f}")

print("\\n" + "="*70)
print("BAYESIAN RESULTS (POSTERIOR MEDIANS)")
print("="*70)
print(f"N‚ÇÄ = {posterior['N0'].median().values:.6f}")
print(f"ŒîœÉ‚ÇÄ = {posterior['Delta0'].median().values:.6f}")
print(f"Œ≤ = {posterior['beta'].median().values:.3f}")
print(f"Œª = {posterior['lambda_param'].median().values:.3f}")
print(f"Œ¥ = {posterior['delta'].median().values:.3f}")

print("\\n" + "="*70)
print("CONVERGENCE DIAGNOSTICS")
print("="*70)
print(f"R-hat all < 1.01: {'‚úì PASS' if all_rhat_good else '‚úó FAIL'}")
print(f"ESS all > 1000: {'‚úì PASS' if all_ess_good else '‚úó FAIL'}")
print(f"Divergences: {n_divergences}")
print(f"Divergence rate: {n_divergences/total_samples*100:.2f}%")

print("\\n" + "="*70)
print("FILES INCLUDED")
print("="*70)
print("IMAGES:")
print("1. data_exploration.png")
print("2. mle_results.png")
print("3. prior_distributions.png")
print("4. trace_plots.png")
print("5. posterior_distributions.png")
print("6. percentiles_shaded.png")
print("7. percentiles_all.png")

print("\\nDATA FILES:")
print("8. mle_estimates.txt")
print("9. fatigue_posterior.nc")
print("10. fatigue_summary.csv")
print("11. percentiles_data.npz")
print("12. analysis_info.py (this file)")
print("13. fatigue_analysis_FULL.py (complete Python script)")

if 'n_datasets' in locals():
    print(f"\\nSYNTHETIC DATA:")
    print(f"14. synthetic_data/ directory ({n_datasets} datasets)")

print("\\n" + "="*70)
print("END OF REPORT")
print("="*70)

# Utility function to load summary
def load_summary():
    """Load and display the statistical summary."""
    try:
        import pandas as pd
        summary = pd.read_csv('fatigue_summary.csv')
        print("\\nSTATISTICAL SUMMARY:")
        print(summary.to_string())
    except FileNotFoundError:
        print("\\nNote: File 'fatigue_summary.csv' not found in current directory.")
    except Exception:
        print("\\nError loading summary: unable to load file")

if __name__ == "__main__":
    print("\\nUtility functions available:")
    print("‚Ä¢ load_summary() - Loads and displays statistical summary")
'''

    # Guardar el archivo .py informativo
    with open(py_script_filename, 'w', encoding='utf-8') as f:
        f.write(info_code)

    print(f"‚úì Archivo informativo creado: {py_script_filename}")

    # ========================================================================
    # 2. GUARDAR EL SCRIPT PYTHON COMPLETO
    # ========================================================================
    print(f"\nüíæ Guardando script Python COMPLETO...")

    # Intentar obtener el c√≥digo fuente actual
    try:
        # Para Google Colab
        if IN_COLAB:
            try:
                # M√©todo 1: Usar IPython para obtener el historial
                from IPython import get_ipython
                ipython = get_ipython()
                if ipython:
                    # Obtener todas las celdas ejecutadas
                    current_code = ""
                    # Intentar obtener el c√≥digo de varias formas
                    try:
                        # M√©todo para notebooks
                        from google.colab import _message
                        cell_num = 1
                        while True:
                            try:
                                cell_data = _message.blocking_request(
                                    'get_code_cell',
                                    {'cell_id': f'codecell_{cell_num}'},
                                    timeout_sec=2
                                )
                                if cell_data and 'code' in cell_data:
                                    current_code += f"# Celda {cell_num}\n{cell_data['code']}\n\n"
                                cell_num += 1
                            except:
                                break
                    except:
                        # M√©todo alternativo: capturar el historial de comandos
                        history = ipython.history_manager.get_range()
                        for h in history:
                            current_code += h[2] + "\n"
            except Exception as e:
                print(f"‚ö† No se pudo obtener el c√≥digo completo de Colab: {e}")
                current_code = ""
        else:
            # Para ejecuci√≥n local desde archivo .py
            try:
                import inspect
                import __main__
                # Intentar obtener el c√≥digo fuente del m√≥dulo principal
                current_code = inspect.getsource(__main__)
            except:
                try:
                    # Leer el archivo actual
                    with open(__file__, 'r', encoding='utf-8') as f:
                        current_code = f.read()
                except:
                    current_code = ""
    except Exception as e:
        print(f"‚ö† Error obteniendo c√≥digo: {e}")
        current_code = ""

    # Si no pudimos obtener el c√≥digo, crear uno representativo
    if not current_code or len(current_code) < 1000:
        print("‚ö† No se pudo obtener el c√≥digo completo, creando versi√≥n representativa...")
        current_code = '''#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
FATIGUE ANALYSIS - WEIBULL MODEL - SCRIPT COMPLETO
==================================================
Script principal para an√°lisis de fatiga usando modelo Weibull.

NOTA: Este es un archivo representativo. El c√≥digo original se ejecut√≥
en un entorno interactivo (Colab/Jupyter) y no se pudo capturar completo.

Para reproducir el an√°lisis completo, consulte:
1. Los archivos de resultados generados
2. Las im√°genes con visualizaciones
3. Los datos sint√©ticos (si se generaron)
4. El archivo analysis_info.py para par√°metros detallados
"""

print("Script representativo del an√°lisis de fatiga Weibull")
print("El an√°lisis completo incluye:")
print("1. Estimaci√≥n de M√°xima Verosimilitud (MLE)")
print("2. Inferencia Bayesiana con PyMC")
print("3. C√°lculo de percentiles de percentiles")
print("4. Generaci√≥n de visualizaciones")
print("5. Opcional: Generaci√≥n de datos sint√©ticos")
'''

    full_script_filename = f"fatigue_analysis_FULL_{timestamp}.py"

    # Guardar el script completo
    with open(full_script_filename, 'w', encoding='utf-8') as f:
        # Encabezado detallado
        header = f'''#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
FATIGUE ANALYSIS - WEIBULL MODEL - SCRIPT COMPLETO
==================================================
Dataset: {dataset_name}
Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Observaciones: {len(cycles_data)}

DESCRIPCI√ìN:
------------
An√°lisis completo de fatiga usando modelo Weibull con formulaci√≥n
adimensional de Castillo-Canteli.

ESTE ES EL SCRIPT COMPLETO QUE SE EJECUT√ì PARA GENERAR LOS RESULTADOS.
Incluye todos los pasos del an√°lisis:

1. CARGA DE DATOS
   - Datos de ejemplo Holmen o archivo del usuario
   - Formatos soportados: CSV, Excel, R/OpenBUGS

2. ESTIMACI√ìN DE M√ÅXIMA VEROSIMILITUD (MLE)
   - Optimizaci√≥n global con differential_evolution
   - C√°lculo de par√°metros del modelo Weibull

3. INFERENCIA BAYESIANA
   - Modelo con PyMC usando priors informativos
   - MCMC sampling con m√∫ltiples cadenas
   - Diagn√≥sticos de convergencia (R-hat, ESS)

4. C√ÅLCULO DE PERCENTILES
   - Percentiles de percentiles
   - Curvas con bandas de incertidumbre
   - Visualizaciones completas

5. GENERACI√ìN DE RESULTADOS
   - Gr√°ficos en formato PNG
   - Datos en CSV, NetCDF, NumPy
   - Archivo ZIP con todos los resultados

6. OPCIONAL: DATOS SINT√âTICOS
   - Generaci√≥n de datasets sint√©ticos
   - Basados en muestras posteriores

PAR√ÅMETROS CLAVE:
-----------------
‚Ä¢ N‚ÇÄ: Ciclos de referencia = {mle_params[0]:.6f}
‚Ä¢ ŒîœÉ‚ÇÄ: Tensi√≥n de referencia = {mle_params[1]:.6f}
‚Ä¢ Œ≤: Par√°metro de forma = {mle_params[2]:.3f}
‚Ä¢ Œª: Par√°metro de localizaci√≥n = {mle_params[3]:.3f}
‚Ä¢ Œ¥: Par√°metro de escala = {mle_params[4]:.3f}

EJECUCI√ìN:
----------
Este script fue ejecutado en {'Google Colab' if IN_COLAB else 'entorno local'}
Configuraci√≥n MCMC:
‚Ä¢ Tune (warmup): {config['n_tune']}
‚Ä¢ Draws (samples): {config['n_draws']}
‚Ä¢ Chains: {config['n_chains']}
‚Ä¢ Target acceptance: {config['target_accept']}

REQUISITOS:
-----------
‚Ä¢ Python 3.7+
‚Ä¢ numpy, pandas, matplotlib
‚Ä¢ pymc, arviz
‚Ä¢ scipy

USO:
----
Para ejecutar el an√°lisis completo, aseg√∫rese de tener instaladas
todas las dependencias y ejecute:

python fatigue_analysis_FULL.py

O en Google Colab:
!python fatigue_analysis_FULL.py

NOTA: Algunas partes interactivas pueden requerir ajustes para
ejecuci√≥n no interactiva.
"""

# ============================================================================
# CONFIGURACI√ìN INICIAL
# ============================================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pymc as pm
import arviz as az
import pytensor.tensor as pt
from scipy import stats
from scipy.optimize import minimize, differential_evolution
import warnings
import sys
import os
import glob
import shutil
import zipfile
from datetime import datetime
import re

warnings.filterwarnings('ignore')
plt.rcParams['figure.figsize'] = [14, 8]
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

print("="*70)
print("FATIGUE ANALYSIS - WEIBULL MODEL")
print("Maximum Likelihood Estimation + Bayesian Inference")
print("="*70)

'''
        f.write(header)
        f.write(current_code)
        # A√±adir pie de p√°gina
        footer = f'''
# ============================================================================
# FIN DEL SCRIPT
# ============================================================================

print("\\n" + "="*70)
print("AN√ÅLISIS COMPLETADO EXITOSAMENTE")
print("="*70)
print(f"Dataset analizado: {dataset_name}")
print(f"Fecha de an√°lisis: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"Resultados guardados en el archivo ZIP generado")

# Informaci√≥n de contacto/resumen
print("\\nRESUMEN DE RESULTADOS:")
print(f"‚Ä¢ Observaciones procesadas: {len(cycles_data)}")
print(f"‚Ä¢ Par√°metros MLE estimados: N‚ÇÄ={mle_params[0]:.6f}, ŒîœÉ‚ÇÄ={mle_params[1]:.6f}")
print(f"‚Ä¢ Muestras MCMC generadas: {config['n_draws'] * config['n_chains']}")
print(f"‚Ä¢ Percentiles calculados: {len(config['percentiles_base'])} base √ó {len(config['percentiles_sub'])} sub")
{'‚Ä¢ Datos sint√©ticos generados: ' + str(n_datasets) + ' datasets' if 'n_datasets' in locals() else '‚Ä¢ No se generaron datos sint√©ticos'}

print("\\n¬°Gracias por usar el sistema de an√°lisis de fatiga Weibull!")
'''
        f.write(footer)

    print(f"‚úì Script COMPLETO guardado: {full_script_filename}")

    # ========================================================================
    # 3. PREPARAR ARCHIVOS PARA EL ZIP
    # ========================================================================
    # Lista de archivos a incluir en el ZIP
    main_files = [
        (data_exploration_file, 'data_exploration.png'),
        (mle_results_file, 'mle_results.png'),
        (mle_estimates_file, 'mle_estimates.txt'),
        (prior_distributions_file, 'prior_distributions.png'),
        (trace_plots_file, 'trace_plots.png'),
        (posterior_distributions_file, 'posterior_distributions.png'),
        (percentiles_shaded_file, 'percentiles_shaded.png'),
        (percentiles_all_curves_file, 'percentiles_all.png'),
        (fatigue_posterior_file, 'fatigue_posterior.nc'),
        (fatigue_summary_file, 'fatigue_summary.csv'),
        (percentiles_file, 'percentiles_data.npz'),
        (py_script_filename, 'analysis_info.py'),
        (full_script_filename, 'fatigue_analysis_FULL.py')  # Script completo
    ]

    # Filtrar solo archivos que existen
    files_to_zip = []
    print(f"\nüì¶ Preparando {len(main_files)} archivos principales...")
    for original_file, short_name in main_files:
        if os.path.exists(original_file):
            files_to_zip.append((original_file, short_name))
            print(f"  ‚úì {short_name}")
        else:
            print(f"  ‚úó {short_name} (no encontrado)")

    # A√±adir directorio de datos sint√©ticos si existe
    if synthetic_dir and os.path.exists(synthetic_dir):
        print(f"\nüìÅ A√±adiendo datos sint√©ticos...")
        synth_file_count = 0
        for root, dirs, files in os.walk(synthetic_dir):
            for file in files:
                file_path = os.path.join(root, file)
                rel_path = os.path.relpath(file_path, synthetic_dir)
                zip_path = f"synthetic_data/{rel_path}"
                files_to_zip.append((file_path, zip_path))
                synth_file_count += 1
        print(f"  ‚úì {synth_file_count} archivos sint√©ticos a√±adidos")

    # ========================================================================
    # 4. CREAR EL ARCHIVO ZIP
    # ========================================================================
    if files_to_zip:
        print(f"\nüì¶ Creando ZIP '{zip_filename}' con {len(files_to_zip)} archivos...")

        total_size = 0
        with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for original_file, zip_name in files_to_zip:
                if os.path.exists(original_file):
                    zipf.write(original_file, zip_name)
                    file_size = os.path.getsize(original_file)
                    total_size += file_size

        print(f"\n‚úÖ ZIP creado exitosamente!")
        print(f"   üì¶ Tama√±o: {total_size/(1024*1024):.2f} MB")
        print(f"   üìÑ Total archivos: {len(files_to_zip)}")

        zip_files_created.append(zip_filename)

        # ====================================================================
        # 5. MOSTRAR CONTENIDO DEL ZIP (VERSI√ìN CORREGIDA)
        # ====================================================================
        print("\n" + "="*60)
        print("CONTENIDO DETALLADO DEL ZIP:")
        print("="*60)

        with zipfile.ZipFile(zip_filename, 'r') as zipf:
            file_list = zipf.namelist()

            # Primero definir las categor√≠as principales
            images = [f for f in file_list if f.endswith('.png')]
            data_files = [f for f in file_list if f.endswith(('.csv', '.txt', '.nc', '.npz'))]
            py_files = [f for f in file_list if f.endswith('.py')]
            synth_files = [f for f in file_list if 'synthetic_data' in f]

            # Calcular archivos "otros"
            all_categorized = images + data_files + py_files + synth_files
            other_files = [f for f in file_list if f not in all_categorized]

            # Organizar en categor√≠as
            categories = {
                'Im√°genes (.png)': images,
                'Archivos de datos': data_files,
                'Scripts Python': py_files,
                'Datos sint√©ticos': synth_files,
                'Otros': other_files
            }

            file_count = 0
            total_size_kb = 0

            for category, files in categories.items():
                if files:
                    print(f"\n{category.upper()} ({len(files)} archivos):")
                    for filename in sorted(files):
                        file_count += 1
                        try:
                            file_info = zipf.getinfo(filename)
                            size_kb = file_info.file_size / 1024
                            total_size_kb += size_kb
                            print(f"  {file_count:3d}. {filename:<50} {size_kb:6.1f} KB")
                        except:
                            print(f"  {file_count:3d}. {filename:<50}")

            print(f"\n{'='*60}")
            print(f"RESUMEN: {file_count} archivos, {total_size_kb/1024:.2f} MB")
            print(f"Script principal incluido: {'fatigue_analysis_FULL.py' if 'fatigue_analysis_FULL.py' in py_files else 'NO'}")

        # ====================================================================
        # 6. DESCARGAR EN COLAB (SI APLICA)
        # ====================================================================
        if IN_COLAB:
            print("\n" + "="*70)
            print("DESCARGANDO ARCHIVO ZIP...")
            print("="*70)
            try:
                files.download(zip_filename)
                print(f"‚úÖ ZIP descargado: {zip_filename}")
                print(f"   üì¶ Tama√±o: {total_size/(1024*1024):.2f} MB")
                print(f"   üìÑ Archivos incluidos: {len(file_list)}")
                print(f"\n‚ö† IMPORTANTE: El archivo 'fatigue_analysis_FULL.py' contiene")
                print("              el c√≥digo Python COMPLETO del an√°lisis")
            except Exception as e:
                print(f"‚ö† No se pudo descargar autom√°ticamente: {e}")
                print(f"üìç Archivo disponible en: {os.path.abspath(zip_filename)}")
        else:
            print(f"\nüìç Archivo ZIP disponible en: {os.path.abspath(zip_filename)}")
            print(f"   üì¶ Tama√±o: {total_size/(1024*1024):.2f} MB")
            print(f"   üìÑ Archivos incluidos: {len(file_list)}")
            print(f"\n‚ö† IMPORTANTE: El archivo 'fatigue_analysis_FULL.py' contiene")
            print("              el c√≥digo Python COMPLETO del an√°lisis")

    else:
        print("‚ö† No se encontraron archivos para comprimir")

    print("\n" + "="*70)

else:
    print("\n‚úì Saltando creaci√≥n de ZIP")

# ============================================================================
# LIMPIEZA TOTAL AUTOM√ÅTICA
# ============================================================================

print("\n" + "="*70)
print("LIMPIANDO TODO AUTOM√ÅTICAMENTE")
print("="*70)

# Patrones de todo lo que se puede eliminar
patrones_eliminar = [
    # Im√°genes
    '*.png',

    # Datos generados
    '*mle_estimates*',
    '*fatigue_posterior*',
    '*fatigue_summary*',
    '*percentiles_of_percentiles*',

    # Archivos de datos originales
    '*_data*.txt',
    '*_data*.csv',
    '*_data*.xlsx',

    # Directorios
    'synthetic_data_*',

    # Scripts generados
    'analysis_info_*',
    'fatigue_analysis_FULL_*',

    # Archivos temporales
    '*.log',
    '*.tmp',
]

# NO eliminar (por seguridad)
no_eliminar = [
    # El script principal si estamos ejecutando desde √©l
    __file__ if '__file__' in globals() else '',

    # Archivos Python del sistema
    '*.py'  # Pero cuidado, solo eliminaremos los espec√≠ficos arriba
]

archivos_eliminados = []

for patron in patrones_eliminar:
    for archivo in glob.glob(patron):
        # Verificar que no est√© en la lista de no eliminar
        eliminar = True
        for exclusion in no_eliminar:
            if exclusion and archivo == exclusion:
                eliminar = False
                break

        if eliminar and os.path.exists(archivo):
            try:
                if os.path.isdir(archivo):
                    shutil.rmtree(archivo)
                    archivos_eliminados.append(f"{archivo}/")
                else:
                    os.remove(archivo)
                    archivos_eliminados.append(archivo)
            except:
                pass

if archivos_eliminados:
    print(f"\nüóëÔ∏è  Eliminados {len(archivos_eliminados)} archivos:")
    for archivo in sorted(archivos_eliminados)[:20]:  # Mostrar primeros 20 ordenados
        print(f"  ‚Ä¢ {archivo}")

    espacio = sum(os.path.getsize(f) for f in archivos_eliminados if os.path.exists(f))
    print(f"\nüíæ Espacio liberado: {espacio/(1024*1024):.1f} MB")
else:
    print("\n‚úÖ No hab√≠a archivos para eliminar.")

print("\n" + "="*70)
print("¬°TODO LIMPIO! ‚úÖ")
print("="*70)

print("\n" + "="*70)
print("ANALYSIS COMPLETED SUCCESSFULLY!")
print("="*70)